<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>sysPipe – systemPipeR</title><link>/sp/spr/</link><description>Recent content in systemPipeR on sysPipe</description><generator>Hugo -- gohugo.io</generator><atom:link href="/sp/spr/index.xml" rel="self" type="application/rss+xml"/><item><title>Sp: Introduction</title><link>/sp/spr/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/introduction/</guid><description>
&lt;!--
- Compile from command-line
Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::html_document'), clean=F); knitr::knit('systemPipeR.Rmd', tangle=TRUE)"; Rscript ../md2jekyll.R systemPipeR.knit.md 2; Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::pdf_document'))"
-->
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
document.querySelector("h1").className = "title";
});
&lt;/script>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
var links = document.links;
for (var i = 0, linksLength = links.length; i &lt; linksLength; i++)
if (links[i].hostname != window.location.hostname)
links[i].target = '_blank';
});
&lt;/script>
&lt;p>&lt;strong>Note:&lt;/strong> if you use &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> in published research, please cite:
Backman, T.W.H and Girke, T. (2016). &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>: NGS Workflow and Report Generation Environment. &lt;em>BMC Bioinformatics&lt;/em>, 17: 388. &lt;a href="https://doi.org/10.1186/s12859-016-1241-0">10.1186/s12859-016-1241-0&lt;/a>.&lt;/p>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>&lt;a href="http://www.bioconductor.org/packages/devel/bioc/html/systemPipeR.html">&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>&lt;/a>
provides flexible utilities for designing, building, and running automated nd-to-end analysis
workflows for a wide range of research applications, including next-generation
sequencing (NGS) experiments (H Backman and Girke 2016). Important features include a
uniform workflow interface across different data analysis applications, automated
report generation, and support for running both R and command-line software,
on local computers or compute clusters (see Figure &lt;a href="#fig:utilities">1&lt;/a>).
The latter supports interactive job submissions and batch submissions to queuing
systems of clusters.&lt;/p>
&lt;p>It has been designed to improve the reproducibility of large-scale data analysis
projects while substantially reducing the time it takes to analyze complex omics
data sets. Its unique features include a uniform workflow interface and management
system that allows the user to run selected steps, customize, and design entirely
new workflows. Also, the package features take advantage of central community S4
classes of the Bioconductor ecosystem and command-line-based software support.&lt;/p>
&lt;p>The main motivation and advantages of using &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> for complex data analysis tasks are:&lt;/p>
&lt;ol>
&lt;li>Facilitates the design of complex workflows involving multiple R/Bioconductor packages&lt;/li>
&lt;li>Common workflow interface for different applications&lt;/li>
&lt;li>Makes analysis with Bioconductor utilities more accessible to new users&lt;/li>
&lt;li>Simplifies usage of command-line software from within R&lt;/li>
&lt;li>Reduces the complexity of using compute clusters for R and command-line software&lt;/li>
&lt;li>Accelerates runtime of workflows via parallelization on computer systems with multiple CPU cores and/or multiple compute nodes&lt;/li>
&lt;li>Improves reproducibility by automating analyses and generation of analysis reports&lt;/li>
&lt;/ol>
&lt;div class="figure" style="text-align: center">
&lt;p>&lt;img src="utilities.png" alt="Relevant features in `systemPipeR`. Workflow design concepts are illustrated under (A &amp;amp; B). Examples of *systemPipeR's* visualization functionalities are given under (C)." width="100%" />&lt;/p>
&lt;p class="caption">
Figure 1: Relevant features in `systemPipeR`. Workflow design concepts are illustrated under (A &amp; B). Examples of *systemPipeR’s* visualization functionalities are given under (C).
&lt;/p>
&lt;/div>
&lt;p>A central concept for designing workflows within the &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> environment
is the use of workflow management containers.
Workflow management containers allow the automation of design, build, run and
scale different steps and tools in data analysis.
&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> adopted the widely used community standard &lt;a href="https://www.commonwl.org/">Common Workflow Language&lt;/a> (CWL)
(Amstutz et al. 2016) for describing parameters analysis workflows in a generic and reproducible
manner. Using this community standard in &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>
has many advantages. For instance, the integration of CWL allows running &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>
workflows from a single specification instance either entirely from within R, from various command-line wrappers (e.g., &lt;em>cwl-runner&lt;/em>) or from other languages (&lt;em>, e.g.,&lt;/em> Bash or Python).
&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> includes support for both command-line and R/Bioconductor software
as well as resources for containerization, parallel evaluations on computer clusters
along with the automated generation of interactive analysis reports.&lt;/p>
&lt;p>An important feature of &lt;em>&lt;code>systemPipeR's&lt;/code>&lt;/em> CWL interface is that it provides two
options to run command-line tools and workflows based on CWL. First, one can
run CWL in its native way via an R-based wrapper utility for &lt;em>cwl-runner&lt;/em> or
&lt;em>cwl-tools&lt;/em> (CWL-based approach). Second, one can run workflows using CWL’s
command-line and workflow instructions from within R (R-based approach). In the
latter case the same CWL workflow definition files (&lt;em>e.g.&lt;/em> &lt;code>*.cwl&lt;/code> and &lt;code>*.yml&lt;/code>)
are used but rendered and executed entirely with R functions defined by
&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>, and thus use CWL mainly as a command-line and workflow
definition format rather than software to run workflows. In this regard
&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> also provides several convenience functions that are useful for
designing and debugging workflows, such as a command-line rendering function to
retrieve the exact command-line strings for each data set and processing step
prior to running a command-line.&lt;/p>
&lt;p>This overview introduces the design of a workflow management container, an S4
class in &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>, as well as the custom command-line interface,
combined with the overview of all the common analysis steps of NGS experiments.&lt;/p>
&lt;h2 id="new-workflow-management-interface">New workflow management interface&lt;/h2>
&lt;p>&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> allows creation (multi-step analyses) and execution of workflow
entirely for R, with control, flexibility, and scalability of the all process.
The execution of the workflow can be sent to a HPC, can be parallelizes,
accelerating results acquisition.&lt;/p>
&lt;p>The flexibility of &lt;em>&lt;code>systemPipeR's&lt;/code>&lt;/em> new interface workflow management class is
the driving factor behind the use of as many steps necessary for the analysis,
as well as the connection between command-line- or R-based software. The
connectivity among all workflow steps is achieved by the &lt;code>SYSargsList&lt;/code> workflow
management class.&lt;/p>
&lt;p>&lt;code>SYSargsList&lt;/code> S4 class is a list-like container where each instance stores all the
input/output paths and parameter components required for a particular data
analysis step (see Figure &lt;a href="#fig:sysargslistImage">2&lt;/a>).&lt;/p>
&lt;p>The &lt;code>SYSargsList&lt;/code> constructor function will generate the instances, using as data
input initial targets files, as well as two-parameter files (for details, see below).
When running preconfigured workflows, the only input the user needs to provide
is the initial targets file containing the paths to the input files (e.g., FASTQ)
along with unique sample labels. Subsequent targets instances are created
automatically, based on the connectivity establish between the steps. The
parameters required for running command-line software is provided by the
parameter (&lt;code>*.cwl&lt;/code> and &lt;code>*.yml&lt;/code>)) files described below.&lt;/p>
&lt;p>The class store one or multiple steps, allowing central control for running,
checking status, and monitor complex workflows from start to finish. This design
enhances the systemPipeR workflow framework with a generalized, flexible, and
robust design.&lt;/p>
&lt;div class="figure" style="text-align: center">
&lt;p>&lt;img src="sysargslist.png" alt="Workflow steps with input/output file operations are controlled by `SYSargs2` objects. Each `SYSargs2`instance is constructed from one targets and two param files. The only input provided by the user is the initial targets file. Subsequent targets instances are created automatically, from the previous output files. Any number of predefined or custom workflow steps are supported. One or many `SYSargs2` objects are organized in an `SYSargsList` container." width="100%" />&lt;/p>
&lt;p class="caption">
Figure 2: Workflow steps with input/output file operations are controlled by `SYSargs2` objects. Each `SYSargs2`instance is constructed from one targets and two param files. The only input provided by the user is the initial targets file. Subsequent targets instances are created automatically, from the previous output files. Any number of predefined or custom workflow steps are supported. One or many `SYSargs2` objects are organized in an `SYSargsList` container.
&lt;/p>
&lt;/div>
&lt;h3 id="reference">Reference&lt;/h3>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-Amstutz2016-ka" class="csl-entry">
&lt;p>Amstutz, Peter, Michael R Crusoe, Nebojša Tijanić, Brad Chapman, John Chilton, Michael Heuer, Andrey Kartashov, et al. 2016. “Common Workflow Language, V1.0,” July. &lt;a href="https://doi.org/10.6084/m9.figshare.3115156.v2">https://doi.org/10.6084/m9.figshare.3115156.v2&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-H_Backman2016-bt" class="csl-entry">
&lt;p>H Backman, Tyler W, and Thomas Girke. 2016. “&lt;span class="nocase">systemPipeR: NGS workflow and report generation environment&lt;/span>.” &lt;em>BMC Bioinformatics&lt;/em> 17 (1): 388. &lt;a href="https://doi.org/10.1186/s12859-016-1241-0">https://doi.org/10.1186/s12859-016-1241-0&lt;/a>.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Sp: Getting Started</title><link>/sp/spr/gettingstarted/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/gettingstarted/</guid><description>
&lt;script src="../../rmarkdown-libs/htmlwidgets/htmlwidgets.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/datatables-binding/datatables.js">&lt;/script>
&lt;script src="../../rmarkdown-libs/jquery/jquery-3.6.0.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
&lt;link href="../../rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/dt-core/js/jquery.dataTables.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/dt-ext-fixedcolumns/css/fixedColumns.dataTables.min.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/dt-ext-fixedcolumns/js/dataTables.fixedColumns.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/dt-ext-scroller/css/scroller.dataTables.min.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/dt-ext-scroller/js/dataTables.scroller.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/crosstalk/js/crosstalk.min.js">&lt;/script>
&lt;script src="../../rmarkdown-libs/htmlwidgets/htmlwidgets.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/datatables-binding/datatables.js">&lt;/script>
&lt;script src="../../rmarkdown-libs/jquery/jquery-3.6.0.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
&lt;link href="../../rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/dt-core/js/jquery.dataTables.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/dt-ext-fixedcolumns/css/fixedColumns.dataTables.min.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/dt-ext-fixedcolumns/js/dataTables.fixedColumns.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/dt-ext-scroller/css/scroller.dataTables.min.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/dt-ext-scroller/js/dataTables.scroller.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/crosstalk/js/crosstalk.min.js">&lt;/script>
&lt;script src="../../rmarkdown-libs/htmlwidgets/htmlwidgets.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/datatables-binding/datatables.js">&lt;/script>
&lt;script src="../../rmarkdown-libs/jquery/jquery-3.6.0.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
&lt;link href="../../rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/dt-core/js/jquery.dataTables.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/dt-ext-fixedcolumns/css/fixedColumns.dataTables.min.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/dt-ext-fixedcolumns/js/dataTables.fixedColumns.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/dt-ext-scroller/css/scroller.dataTables.min.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/dt-ext-scroller/js/dataTables.scroller.min.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/crosstalk/js/crosstalk.min.js">&lt;/script>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
document.querySelector("h1").className = "title";
});
&lt;/script>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
var links = document.links;
for (var i = 0, linksLength = links.length; i &lt; linksLength; i++)
if (links[i].hostname != window.location.hostname)
links[i].target = '_blank';
});
&lt;/script>
&lt;h2 id="getting-started">Getting Started&lt;/h2>
&lt;h3 id="installation">Installation&lt;/h3>
&lt;p>&lt;a href="http://www.bioconductor.org/packages/devel/bioc/html/systemPipeR.html">&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>&lt;/a>
environment can be installed from the R console using the &lt;a href="https://cran.r-project.org/web/packages/BiocManager/index.html">&lt;em>&lt;code>BiocManager::install&lt;/code>&lt;/em>&lt;/a>
command. The associated data package &lt;a href="http://www.bioconductor.org/packages/devel/data/experiment/html/systemPipeRdata.html">&lt;em>&lt;code>systemPipeRdata&lt;/code>&lt;/em>&lt;/a>
can be installed the same way. The latter is a helper package for generating &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>
workflow environments with a single command containing all parameter files and
sample data required to quickly test and run workflows.&lt;/p>
&lt;pre>&lt;code class="language-r">if (!requireNamespace(&amp;quot;BiocManager&amp;quot;, quietly = TRUE)) install.packages(&amp;quot;BiocManager&amp;quot;)
BiocManager::install(&amp;quot;systemPipeR&amp;quot;)
BiocManager::install(&amp;quot;systemPipeRdata&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Please note that if you desire to use a third-party command-line tool, the particular
tool and dependencies need to be installed and exported in your PATH.
See &lt;a href="https://systempipe.org/sp/spr/sprinstall/">details&lt;/a>.&lt;/p>
&lt;h3 id="loading-package-and-documentation">Loading package and documentation&lt;/h3>
&lt;pre>&lt;code class="language-r">library(&amp;quot;systemPipeR&amp;quot;) # Loads the package
library(help = &amp;quot;systemPipeR&amp;quot;) # Lists package info
vignette(&amp;quot;systemPipeR&amp;quot;) # Opens vignette
&lt;/code>&lt;/pre>
&lt;h3 id="how-to-get-help-for-systempiper">How to get help for systemPipeR&lt;/h3>
&lt;p>All questions about the package or any particular function should be posted to
the Bioconductor support site &lt;a href="https://support.bioconductor.org">https://support.bioconductor.org&lt;/a>.&lt;/p>
&lt;p>Please add the “&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>” tag to your question, and the package authors will
automatically receive an alert.&lt;/p>
&lt;p>We appreciate receiving reports of bugs in the functions or documentation and
suggestions for improvement. For that, please consider opening an issue at
&lt;a href="https://github.com/tgirke/systemPipeR/issues/new">GitHub&lt;/a>.&lt;/p>
&lt;h2 id="project-structure">Project structure&lt;/h2>
&lt;p>&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> expects a project directory structure that consists of a directory
where users may store all the raw data, the results directory that will be reserved
for all the outfiles files or new output folders, and the parameters directory.&lt;/p>
&lt;p>This structure allows reproducibility and collaboration across the data science
team since internally relative paths are used. Users could transfer this project
to a different location and still be able to run the entire workflow. Also, it
increases efficiency and data management once the raw data is kept in a separate
folder and avoids duplication.&lt;/p>
&lt;h3 id="directory-structure">Directory Structure&lt;/h3>
&lt;p>&lt;a href="http://bioconductor.org/packages/devel/data/experiment/html/systemPipeRdata.html">&lt;em>&lt;code>systemPipeRdata&lt;/code>&lt;/em>&lt;/a>,
helper package, provides pre-configured workflows, reporting
templates, and sample data loaded as demonstrated below. With a single command,
the package allows creating the workflow environment containing the structure
described here (see Figure &lt;a href="#fig:dir">1&lt;/a>).&lt;/p>
&lt;p>Directory names are indicated in &lt;span style="color:grey">&lt;em>&lt;strong>green&lt;/strong>&lt;/em>&lt;/span>.
Users can change this structure as needed, but need to adjust the code in their
workflows accordingly.&lt;/p>
&lt;ul>
&lt;li>&lt;span style="color:green">&lt;em>&lt;strong>workflow/&lt;/strong>&lt;/em>&lt;/span> (&lt;em>e.g.&lt;/em> &lt;em>myproject/&lt;/em>)
&lt;ul>
&lt;li>This is the root directory of the R session running the workflow.&lt;/li>
&lt;li>Run script ( &lt;em>*.Rmd&lt;/em>) and sample annotation (&lt;em>targets.txt&lt;/em>) files are located here.&lt;/li>
&lt;li>Note, this directory can have any name (&lt;em>e.g.&lt;/em> &lt;span style="color:green">&lt;em>&lt;strong>myproject&lt;/strong>&lt;/em>&lt;/span>). Changing its name does not require any modifications in the run script(s).&lt;/li>
&lt;li>&lt;strong>Important subdirectories&lt;/strong>:
&lt;ul>
&lt;li>&lt;span style="color:green">&lt;em>&lt;strong>param/&lt;/strong>&lt;/em>&lt;/span>
&lt;ul>
&lt;li>&lt;span style="color:green">&lt;em>&lt;strong>param/cwl/&lt;/strong>&lt;/em>&lt;/span>: This subdirectory stores all the parameter and configuration files. To organize workflows, each can have its own subdirectory, where all &lt;code>*.cwl&lt;/code> and &lt;code>*input.yml&lt;/code> files need to be in the same subdirectory.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;span style="color:green">&lt;em>&lt;strong>data/&lt;/strong>&lt;/em> &lt;/span>
&lt;ul>
&lt;li>Raw data (&lt;em>e.g.&lt;/em> FASTQ files)&lt;/li>
&lt;li>FASTA file of reference (&lt;em>e.g.&lt;/em> reference genome)&lt;/li>
&lt;li>Annotation files&lt;/li>
&lt;li>Metadata&lt;/li>
&lt;li>etc.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;span style="color:green">&lt;em>&lt;strong>results/&lt;/strong>&lt;/em>&lt;/span>
&lt;ul>
&lt;li>Analysis results are usually written to this directory, including: alignment, variant and peak files (BAM, VCF, BED); tabular result files; and image/plot files&lt;/li>
&lt;li>Note, the user has the option to organize results files for a given sample and analysis step in a separate subdirectory.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="figure" style="text-align: center">
&lt;p>&lt;img src="spr_project.png" alt="systemPipeR's preconfigured directory structure." width="100%" />&lt;/p>
&lt;p class="caption">
Figure 1: systemPipeR’s preconfigured directory structure.
&lt;/p>
&lt;/div>
&lt;p>The following parameter files are included in each workflow template:&lt;/p>
&lt;ol>
&lt;li>&lt;em>&lt;code>targets.txt&lt;/code>&lt;/em>: initial one provided by user; downstream &lt;em>&lt;code>targets_*.txt&lt;/code>&lt;/em> files are generated automatically&lt;/li>
&lt;li>&lt;em>&lt;code>*.param/cwl&lt;/code>&lt;/em>: defines parameter for input/output file operations, &lt;em>e.g.&lt;/em>:
&lt;ul>
&lt;li>&lt;em>&lt;code>hisat2/hisat2-mapping-se.cwl&lt;/code>&lt;/em>&lt;/li>
&lt;li>&lt;em>&lt;code>hisat2/hisat2-mapping-se.yml&lt;/code>&lt;/em>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;code>*_run.sh&lt;/code>&lt;/em>: optional bash scripts&lt;/li>
&lt;li>Configuration files for computer cluster environments (skip on single machines):
&lt;ul>
&lt;li>&lt;em>&lt;code>.batchtools.conf.R&lt;/code>&lt;/em>: defines the type of scheduler for &lt;em>&lt;code>batchtools&lt;/code>&lt;/em> pointing to template file of cluster, and located in user’s home directory&lt;/li>
&lt;li>&lt;em>&lt;code>batchtools.*.tmpl&lt;/code>&lt;/em>: specifies parameters of scheduler used by a system, &lt;em>e.g.&lt;/em> Torque, SGE, Slurm, etc.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="structure-of-initial-targets-file">Structure of initial &lt;em>&lt;code>targets&lt;/code>&lt;/em> file&lt;/h3>
&lt;p>The &lt;em>&lt;code>targets&lt;/code>&lt;/em> file defines all input files (&lt;em>e.g.&lt;/em> FASTQ, BAM, BCF) and sample
comparisons of an analysis workflow. The following shows the format of a sample
&lt;em>&lt;code>targets&lt;/code>&lt;/em> file included in the package. It also can be viewed and downloaded
from &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>’s GitHub repository &lt;a href="https://github.com/tgirke/systemPipeR/blob/master/inst/extdata/targets.txt">here&lt;/a>.
In a target file with a single type of input files, here FASTQ files of
single-end (SE) reads, the first column describe the path and the second column
represents a unique id name for each sample. The third column called &lt;code>Factor&lt;/code>
represents the biological replicates. All subsequent columns are additional
information, and any number of extra columns can be added as needed.&lt;/p>
&lt;p>Users should note here, the usage of targets files is optional when using
&lt;em>&lt;code>systemPipeR's&lt;/code>&lt;/em> new workflow management interface. They can be replaced by a standard YAML
input file used by CWL. Since for organizing experimental variables targets
files are extremely useful and user-friendly. Thus, we encourage users to keep using
them.&lt;/p>
&lt;h4 id="structure-of-targets-file-for-single-end-se-samples">Structure of &lt;em>&lt;code>targets&lt;/code>&lt;/em> file for single-end (SE) samples&lt;/h4>
&lt;pre>&lt;code class="language-r">targetspath &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targets.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
showDF(read.delim(targetspath, comment.char = &amp;quot;#&amp;quot;))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Loading required namespace: DT
&lt;/code>&lt;/pre>
&lt;div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","vertical":false,"extensions":["FixedColumns","Scroller"],"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18"],["./data/SRR446027_1.fastq.gz","./data/SRR446028_1.fastq.gz","./data/SRR446029_1.fastq.gz","./data/SRR446030_1.fastq.gz","./data/SRR446031_1.fastq.gz","./data/SRR446032_1.fastq.gz","./data/SRR446033_1.fastq.gz","./data/SRR446034_1.fastq.gz","./data/SRR446035_1.fastq.gz","./data/SRR446036_1.fastq.gz","./data/SRR446037_1.fastq.gz","./data/SRR446038_1.fastq.gz","./data/SRR446039_1.fastq.gz","./data/SRR446040_1.fastq.gz","./data/SRR446041_1.fastq.gz","./data/SRR446042_1.fastq.gz","./data/SRR446043_1.fastq.gz","./data/SRR446044_1.fastq.gz"],["M1A","M1B","A1A","A1B","V1A","V1B","M6A","M6B","A6A","A6B","V6A","V6B","M12A","M12B","A12A","A12B","V12A","V12B"],["M1","M1","A1","A1","V1","V1","M6","M6","A6","A6","V6","V6","M12","M12","A12","A12","V12","V12"],["Mock.1h.A","Mock.1h.B","Avr.1h.A","Avr.1h.B","Vir.1h.A","Vir.1h.B","Mock.6h.A","Mock.6h.B","Avr.6h.A","Avr.6h.B","Vir.6h.A","Vir.6h.B","Mock.12h.A","Mock.12h.B","Avr.12h.A","Avr.12h.B","Vir.12h.A","Vir.12h.B"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],["23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012"]],"container":"&lt;table class=\"display\">\n &lt;thead>\n &lt;tr>\n &lt;th> &lt;\/th>\n &lt;th>FileName&lt;\/th>\n &lt;th>SampleName&lt;\/th>\n &lt;th>Factor&lt;\/th>\n &lt;th>SampleLong&lt;\/th>\n &lt;th>Experiment&lt;\/th>\n &lt;th>Date&lt;\/th>\n &lt;\/tr>\n &lt;\/thead>\n&lt;\/table>","options":{"scrollX":true,"fixedColumns":true,"deferRender":true,"scrollY":200,"scroller":true,"columnDefs":[{"className":"dt-right","targets":5},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p>To work with custom data, users need to generate a &lt;em>&lt;code>targets&lt;/code>&lt;/em> file containing
the paths to their own FASTQ files and then provide under &lt;em>&lt;code>targetspath&lt;/code>&lt;/em> the
path to the corresponding &lt;em>&lt;code>targets&lt;/code>&lt;/em> file.&lt;/p>
&lt;h4 id="structure-of-targets-file-for-paired-end-pe-samples">Structure of &lt;em>&lt;code>targets&lt;/code>&lt;/em> file for paired-end (PE) samples&lt;/h4>
&lt;p>For paired-end (PE) samples, the structure of the targets file is similar, where
users need to provide two FASTQ path columns: &lt;em>&lt;code>FileName1&lt;/code>&lt;/em> and &lt;em>&lt;code>FileName2&lt;/code>&lt;/em>
with the paths to the PE FASTQ files.&lt;/p>
&lt;pre>&lt;code class="language-r">targetspath &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targetsPE.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
showDF(read.delim(targetspath, comment.char = &amp;quot;#&amp;quot;))
&lt;/code>&lt;/pre>
&lt;div id="htmlwidget-2" style="width:100%;height:auto;" class="datatables html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-2">{"x":{"filter":"none","vertical":false,"extensions":["FixedColumns","Scroller"],"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18"],["./data/SRR446027_1.fastq.gz","./data/SRR446028_1.fastq.gz","./data/SRR446029_1.fastq.gz","./data/SRR446030_1.fastq.gz","./data/SRR446031_1.fastq.gz","./data/SRR446032_1.fastq.gz","./data/SRR446033_1.fastq.gz","./data/SRR446034_1.fastq.gz","./data/SRR446035_1.fastq.gz","./data/SRR446036_1.fastq.gz","./data/SRR446037_1.fastq.gz","./data/SRR446038_1.fastq.gz","./data/SRR446039_1.fastq.gz","./data/SRR446040_1.fastq.gz","./data/SRR446041_1.fastq.gz","./data/SRR446042_1.fastq.gz","./data/SRR446043_1.fastq.gz","./data/SRR446044_1.fastq.gz"],["./data/SRR446027_2.fastq.gz","./data/SRR446028_2.fastq.gz","./data/SRR446029_2.fastq.gz","./data/SRR446030_2.fastq.gz","./data/SRR446031_2.fastq.gz","./data/SRR446032_2.fastq.gz","./data/SRR446033_2.fastq.gz","./data/SRR446034_2.fastq.gz","./data/SRR446035_2.fastq.gz","./data/SRR446036_2.fastq.gz","./data/SRR446037_2.fastq.gz","./data/SRR446038_2.fastq.gz","./data/SRR446039_2.fastq.gz","./data/SRR446040_2.fastq.gz","./data/SRR446041_2.fastq.gz","./data/SRR446042_2.fastq.gz","./data/SRR446043_2.fastq.gz","./data/SRR446044_2.fastq.gz"],["M1A","M1B","A1A","A1B","V1A","V1B","M6A","M6B","A6A","A6B","V6A","V6B","M12A","M12B","A12A","A12B","V12A","V12B"],["M1","M1","A1","A1","V1","V1","M6","M6","A6","A6","V6","V6","M12","M12","A12","A12","V12","V12"],["Mock.1h.A","Mock.1h.B","Avr.1h.A","Avr.1h.B","Vir.1h.A","Vir.1h.B","Mock.6h.A","Mock.6h.B","Avr.6h.A","Avr.6h.B","Vir.6h.A","Vir.6h.B","Mock.12h.A","Mock.12h.B","Avr.12h.A","Avr.12h.B","Vir.12h.A","Vir.12h.B"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],["23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012","23-Mar-2012"]],"container":"&lt;table class=\"display\">\n &lt;thead>\n &lt;tr>\n &lt;th> &lt;\/th>\n &lt;th>FileName1&lt;\/th>\n &lt;th>FileName2&lt;\/th>\n &lt;th>SampleName&lt;\/th>\n &lt;th>Factor&lt;\/th>\n &lt;th>SampleLong&lt;\/th>\n &lt;th>Experiment&lt;\/th>\n &lt;th>Date&lt;\/th>\n &lt;\/tr>\n &lt;\/thead>\n&lt;\/table>","options":{"scrollX":true,"fixedColumns":true,"deferRender":true,"scrollY":200,"scroller":true,"columnDefs":[{"className":"dt-right","targets":6},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;h4 id="structure-of-targets-file-for-hello-world-example">Structure of &lt;em>&lt;code>targets&lt;/code>&lt;/em> file for “Hello World” example&lt;/h4>
&lt;p>In this example, &lt;em>&lt;code>targets&lt;/code>&lt;/em> file presents only two columns, which the first one
are the different phrases used by the &lt;code>echo&lt;/code> command-line and the second column
it is the sample &lt;code>id&lt;/code>. The &lt;code>id&lt;/code> column is required, and each sample id should be unique.&lt;/p>
&lt;pre>&lt;code class="language-r">targetspath &amp;lt;- system.file(&amp;quot;extdata/cwl/example/targets_example.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
showDF(read.delim(targetspath, comment.char = &amp;quot;#&amp;quot;))
&lt;/code>&lt;/pre>
&lt;div id="htmlwidget-3" style="width:100%;height:auto;" class="datatables html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-3">{"x":{"filter":"none","vertical":false,"extensions":["FixedColumns","Scroller"],"data":[["1","2","3"],["Hello World!","Hello USA!","Hello Bioconductor!"],["M1","M2","M3"]],"container":"&lt;table class=\"display\">\n &lt;thead>\n &lt;tr>\n &lt;th> &lt;\/th>\n &lt;th>Message&lt;\/th>\n &lt;th>SampleName&lt;\/th>\n &lt;\/tr>\n &lt;\/thead>\n&lt;\/table>","options":{"scrollX":true,"fixedColumns":true,"deferRender":true,"scrollY":200,"scroller":true,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;h4 id="sample-comparisons">Sample comparisons&lt;/h4>
&lt;p>Sample comparisons are defined in the header lines of the &lt;em>&lt;code>targets&lt;/code>&lt;/em> file
starting with ‘&lt;code># &amp;lt;CMP&amp;gt;&lt;/code>.’&lt;/p>
&lt;pre>&lt;code class="language-r">targetspath &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targetsPE.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
readLines(targetspath)[1:4]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;# Project ID: Arabidopsis - Pseudomonas alternative splicing study (SRA: SRP010938; PMID: 24098335)&amp;quot;
## [2] &amp;quot;# The following line(s) allow to specify the contrasts needed for comparative analyses, such as DEG identification. All possible comparisons can be specified with 'CMPset: ALL'.&amp;quot;
## [3] &amp;quot;# &amp;lt;CMP&amp;gt; CMPset1: M1-A1, M1-V1, A1-V1, M6-A6, M6-V6, A6-V6, M12-A12, M12-V12, A12-V12&amp;quot;
## [4] &amp;quot;# &amp;lt;CMP&amp;gt; CMPset2: ALL&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The function &lt;em>&lt;code>readComp&lt;/code>&lt;/em> imports the comparison information and stores it in a
&lt;em>&lt;code>list&lt;/code>&lt;/em>. Alternatively, &lt;em>&lt;code>readComp&lt;/code>&lt;/em> can obtain the comparison information from
the corresponding &lt;em>&lt;code>SYSargsList&lt;/code>&lt;/em> step (see below). Note, these header lines are
optional. They are mainly useful for controlling comparative analyses according
to certain biological expectations, such as identifying differentially expressed
genes in RNA-Seq experiments based on simple pair-wise comparisons.&lt;/p>
&lt;pre>&lt;code class="language-r">readComp(file = targetspath, format = &amp;quot;vector&amp;quot;, delim = &amp;quot;-&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $CMPset1
## [1] &amp;quot;M1-A1&amp;quot; &amp;quot;M1-V1&amp;quot; &amp;quot;A1-V1&amp;quot; &amp;quot;M6-A6&amp;quot; &amp;quot;M6-V6&amp;quot; &amp;quot;A6-V6&amp;quot; &amp;quot;M12-A12&amp;quot;
## [8] &amp;quot;M12-V12&amp;quot; &amp;quot;A12-V12&amp;quot;
##
## $CMPset2
## [1] &amp;quot;M1-A1&amp;quot; &amp;quot;M1-V1&amp;quot; &amp;quot;M1-M6&amp;quot; &amp;quot;M1-A6&amp;quot; &amp;quot;M1-V6&amp;quot; &amp;quot;M1-M12&amp;quot; &amp;quot;M1-A12&amp;quot;
## [8] &amp;quot;M1-V12&amp;quot; &amp;quot;A1-V1&amp;quot; &amp;quot;A1-M6&amp;quot; &amp;quot;A1-A6&amp;quot; &amp;quot;A1-V6&amp;quot; &amp;quot;A1-M12&amp;quot; &amp;quot;A1-A12&amp;quot;
## [15] &amp;quot;A1-V12&amp;quot; &amp;quot;V1-M6&amp;quot; &amp;quot;V1-A6&amp;quot; &amp;quot;V1-V6&amp;quot; &amp;quot;V1-M12&amp;quot; &amp;quot;V1-A12&amp;quot; &amp;quot;V1-V12&amp;quot;
## [22] &amp;quot;M6-A6&amp;quot; &amp;quot;M6-V6&amp;quot; &amp;quot;M6-M12&amp;quot; &amp;quot;M6-A12&amp;quot; &amp;quot;M6-V12&amp;quot; &amp;quot;A6-V6&amp;quot; &amp;quot;A6-M12&amp;quot;
## [29] &amp;quot;A6-A12&amp;quot; &amp;quot;A6-V12&amp;quot; &amp;quot;V6-M12&amp;quot; &amp;quot;V6-A12&amp;quot; &amp;quot;V6-V12&amp;quot; &amp;quot;M12-A12&amp;quot; &amp;quot;M12-V12&amp;quot;
## [36] &amp;quot;A12-V12&amp;quot;
&lt;/code>&lt;/pre>
&lt;h3 id="downstream-targets-files-description">Downstream targets files description&lt;/h3>
&lt;p>After the step which required the initial targets file information, the downstream
targets files are created automatically (see Figure &lt;a href="#fig:targetsFig">2&lt;/a>).
Each step that uses the previous step outfiles as an input, the new targets input
will be managed internally by the workflow instances, establishing connectivity
among the steps in the workflow.
&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> provides features to automatically and systematically build this
connection, providing security that all the samples will be managed efficiently
and reproducibly.&lt;/p>
&lt;div class="figure" style="text-align: center">
&lt;p>&lt;img src="targets_con.png" alt="_systemPipeR_ automatically creates the downstream `targets` files based on the previous steps outfiles. A) Usually, users provide the initial `targets` files, and this step will generate some outfiles, as demonstrated on B. Then, those files are used to build the new `targets` files as inputs in the next step. _`systemPipeR`_ (C) manages this connectivity among the steps automatically for the users." width="100%" />&lt;/p>
&lt;p class="caption">
Figure 2: *systemPipeR* automatically creates the downstream `targets` files based on the previous steps outfiles. A) Usually, users provide the initial `targets` files, and this step will generate some outfiles, as demonstrated on B. Then, those files are used to build the new `targets` files as inputs in the next step. *`systemPipeR`* (C) manages this connectivity among the steps automatically for the users.
&lt;/p>
&lt;/div>
&lt;h2 id="structure-of-the-new-parameters-files">Structure of the new parameters files&lt;/h2>
&lt;p>The parameters and configuration required for running command-line software are
provided by the widely used community standard &lt;a href="https://www.commonwl.org/">Common Workflow Language&lt;/a> (CWL) (Amstutz et al. 2016), which describes
parameters analysis workflows in a generic and reproducible manner.
For R-based workflow steps, param files are not required.
For a complete overview of the CWL syntax, please see this &lt;a href="https://systempipe.org/sp/spr/cwl_syntax/">section&lt;/a>.
Also, we have a dedicated section explain how to &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> establish the
connection between the CWL parameters files and the targets files. Please see &lt;a href="https://systempipe.org/sp/spr/cwl_and_spr">here&lt;/a>.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-Amstutz2016-ka" class="csl-entry">
&lt;p>Amstutz, Peter, Michael R Crusoe, Nebojša Tijanić, Brad Chapman, John Chilton, Michael Heuer, Andrey Kartashov, et al. 2016. “Common Workflow Language, V1.0,” July. &lt;a href="https://doi.org/10.6084/m9.figshare.3115156.v2">https://doi.org/10.6084/m9.figshare.3115156.v2&lt;/a>.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Sp: How to run a Workflow</title><link>/sp/spr/spr_run/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/spr_run/</guid><description>
&lt;script src="../../rmarkdown-libs/htmlwidgets/htmlwidgets.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/plotwf/plotwf.css" rel="stylesheet" />
&lt;script src="../../rmarkdown-libs/plotwf/viz.js">&lt;/script>
&lt;script src="../../rmarkdown-libs/plotwf/full.render.js">&lt;/script>
&lt;script src="../../rmarkdown-libs/plotwf-binding/plotwf.js">&lt;/script>
&lt;!--
- Compile from command-line
Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::html_document'), clean=F); knitr::knit('systemPipeR.Rmd', tangle=TRUE)"; Rscript ../md2jekyll.R systemPipeR.knit.md 2; Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::pdf_document'))"
-->
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
document.querySelector("h1").className = "title";
});
&lt;/script>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
var links = document.links;
for (var i = 0, linksLength = links.length; i &lt; linksLength; i++)
if (links[i].hostname != window.location.hostname)
links[i].target = '_blank';
});
&lt;/script>
&lt;h1 id="project-initialization">Project initialization&lt;/h1>
&lt;p>To create a Workflow within &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>, we can start by defining an empty
container and checking the directory structure:&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- SPRproject(projPath = tempdir())
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Creating directory: /tmp/RtmpzxxPO5/data
## Creating directory: /tmp/RtmpzxxPO5/param
## Creating directory: /tmp/RtmpzxxPO5/results
## Creating directory '/tmp/RtmpzxxPO5/.SPRproject'
## Creating file '/tmp/RtmpzxxPO5/.SPRproject/SYSargsList.yml'
## Your current working directory is different from the directory chosen for the Project Workflow.
## For accurate location of the files and running the Workflow, please set the working directory to
## 'setwd(/tmp/RtmpzxxPO5)'
&lt;/code>&lt;/pre>
&lt;p>Internally, &lt;code>SPRproject&lt;/code> function will create a hidden folder called &lt;code>.SPRproject&lt;/code>,
by default, to store all the log files.
A &lt;code>YAML&lt;/code> file, here called &lt;code>SYSargsList.yml&lt;/code>, has been created, which initially
contains the basic location of the project structure; however, every time the
workflow object &lt;code>sal&lt;/code> is updated in R, the new information will also be store in this
flat-file database for easy recovery.
If you desire different names for the logs folder and the &lt;code>YAML&lt;/code> file, these can
be modified as follows:&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- SPRproject(logs.dir = &amp;quot;.SPRproject&amp;quot;, sys.file = &amp;quot;.SPRproject/SYSargsList.yml&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Also, this function will check and/or create the basic folder structure if missing,
which means &lt;code>data&lt;/code>, &lt;code>param&lt;/code>, and &lt;code>results&lt;/code> folder, as described &lt;a href="#dir">here&lt;/a>.
If the user wants to use a different names for these directories, can be specified
as follows:&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- SPRproject(data = &amp;quot;data&amp;quot;, param = &amp;quot;param&amp;quot;, results = &amp;quot;results&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>It is possible to separate all the R objects created within the workflow analysis
from the current environment. &lt;code>SPRproject&lt;/code> function provides the option to create
a new environment, and in this way, it is not overwriting any object you may want
to have at your current section.&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- SPRproject(envir = new.env())
&lt;/code>&lt;/pre>
&lt;p>In this stage, the object &lt;code>sal&lt;/code> is a empty container, except for the project information. The project information can be accessed by the &lt;code>projectInfo&lt;/code> method:&lt;/p>
&lt;pre>&lt;code class="language-r">sal
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## No workflow steps added
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">projectInfo(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $project
## [1] &amp;quot;/home/dcassol/danielac@ucr.edu/projects/SP/SPR_org/systemPipeR.github.io_docsy/content/en/sp/spr&amp;quot;
##
## $data
## [1] &amp;quot;data&amp;quot;
##
## $param
## [1] &amp;quot;param&amp;quot;
##
## $results
## [1] &amp;quot;results&amp;quot;
##
## $logsDir
## [1] &amp;quot;.SPRproject&amp;quot;
##
## $sysargslist
## [1] &amp;quot;.SPRproject/SYSargsList.yml&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Also, the &lt;code>length&lt;/code> function will return how many steps this workflow contains and
in this case it is empty, as follow:&lt;/p>
&lt;pre>&lt;code class="language-r">length(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0
&lt;/code>&lt;/pre>
&lt;h1 id="workflow-design">Workflow Design&lt;/h1>
&lt;p>&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> workflows can be designed and built from start to finish with a
single command, importing from an R Markdown file or stepwise in interactive
mode from the R console.
In the &lt;a href="#appendstep">next section&lt;/a>, we will demonstrate how to build the workflow in an
interactive mode, and in the &lt;a href="#importWF">following section&lt;/a>, we will show how to build from a
file.&lt;/p>
&lt;p>New workflows are constructed, or existing ones modified, by connecting each
step via &lt;code>appendStep&lt;/code> method. Each &lt;code>SYSargsList&lt;/code> instance contains instructions
needed for processing a set of input files with a specific command-line or R
software, as well as the paths to the corresponding outfiles generated by a
particular tool/step.&lt;/p>
&lt;p>To build R code based step, the constructor function &lt;code>Linewise&lt;/code> is used.
For more details about this S4 class container, see &lt;a href="#linewise">here&lt;/a>.&lt;/p>
&lt;h2 id="build-workflow-interactive">Build workflow interactive&lt;/h2>
&lt;p>This tutorial shows a very simple example for describing and explaining all main
features available within systemPipeR to design, build, manage, run, and
visualize the workflow. In summary, we are exporting a dataset to multiple
files, compressing and decompressing each one of the files, and importing to R,
and finally performing a statistical analysis.&lt;/p>
&lt;p>In the previous section, we initialize the project by building the &lt;code>sal&lt;/code> object.
Until this moment, the container has no steps:&lt;/p>
&lt;pre>&lt;code class="language-r">sal
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## No workflow steps added
&lt;/code>&lt;/pre>
&lt;p>Next, we need to populate the object created with the first step in the
workflow.&lt;/p>
&lt;h3 id="adding-the-first-step">Adding the first step&lt;/h3>
&lt;p>The first step is R code based, and we are splitting the &lt;code>iris&lt;/code> dataset by &lt;code>Species&lt;/code>
and for each &lt;code>Species&lt;/code> will be saved on file. Please note that this code will
not be executed now; it is just store in the container for further execution.&lt;/p>
&lt;p>This constructor function requires the &lt;code>step_name&lt;/code> and the R-based code under
the &lt;code>code&lt;/code> argument.
The R code should be enclosed by braces (&lt;code>{}&lt;/code>) and separated by a new line.&lt;/p>
&lt;pre>&lt;code class="language-r">appendStep(sal) &amp;lt;- LineWise(code = {
mapply(function(x, y) write.csv(x, y), split(iris, factor(iris$Species)), file.path(&amp;quot;results&amp;quot;,
paste0(names(split(iris, factor(iris$Species))), &amp;quot;.csv&amp;quot;)))
}, step_name = &amp;quot;export_iris&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>For a brief overview of the workflow, we can check the object as follows:&lt;/p>
&lt;pre>&lt;code class="language-r">sal
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. export_iris --&amp;gt; Status: Pending
##
&lt;/code>&lt;/pre>
&lt;p>Also, for printing and double-check the R code in the step, we can use the
&lt;code>codeLine&lt;/code> method:&lt;/p>
&lt;pre>&lt;code class="language-r">codeLine(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## export_iris
## mapply(function(x, y) write.csv(x, y), split(iris, factor(iris$Species)), file.path(&amp;quot;results&amp;quot;, paste0(names(split(iris, factor(iris$Species))), &amp;quot;.csv&amp;quot;)))
&lt;/code>&lt;/pre>
&lt;h3 id="adding-more-steps">Adding more steps&lt;/h3>
&lt;p>Next, an example of how to compress the exported files using
&lt;a href="https://www.gnu.org/software/gzip/">&lt;code>gzip&lt;/code>&lt;/a> command-line.&lt;/p>
&lt;p>The constructor function creates an &lt;code>SYSargsList&lt;/code> S4 class object using data from
three input files:&lt;/p>
&lt;pre>&lt;code>- CWL command-line specification file (`wf_file` argument);
- Input variables (`input_file` argument);
- Targets file (`targets` argument).
&lt;/code>&lt;/pre>
&lt;p>In CWL, files with the extension &lt;code>.cwl&lt;/code> define the parameters of a chosen
command-line step or workflow, while files with the extension &lt;code>.yml&lt;/code> define the
input variables of command-line steps.&lt;/p>
&lt;p>The &lt;code>targets&lt;/code> file is optional for workflow steps lacking &lt;code>input&lt;/code> files. The connection
between &lt;code>input&lt;/code> variables and the &lt;code>targets&lt;/code> file is defined under the &lt;code>inputvars&lt;/code>
argument. It is required a &lt;code>named vector&lt;/code>, where each element name needs to match
with column names in the &lt;code>targets&lt;/code> file, and the value must match the names of
the &lt;code>input&lt;/code> variables defined in the &lt;code>*.yml&lt;/code> files (see Figure &lt;a href="#fig:sprCWL">&lt;strong>??&lt;/strong>&lt;/a>).&lt;/p>
&lt;p>A detailed description of the dynamic between &lt;code>input&lt;/code> variables and &lt;code>targets&lt;/code>
files can be found &lt;a href="#cwl_targets">here&lt;/a>.
In addition, the CWL syntax overview can be found &lt;a href="#cwl">here&lt;/a>.&lt;/p>
&lt;p>Besides all the data form &lt;code>targets&lt;/code>, &lt;code>wf_file&lt;/code>, &lt;code>input_file&lt;/code> and &lt;code>dir_path&lt;/code> arguments,
&lt;code>SYSargsList&lt;/code> constructor function options include:&lt;/p>
&lt;ul>
&lt;li>&lt;code>step_name&lt;/code>: a unique &lt;em>name&lt;/em> for the step. This is not mandatory; however,
it is highly recommended. If no name is provided, a default &lt;code>step_x&lt;/code>, where
&lt;code>x&lt;/code> reflects the step index, will be added.&lt;/li>
&lt;li>&lt;code>dir&lt;/code>: this option allows creating an exclusive subdirectory for the step
in the workflow. All the outfiles and log files for this particular step will
be generated in the respective folders.&lt;/li>
&lt;li>&lt;code>dependency&lt;/code>: after the first step, all the additional steps appended to
the workflow require the information of the dependency tree.&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>appendStep&amp;lt;-&lt;/code> method is used to append a new step in the workflow.&lt;/p>
&lt;pre>&lt;code class="language-r">targetspath &amp;lt;- system.file(&amp;quot;extdata/cwl/gunzip&amp;quot;, &amp;quot;targets_gunzip.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
appendStep(sal) &amp;lt;- SYSargsList(step_name = &amp;quot;gzip&amp;quot;, targets = targetspath, dir = TRUE,
wf_file = &amp;quot;gunzip/workflow_gzip.cwl&amp;quot;, input_file = &amp;quot;gunzip/gzip.yml&amp;quot;, dir_path = system.file(&amp;quot;extdata/cwl&amp;quot;,
package = &amp;quot;systemPipeR&amp;quot;), inputvars = c(FileName = &amp;quot;_FILE_PATH_&amp;quot;, SampleName = &amp;quot;_SampleName_&amp;quot;),
dependency = &amp;quot;export_iris&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Note: This will not work if the &lt;code>gzip&lt;/code> is not available on your system
(installed and exported to PATH) and may only work on Windows systems using PowerShell.&lt;/p>
&lt;p>For a overview of the workflow, we can check the object as follows:&lt;/p>
&lt;pre>&lt;code class="language-r">sal
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. export_iris --&amp;gt; Status: Pending
## 2. gzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 2.1. gzip
## cmdlist: 3 | Pending: 3
##
&lt;/code>&lt;/pre>
&lt;p>Note that we have two steps, and it is expected three files from the second step.
Also, the workflow status is &lt;em>Pending&lt;/em>, which means the workflow object is
rendered in R; however, we did not execute the workflow yet.
In addition to this summary, it can be observed this step has three command lines.&lt;/p>
&lt;p>For more details about the command-line rendered for each target file, it can be
checked as follows:&lt;/p>
&lt;pre>&lt;code class="language-r">cmdlist(sal, step = &amp;quot;gzip&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $gzip
## $gzip$SE
## $gzip$SE$gzip
## [1] &amp;quot;gzip -c results/setosa.csv &amp;gt; results/SE.csv.gz&amp;quot;
##
##
## $gzip$VE
## $gzip$VE$gzip
## [1] &amp;quot;gzip -c results/versicolor.csv &amp;gt; results/VE.csv.gz&amp;quot;
##
##
## $gzip$VI
## $gzip$VI$gzip
## [1] &amp;quot;gzip -c results/virginica.csv &amp;gt; results/VI.csv.gz&amp;quot;
&lt;/code>&lt;/pre>
&lt;h4 id="using-the-outfiles-for-the-next-step">Using the &lt;code>outfiles&lt;/code> for the next step&lt;/h4>
&lt;p>For building this step, all the previous procedures are being used to append the
next step. However, here, we can observe power features that build the
connectivity between steps in the workflow.&lt;/p>
&lt;p>In this example, we would like to use the outfiles from &lt;em>gzip&lt;/em> Step, as
input from the next step, which is the &lt;em>gunzip&lt;/em>. In this case, let’s look at the
outfiles from the first step:&lt;/p>
&lt;pre>&lt;code class="language-r">outfiles(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## DataFrame with 0 rows and 0 columns
##
## $gzip
## DataFrame with 3 rows and 1 column
## gzip_file
## &amp;lt;character&amp;gt;
## 1 results/SE.csv.gz
## 2 results/VE.csv.gz
## 3 results/VI.csv.gz
&lt;/code>&lt;/pre>
&lt;p>The column we want to use is “gzip_file.” For the argument &lt;code>targets&lt;/code> in the
&lt;code>SYSargsList&lt;/code> function, it should provide the name of the correspondent step in
the Workflow and which &lt;code>outfiles&lt;/code> you would like to be incorporated in the next
step.
The argument &lt;code>inputvars&lt;/code> allows the connectivity between &lt;code>outfiles&lt;/code> and the
new &lt;code>targets&lt;/code> file. Here, the name of the previous &lt;code>outfiles&lt;/code> should be provided
it. Please note that all &lt;code>outfiles&lt;/code> column names must be unique.&lt;/p>
&lt;p>It is possible to keep all the original columns from the &lt;code>targets&lt;/code> files or remove
some columns for a clean &lt;code>targets&lt;/code> file.
The argument &lt;code>rm_targets_col&lt;/code> provides this flexibility, where it is possible to
specify the names of the columns that should be removed. If no names are passing
here, the new columns will be appended.&lt;/p>
&lt;pre>&lt;code class="language-r">appendStep(sal) &amp;lt;- SYSargsList(step_name = &amp;quot;gunzip&amp;quot;, targets = &amp;quot;gzip&amp;quot;, dir = TRUE,
wf_file = &amp;quot;gunzip/workflow_gunzip.cwl&amp;quot;, input_file = &amp;quot;gunzip/gunzip.yml&amp;quot;, dir_path = system.file(&amp;quot;extdata/cwl&amp;quot;,
package = &amp;quot;systemPipeR&amp;quot;), inputvars = c(gzip_file = &amp;quot;_FILE_PATH_&amp;quot;, SampleName = &amp;quot;_SampleName_&amp;quot;),
rm_targets_col = &amp;quot;FileName&amp;quot;, dependency = &amp;quot;gzip&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>We can check the targets automatically create for this step,
based on the previous &lt;code>outfiles&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-r">targetsWF(sal[3])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $gunzip
## DataFrame with 3 rows and 2 columns
## gzip_file SampleName
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 results/SE.csv.gz SE
## 2 results/VE.csv.gz VE
## 3 results/VI.csv.gz VI
&lt;/code>&lt;/pre>
&lt;p>We can also check all the expected &lt;code>outfiles&lt;/code> for this particular step, as follows:&lt;/p>
&lt;pre>&lt;code class="language-r">outfiles(sal[3])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $gunzip
## DataFrame with 3 rows and 1 column
## gunzip_file
## &amp;lt;character&amp;gt;
## 1 results/SE.csv
## 2 results/VE.csv
## 3 results/VI.csv
&lt;/code>&lt;/pre>
&lt;p>Now, we can observe that the third step has been added and contains one substep.&lt;/p>
&lt;pre>&lt;code class="language-r">sal
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. export_iris --&amp;gt; Status: Pending
## 2. gzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 2.1. gzip
## cmdlist: 3 | Pending: 3
## 3. gunzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 3.1. gunzip
## cmdlist: 3 | Pending: 3
##
&lt;/code>&lt;/pre>
&lt;p>In addition, we can access all the command lines for each one of the substeps.&lt;/p>
&lt;pre>&lt;code class="language-r">cmdlist(sal[&amp;quot;gzip&amp;quot;], targets = 1)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $gzip
## $gzip$SE
## $gzip$SE$gzip
## [1] &amp;quot;gzip -c results/setosa.csv &amp;gt; results/SE.csv.gz&amp;quot;
&lt;/code>&lt;/pre>
&lt;h4 id="getting-data-from-a-workflow-instance">Getting data from a workflow instance&lt;/h4>
&lt;p>The final step in this simple workflow is an R code step. For that, we are using
the &lt;code>LineWise&lt;/code> constructor function as demonstrated above.&lt;/p>
&lt;p>One interesting feature showed here is the &lt;code>getColumn&lt;/code> method that allows
extracting the information for a workflow instance. Those files can be used in
an R code, as demonstrated below.&lt;/p>
&lt;pre>&lt;code class="language-r">getColumn(sal, step = &amp;quot;gunzip&amp;quot;, &amp;quot;outfiles&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## SE VE VI
## &amp;quot;results/SE.csv&amp;quot; &amp;quot;results/VE.csv&amp;quot; &amp;quot;results/VI.csv&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">appendStep(sal) &amp;lt;- LineWise(code = {
df &amp;lt;- lapply(getColumn(sal, step = &amp;quot;gunzip&amp;quot;, &amp;quot;outfiles&amp;quot;), function(x) read.delim(x,
sep = &amp;quot;,&amp;quot;)[-1])
df &amp;lt;- do.call(rbind, df)
stats &amp;lt;- data.frame(cbind(mean = apply(df[, 1:4], 2, mean), sd = apply(df[, 1:4],
2, sd)))
stats$species &amp;lt;- rownames(stats)
plot &amp;lt;- ggplot2::ggplot(stats, ggplot2::aes(x = species, y = mean, fill = species)) +
ggplot2::geom_bar(stat = &amp;quot;identity&amp;quot;, color = &amp;quot;black&amp;quot;, position = ggplot2::position_dodge()) +
ggplot2::geom_errorbar(ggplot2::aes(ymin = mean - sd, ymax = mean + sd),
width = 0.2, position = ggplot2::position_dodge(0.9))
}, step_name = &amp;quot;iris_stats&amp;quot;, dependency = &amp;quot;gzip&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h2 id="build-workflow-from-a-r-markdown">Build workflow from a {R Markdown}&lt;/h2>
&lt;p>The precisely same workflow can be created by importing the steps from an
R Markdown file.
As demonstrated above, it is required to initialize the project with &lt;code>SPRproject&lt;/code> function.&lt;/p>
&lt;p>&lt;code>importWF&lt;/code> function will scan and import all the R chunk from the R Markdown file
and build all the workflow instances. Then, each R chuck in the file will be
converted in a workflow step.&lt;/p>
&lt;pre>&lt;code class="language-r">sal_rmd &amp;lt;- SPRproject(logs.dir = &amp;quot;.SPRproject_rmd&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Creating directory '/home/dcassol/danielac@ucr.edu/projects/SP/SPR_org/systemPipeR.github.io_docsy/content/en/sp/spr/.SPRproject_rmd'
## Creating file '/home/dcassol/danielac@ucr.edu/projects/SP/SPR_org/systemPipeR.github.io_docsy/content/en/sp/spr/.SPRproject_rmd/SYSargsList.yml'
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">sal_rmd &amp;lt;- importWF(sal_rmd, file_path = system.file(&amp;quot;extdata&amp;quot;, &amp;quot;spr_simple_wf.Rmd&amp;quot;,
package = &amp;quot;systemPipeR&amp;quot;))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Reading Rmd file
##
## ---- Actions ----
## Checking chunk SPR option
## Ignore non-SPR chunks: 17
## Checking chunk eval values
## Resolve step names
## Check duplicated step names
## Checking chunk dependencies
## Use the previous step as dependency for steps without 'spr.dep' options: 27
## Parse chunk code
## ---- Succes! Create output ----
## Now importing step 'export_iris'
## Now importing step 'gzip'
## Now importing step 'gunzip'
## Now importing step 'stats'
&lt;/code>&lt;/pre>
&lt;p>Let’s explore the workflow to check the steps:&lt;/p>
&lt;pre>&lt;code class="language-r">stepsWF(sal_rmd)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## Instance of 'LineWise'
## Code Chunk length: 1
##
## $gzip
## Instance of 'SYSargs2':
## Slot names/accessors:
## targets: 3 (SE...VI), targetsheader: 1 (lines)
## modules: 0
## wf: 1, clt: 1, yamlinput: 4 (inputs)
## input: 3, output: 3
## cmdlist: 3
## Sub Steps:
## 1. gzip (rendered: TRUE)
##
##
##
## $gunzip
## Instance of 'SYSargs2':
## Slot names/accessors:
## targets: 3 (SE...VI), targetsheader: 1 (lines)
## modules: 0
## wf: 1, clt: 1, yamlinput: 4 (inputs)
## input: 3, output: 3
## cmdlist: 3
## Sub Steps:
## 1. gunzip (rendered: TRUE)
##
##
##
## $stats
## Instance of 'LineWise'
## Code Chunk length: 5
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">dependency(sal_rmd)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## [1] &amp;quot;&amp;quot;
##
## $gzip
## [1] &amp;quot;export_iris&amp;quot;
##
## $gunzip
## [1] &amp;quot;gzip&amp;quot;
##
## $stats
## [1] &amp;quot;gunzip&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">codeLine(sal_rmd)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## gzip AND gunzip step have been dropped because it is not a LineWise object.
## export_iris
## mapply(function(x, y) write.csv(x, y), split(iris, factor(iris$Species)), file.path(&amp;quot;results&amp;quot;, paste0(names(split(iris, factor(iris$Species))), &amp;quot;.csv&amp;quot;)))
## stats
## df &amp;lt;- lapply(getColumn(sal, step = &amp;quot;gunzip&amp;quot;, &amp;quot;outfiles&amp;quot;), function(x) read.delim(x, sep = &amp;quot;,&amp;quot;)[-1])
## df &amp;lt;- do.call(rbind, df)
## stats &amp;lt;- data.frame(cbind(mean = apply(df[, 1:4], 2, mean), sd = apply(df[, 1:4], 2, sd)))
## stats$species &amp;lt;- rownames(stats)
## plot &amp;lt;- ggplot2::ggplot(stats, ggplot2::aes(x = species, y = mean, fill = species)) + ggplot2::geom_bar(stat = &amp;quot;identity&amp;quot;, color = &amp;quot;black&amp;quot;, position = ggplot2::position_dodge()) + ggplot2::geom_errorbar(ggplot2::aes(ymin = mean - sd, ymax = mean + sd), width = 0.2, position = ggplot2::position_dodge(0.9))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">targetsWF(sal_rmd)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## DataFrame with 0 rows and 0 columns
##
## $gzip
## DataFrame with 3 rows and 2 columns
## FileName SampleName
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 results/setosa.csv SE
## 2 results/versicolor.csv VE
## 3 results/virginica.csv VI
##
## $gunzip
## DataFrame with 3 rows and 2 columns
## gzip_file SampleName
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 results/SE.csv.gz SE
## 2 results/VE.csv.gz VE
## 3 results/VI.csv.gz VI
##
## $stats
## DataFrame with 0 rows and 0 columns
&lt;/code>&lt;/pre>
&lt;h3 id="rules-to-create-the-r-markdown-to-import-as-workflow">Rules to create the R Markdown to import as workflow&lt;/h3>
&lt;p>To include a particular code chunk from the R Markdown file in the workflow
analysis, please use the following code chunk options:&lt;/p>
&lt;pre>&lt;code>- `spr='r'`: for code chunks with R code lines;
- `spr='sysargs'`: for code chunks with an `SYSargsList` object;
- `spr.dep=&amp;lt;StepName&amp;gt;`: for specify the previous dependency.
&lt;/code>&lt;/pre>
&lt;p>For example:&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>```{r step_1, eval=TRUE, spr=‘r,’ spr.dep=‘step_0’}&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>```{r step_2, eval=TRUE, spr=‘sysargs,’ spr.dep=‘step_1’}&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>For &lt;code>spr = 'sysargs'&lt;/code>, the last object assigned must to be the &lt;code>SYSargsList&lt;/code>, for example:&lt;/p>
&lt;pre>&lt;code class="language-r">targetspath &amp;lt;- system.file(&amp;quot;extdata/cwl/example/targets_example.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
HW_mul &amp;lt;- SYSargsList(step_name = &amp;quot;Example&amp;quot;, targets = targetspath, wf_file = &amp;quot;example/example.cwl&amp;quot;,
input_file = &amp;quot;example/example.yml&amp;quot;, dir_path = system.file(&amp;quot;extdata/cwl&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;),
inputvars = c(Message = &amp;quot;_STRING_&amp;quot;, SampleName = &amp;quot;_SAMPLE_&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>Also, note that all the required files or objects to generate one particular
command-line step must be defined in a R code chunk imported.
The motivation for this is that when R Markdown files are imported, the
&lt;code>spr = 'sysargs'&lt;/code> R chunk will be evaluated and stored in the workflow control
class as the &lt;code>SYSargsList&lt;/code> object, while the R code based (&lt;code>spr = 'r'&lt;/code>) is not
evaluated, and until the workflow is executed it will be store as an expression.&lt;/p>
&lt;h1 id="running-the-workflow">Running the workflow&lt;/h1>
&lt;p>For running the workflow, &lt;code>runWF&lt;/code> function will execute all the command lines
store in the workflow container.&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- runWF(sal)
&lt;/code>&lt;/pre>
&lt;p>This essential function allows the user to choose one or multiple steps to be
executed using the &lt;code>steps&lt;/code> argument. However, it is necessary to follow the
workflow dependency graph. If a selected step depends on a previous step(s) that
was not executed, the execution will fail.&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- runWF(sal, steps = c(1, 3))
&lt;/code>&lt;/pre>
&lt;p>Also, it allows forcing the execution of the steps, even if the status of the
step is &lt;code>'Success'&lt;/code> and all the expected &lt;code>outfiles&lt;/code> exists.
Another feature of the &lt;code>runWF&lt;/code> function is ignoring all the warnings
and errors and running the workflow by the arguments &lt;code>warning.stop&lt;/code> and
&lt;code>error.stop&lt;/code>, respectively.&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- runWF(sal, force = TRUE, warning.stop = FALSE, error.stop = TRUE)
&lt;/code>&lt;/pre>
&lt;p>When the project was initialized by &lt;code>SPRproject&lt;/code> function, it was created an
environment for all objects created during the workflow execution. This
environment can be accessed as follows:&lt;/p>
&lt;pre>&lt;code class="language-r">viewEnvir(sal)
&lt;/code>&lt;/pre>
&lt;p>The workflow execution allows to save this environment for future recovery:&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- runWF(sal, saveEnv = TRUE)
&lt;/code>&lt;/pre>
&lt;h2 id="workflow-status">Workflow status&lt;/h2>
&lt;p>To check the summary of the workflow, we can use:&lt;/p>
&lt;pre>&lt;code class="language-r">sal
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. export_iris --&amp;gt; Status: Pending
## 2. gzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 2.1. gzip
## cmdlist: 3 | Pending: 3
## 3. gunzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 3.1. gunzip
## cmdlist: 3 | Pending: 3
## 4. iris_stats --&amp;gt; Status: Pending
##
&lt;/code>&lt;/pre>
&lt;p>To access more details about the workflow instances, we can use the &lt;code>statusWF&lt;/code> method:&lt;/p>
&lt;pre>&lt;code class="language-r">statusWF(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## DataFrame with 1 row and 2 columns
## Step status.summary
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 export_iris Pending
##
## $gzip
## DataFrame with 3 rows and 5 columns
## Targets Total_Files Existing_Files Missing_Files gzip
## &amp;lt;character&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;factor&amp;gt;
## 1 SE 1 0 1 Pending
## 2 VE 1 0 1 Pending
## 3 VI 1 0 1 Pending
##
## $gunzip
## DataFrame with 3 rows and 5 columns
## Targets Total_Files Existing_Files Missing_Files gunzip
## &amp;lt;character&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;factor&amp;gt;
## 1 SE 1 0 1 Pending
## 2 VE 1 0 1 Pending
## 3 VI 1 0 1 Pending
##
## $iris_stats
## DataFrame with 1 row and 2 columns
## Step status.summary
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 iris_stats Pending
&lt;/code>&lt;/pre>
&lt;h2 id="parallelization-on-clusters">Parallelization on clusters&lt;/h2>
&lt;p>This section of the tutorial provides an introduction to the usage of the
&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> features on a cluster.&lt;/p>
&lt;p>The computation can be greatly accelerated by processing many files
in parallel using several compute nodes of a cluster, where a scheduling/queuing
system is used for load balancing. For this the &lt;code>clusterRun&lt;/code> function submits
the computing requests to the scheduler using the run specifications
defined by &lt;code>runWF&lt;/code>.&lt;/p>
&lt;p>A named list provides the computational resources. By default, it can be defined
the upper time limit in minutes for jobs before they get killed by the scheduler,
memory limit in Mb, number of &lt;code>CPUs&lt;/code>, and number of tasks.&lt;/p>
&lt;p>The number of independent parallel cluster processes is defined under the
&lt;code>Njobs&lt;/code> argument. The following example will run one process in parallel using
for each 4 CPU cores. If the resources available on a cluster allow running all
the processes simultaneously, then the shown sample submission will utilize in
total four CPU cores (&lt;code>NJobs * ncpus&lt;/code>). Note, &lt;code>clusterRun&lt;/code> can be used
with most queueing systems as it is based on utilities from the &lt;em>&lt;code>batchtools&lt;/code>&lt;/em>
package which supports the use of template files (&lt;em>&lt;code>*.tmpl&lt;/code>&lt;/em>) for defining the
run parameters of different schedulers. To run the following code, one needs to
have both a &lt;code>conf file&lt;/code> (see &lt;em>&lt;code>.batchtools.conf.R&lt;/code>&lt;/em> samples &lt;a href="https://mllg.github.io/batchtools/">here&lt;/a>)
and a template file (see &lt;em>&lt;code>*.tmpl&lt;/code>&lt;/em> samples &lt;a href="https://github.com/mllg/batchtools/tree/master/inst/templates">here&lt;/a>)
for the queueing available on a system. The following example uses the sample
&lt;code>conf&lt;/code> and &lt;code>template&lt;/code> files for the &lt;code>Slurm&lt;/code> scheduler provided by this package.&lt;/p>
&lt;pre>&lt;code class="language-r">library(batchtools)
resources &amp;lt;- list(walltime = 120, ntasks = 1, ncpus = 4, memory = 1024)
sal &amp;lt;- clusterRun(sal, FUN = runWF, more.args = list(), conffile = &amp;quot;.batchtools.conf.R&amp;quot;,
template = &amp;quot;batchtools.slurm.tmpl&amp;quot;, Njobs = 1, runid = &amp;quot;01&amp;quot;, resourceList = resources)
&lt;/code>&lt;/pre>
&lt;p>Note: The example is submitting the jog to &lt;code>short&lt;/code> partition. If you desire to
use a different partition, please adjust accordingly (&lt;code>batchtools.slurm.tmpl&lt;/code>).&lt;/p>
&lt;h1 id="visualize-workflow">Visualize workflow&lt;/h1>
&lt;p>&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> workflows instances can be visualized with the &lt;code>plotWF&lt;/code> function.&lt;/p>
&lt;p>This function will make a plot of selected workflow instance and the following
information is displayed on the plot:&lt;/p>
&lt;pre>&lt;code>- Workflow structure (dependency graphs between different steps);
- Workflow step status, *e.g.* `Success`, `Error`, `Pending`, `Warnings`;
- Sample status and statistics;
- Workflow timing: running duration time.
&lt;/code>&lt;/pre>
&lt;p>If no argument is provided, the basic plot will automatically detect width,
height, layout, plot method, branches, &lt;em>etc&lt;/em>.&lt;/p>
&lt;pre>&lt;code class="language-r">plotWF(sal, show_legend = TRUE, width = &amp;quot;80%&amp;quot;, rstudio = TRUE)
&lt;/code>&lt;/pre>
&lt;div id="htmlwidget-1" style="width:80%;height:480px;" class="plotwf html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-1">{"x":{"dot":"digraph {\n node[fontsize=20];\n subgraph {\n node[color=\"dodgerblue\"];\n export_iris -> gzip -> iris_stats[color=\"dodgerblue\"]\n }\n gzip -> gunzip\n \n export_iris[label=&lt;&lt;b>&lt;font color=\"black\">export_iris&lt;\/font>&lt;br>&lt;\/br>&lt;font color=\"#5cb85c\">0&lt;\/font>/&lt;font color=\"#f0ad4e\">0&lt;\/font>/&lt;font color=\"#d9534f\">0&lt;\/font>/&lt;font color=\"blue\">1&lt;\/font>&lt;\/b>; &lt;font color=\"black\">0s&lt;\/font>> tooltip=\"step export_iris: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2021-08-06 16:41:49; End time: 2021-08-06 16:41:49; Duration: 00:00:00\"]\n gzip[label=&lt;&lt;b>&lt;font color=\"black\">gzip&lt;\/font>&lt;br>&lt;\/br>&lt;font color=\"#5cb85c\">0&lt;\/font>/&lt;font color=\"#f0ad4e\">0&lt;\/font>/&lt;font color=\"#d9534f\">0&lt;\/font>/&lt;font color=\"blue\">3&lt;\/font>&lt;\/b>; &lt;font color=\"black\">0s&lt;\/font>> , style=\"rounded\", shape=\"box\" tooltip=\"step gzip: 0 samples passed; 0 samples have warnings; 0 samples have errors; 3 samples in total; Start time: 2021-08-06 16:41:49; End time: 2021-08-06 16:41:49; Duration: 00:00:00\"]\n gunzip[label=&lt;&lt;b>&lt;font color=\"black\">gunzip&lt;\/font>&lt;br>&lt;\/br>&lt;font color=\"#5cb85c\">0&lt;\/font>/&lt;font color=\"#f0ad4e\">0&lt;\/font>/&lt;font color=\"#d9534f\">0&lt;\/font>/&lt;font color=\"blue\">3&lt;\/font>&lt;\/b>; &lt;font color=\"black\">0s&lt;\/font>> , style=\"rounded\", shape=\"box\" tooltip=\"step gunzip: 0 samples passed; 0 samples have warnings; 0 samples have errors; 3 samples in total; Start time: 2021-08-06 16:41:49; End time: 2021-08-06 16:41:49; Duration: 00:00:00\"]\n iris_stats[label=&lt;&lt;b>&lt;font color=\"black\">iris_stats&lt;\/font>&lt;br>&lt;\/br>&lt;font color=\"#5cb85c\">0&lt;\/font>/&lt;font color=\"#f0ad4e\">0&lt;\/font>/&lt;font color=\"#d9534f\">0&lt;\/font>/&lt;font color=\"blue\">1&lt;\/font>&lt;\/b>; &lt;font color=\"black\">0s&lt;\/font>> tooltip=\"step iris_stats: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2021-08-06 16:41:49; End time: 2021-08-06 16:41:49; Duration: 00:00:00\"]\n subgraph cluster_legend {\n rankdir=TB;\n color=\"#EEEEEE\";\n style=filled;\n node [style=filled];\n {rank=same; R_step; Sysargs_step; Main_branch}\n Main_branch -> Sysargs_step -> R_step[color=\"#EEEEEE\"]\n Main_branch[label=\"Main branch\" color=\"dodgerblue\", style=\"filled\", fillcolor=white]; Sysargs_step ->step_state[color=\"#EEEEEE\"];\n step_state[style=\"filled\", shape=\"box\" color=white, label =&lt;\n &lt;table>\n &lt;tr>&lt;td>&lt;b>Step Colors&lt;\/b>&lt;\/td>&lt;\/tr>\n &lt;tr>&lt;td>&lt;font color=\"black\">Pending steps&lt;\/font>; &lt;font color=\"#5cb85c\">Successful steps&lt;\/font>; &lt;font color=\"#d9534f\">Failed steps&lt;\/font>&lt;\/td>&lt;\/tr>\n &lt;tr>&lt;td>&lt;b>Targets Files / Code Chunk &lt;\/b>&lt;\/td>&lt;\/tr>&lt;tr>&lt;td>&lt;font color=\"#5cb85c\">0 (pass) &lt;\/font> | &lt;font color=\"#f0ad4e\">0 (warning) &lt;\/font> | &lt;font color=\"#d9534f\">0 (error) &lt;\/font> | &lt;font color=\"blue\">0 (total)&lt;\/font>; Duration&lt;\/td>&lt;\/tr>&lt;\/table>\n >];\n label=\"Legends\";\n fontsize = 30;\n Sysargs_step[label=\"Sysargs step\" style=\"rounded, filled\", shape=\"box\", fillcolor=white];\n R_step[label=\"R step\" style=\"rounded, filled\", fillcolor=white];\n }\n\n}\n","plotid":"sprwf-48365271","responsive":true,"width":"80%","height":null,"plot_method":"renderSVGElement","rmd":true,"msg":""},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p>For more details about the &lt;code>plotWF&lt;/code> function, please see &lt;a href="#plotWF">here&lt;/a>.&lt;/p>
&lt;h1 id="technical-report">Technical report&lt;/h1>
&lt;p>&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> compiles all the workflow execution logs in one central location,
making it easier to check any standard output (&lt;code>stdout&lt;/code>) or standard error
(&lt;code>stderr&lt;/code>) for any command-line tools used on the workflow or the R code &lt;code>stdout&lt;/code>.
Also, the workflow plot is appended at the beginning of the report, making it
easier to click on the respective step.&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- renderLogs(sal)
&lt;/code>&lt;/pre>
&lt;h1 id="exported-the-workflow">Exported the workflow&lt;/h1>
&lt;p>&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> workflow management system allows to translate and export the
workflow build interactively to R Markdown format or an executable bash script.
This feature advances the reusability of the workflow, as well as the flexibility
for workflow execution.&lt;/p>
&lt;h2 id="r-markdown-file">R Markdown file&lt;/h2>
&lt;p>&lt;code>sal2rmd&lt;/code> function takes an &lt;code>SYSargsList&lt;/code> workflow container and translates it to
SPR workflow template R markdown format. This file can be imported with the
&lt;code>importWF&lt;/code> function, as demonstrated above.&lt;/p>
&lt;pre>&lt;code class="language-r">sal2rmd(sal)
&lt;/code>&lt;/pre>
&lt;h2 id="bash-script">Bash script&lt;/h2>
&lt;p>&lt;code>sal2bash&lt;/code> function takes an &lt;code>SYSargsList&lt;/code> workflow container and translates
it to an executable bash script, so one can run the workflow without loading
&lt;code>SPR&lt;/code> or using an R console.&lt;/p>
&lt;pre>&lt;code class="language-r">sal2bash(sal)
&lt;/code>&lt;/pre>
&lt;p>It will be generated on the project root an executable bash script, called by
default the &lt;code>spr_wf.sh&lt;/code>. Also, a directory &lt;code>./spr_wf&lt;/code> will be created and store
all the R scripts based on the workflow steps. Please note that this function will
“collapse” adjacent R steps into one file as much as possible.&lt;/p>
&lt;h1 id="project-resume-and-restart">Project Resume and Restart&lt;/h1>
&lt;p>If you desire to resume or restart a project that has been initialized in the past,
&lt;code>SPRproject&lt;/code> function allows this operation.&lt;/p>
&lt;p>With the resume option, it is possible to load the &lt;code>SYSargsList&lt;/code> object in R and
resume the analysis. Please, make sure to provide the &lt;code>logs.dir&lt;/code> location, and the
corresponded &lt;code>YAML&lt;/code> file name.
The current working directory needs to be in the project root directory.&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- SPRproject(resume = TRUE, logs.dir = &amp;quot;.SPRproject&amp;quot;, sys.file = &amp;quot;.SPRproject/SYSargsList.yml&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>If you choose to save the environment in the last analysis, you can recover all
the files created in that particular section. &lt;code>SPRproject&lt;/code> function allows this
with &lt;code>load.envir&lt;/code> argument. Please note that the environment was saved only with
you run the workflow in the last section (&lt;code>runWF()&lt;/code>).&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- SPRproject(resume = TRUE, load.envir = TRUE)
&lt;/code>&lt;/pre>
&lt;p>After loading the workflow at your current section, you can check the objects
created in the old environment and decide if it is necessary to copy them to the
current environment.&lt;/p>
&lt;pre>&lt;code class="language-r">viewEnvir(sal)
copyEnvir(sal, list = &amp;quot;plot&amp;quot;, new.env = globalenv())
&lt;/code>&lt;/pre>
&lt;p>This option will keep all previous logs in the folder; however, if you desire to
clean the execution history and restart the workflow, the &lt;code>restart=TRUE&lt;/code> option
can be used.&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- SPRproject(restart = TRUE, overwrite = TRUE, load.envir = FALSE)
&lt;/code>&lt;/pre>
&lt;p>The last and more drastic option from &lt;code>SYSproject&lt;/code> function is to overwrite the
logs and the workflow. This option will delete the hidden folder and the
information on the &lt;code>SYSargsList.yml&lt;/code> files. This will not delete any parameter
file nor any results it was created in previous runs. Please use with caution.&lt;/p>
&lt;pre>&lt;code class="language-r">sal &amp;lt;- SPRproject(overwrite = TRUE)
&lt;/code>&lt;/pre>
&lt;h1 id="exploring-workflow-instances">Exploring workflow instances&lt;/h1>
&lt;p>&lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> provide several accessor methods and useful functions to explore
&lt;code>SYSargsList&lt;/code> workflow object.&lt;/p>
&lt;h2 id="accessor-methods">Accessor Methods&lt;/h2>
&lt;p>Several accessor methods are available that are named after the slot names of
the &lt;code>SYSargsList&lt;/code> workflow object.&lt;/p>
&lt;pre>&lt;code class="language-r">names(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;stepsWF&amp;quot; &amp;quot;statusWF&amp;quot; &amp;quot;targetsWF&amp;quot;
## [4] &amp;quot;outfiles&amp;quot; &amp;quot;SEobj&amp;quot; &amp;quot;dependency&amp;quot;
## [7] &amp;quot;targets_connection&amp;quot; &amp;quot;projectInfo&amp;quot; &amp;quot;runInfo&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Check the length of the workflow:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">length(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 4
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Check the steps of the workflow:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">stepsWF(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## Instance of 'LineWise'
## Code Chunk length: 1
##
## $gzip
## Instance of 'SYSargs2':
## Slot names/accessors:
## targets: 3 (SE...VI), targetsheader: 1 (lines)
## modules: 0
## wf: 1, clt: 1, yamlinput: 4 (inputs)
## input: 3, output: 3
## cmdlist: 3
## Sub Steps:
## 1. gzip (rendered: TRUE)
##
##
##
## $gunzip
## Instance of 'SYSargs2':
## Slot names/accessors:
## targets: 3 (SE...VI), targetsheader: 1 (lines)
## modules: 0
## wf: 1, clt: 1, yamlinput: 4 (inputs)
## input: 3, output: 3
## cmdlist: 3
## Sub Steps:
## 1. gunzip (rendered: TRUE)
##
##
##
## $iris_stats
## Instance of 'LineWise'
## Code Chunk length: 5
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Checking the command-line for each target sample:&lt;/li>
&lt;/ul>
&lt;p>&lt;code>cmdlist()&lt;/code> method printing the system commands for running command-line
software as specified by a given &lt;code>*.cwl&lt;/code> file combined with the paths to the
input samples (&lt;em>e.g.&lt;/em> FASTQ files) provided by a &lt;code>targets&lt;/code> file. The example below
shows the &lt;code>cmdlist()&lt;/code> output for running &lt;code>gzip&lt;/code> and &lt;code>gunzip&lt;/code> on the first sample.
Evaluating the output of &lt;code>cmdlist()&lt;/code> can be very helpful for designing
and debugging &lt;code>*.cwl&lt;/code> files of new command-line software or changing the
parameter settings of existing ones.&lt;/p>
&lt;pre>&lt;code class="language-r">cmdlist(sal, step = c(2, 3), targets = 1)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $gzip
## $gzip$SE
## $gzip$SE$gzip
## [1] &amp;quot;gzip -c results/setosa.csv &amp;gt; results/SE.csv.gz&amp;quot;
##
##
##
## $gunzip
## $gunzip$SE
## $gunzip$SE$gunzip
## [1] &amp;quot;gunzip -c results/SE.csv.gz &amp;gt; results/SE.csv&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Check the workflow status:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">statusWF(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## DataFrame with 1 row and 2 columns
## Step status.summary
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 export_iris Pending
##
## $gzip
## DataFrame with 3 rows and 5 columns
## Targets Total_Files Existing_Files Missing_Files gzip
## &amp;lt;character&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;factor&amp;gt;
## 1 SE 1 0 1 Pending
## 2 VE 1 0 1 Pending
## 3 VI 1 0 1 Pending
##
## $gunzip
## DataFrame with 3 rows and 5 columns
## Targets Total_Files Existing_Files Missing_Files gunzip
## &amp;lt;character&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;numeric&amp;gt; &amp;lt;factor&amp;gt;
## 1 SE 1 0 1 Pending
## 2 VE 1 0 1 Pending
## 3 VI 1 0 1 Pending
##
## $iris_stats
## DataFrame with 1 row and 2 columns
## Step status.summary
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 iris_stats Pending
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Check the workflow targets files:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">targetsWF(sal[2])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $gzip
## DataFrame with 3 rows and 2 columns
## FileName SampleName
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 results/setosa.csv SE
## 2 results/versicolor.csv VE
## 3 results/virginica.csv VI
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Checking the expected outfiles files:&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>outfiles&lt;/code> components of &lt;code>SYSargsList&lt;/code> define the expected outfiles files
for each step in the workflow, some of which are the input for the next workflow step.&lt;/p>
&lt;pre>&lt;code class="language-r">outfiles(sal[2])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $gzip
## DataFrame with 3 rows and 1 column
## gzip_file
## &amp;lt;character&amp;gt;
## 1 results/SE.csv.gz
## 2 results/VE.csv.gz
## 3 results/VI.csv.gz
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Check the workflow dependencies:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">dependency(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## [1] &amp;quot;&amp;quot;
##
## $gzip
## [1] &amp;quot;export_iris&amp;quot;
##
## $gunzip
## [1] &amp;quot;gzip&amp;quot;
##
## $iris_stats
## [1] &amp;quot;gzip&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Check the sample comparisons:&lt;/li>
&lt;/ul>
&lt;p>Sample comparisons are defined in the header lines of the &lt;code>targets&lt;/code> file
starting with ‘&lt;code># &amp;lt;CMP&amp;gt;&lt;/code>.’ This information can be accessed as follows:&lt;/p>
&lt;pre>&lt;code class="language-r">targetsheader(sal, step = &amp;quot;Quality&amp;quot;)
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Get the workflow steps names:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">stepName(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;export_iris&amp;quot; &amp;quot;gzip&amp;quot; &amp;quot;gunzip&amp;quot; &amp;quot;iris_stats&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Get the Sample Id for on particular step:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">SampleName(sal, step = &amp;quot;gzip&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;SE&amp;quot; &amp;quot;VE&amp;quot; &amp;quot;VI&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">SampleName(sal, step = &amp;quot;iris_stats&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## This step doesn't contain multiple samples.
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Get the &lt;code>outfiles&lt;/code> or &lt;code>targets&lt;/code> column files:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">getColumn(sal, &amp;quot;outfiles&amp;quot;, step = &amp;quot;gzip&amp;quot;, column = &amp;quot;gzip_file&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## SE VE VI
## &amp;quot;results/SE.csv.gz&amp;quot; &amp;quot;results/VE.csv.gz&amp;quot; &amp;quot;results/VI.csv.gz&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">getColumn(sal, &amp;quot;targetsWF&amp;quot;, step = &amp;quot;gzip&amp;quot;, column = &amp;quot;FileName&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## SE VE VI
## &amp;quot;results/setosa.csv&amp;quot; &amp;quot;results/versicolor.csv&amp;quot; &amp;quot;results/virginica.csv&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Get the R code for a &lt;code>LineWise&lt;/code> step:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">codeLine(sal, step = &amp;quot;export_iris&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## export_iris
## mapply(function(x, y) write.csv(x, y), split(iris, factor(iris$Species)), file.path(&amp;quot;results&amp;quot;, paste0(names(split(iris, factor(iris$Species))), &amp;quot;.csv&amp;quot;)))
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>View all the objects in the running environment:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">viewEnvir(sal)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## &amp;lt;environment: 0x55bbae3bc680&amp;gt;
## character(0)
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Copy one or multiple objects from the running environment to a new environment:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">copyEnvir(sal, list = c(&amp;quot;plot&amp;quot;), new.env = globalenv(), silent = FALSE)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## &amp;lt;environment: 0x55bbae3bc680&amp;gt;
## Copying to 'new.env':
## plot
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Accessing the &lt;code>*.yml&lt;/code> data&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">yamlinput(sal, step = &amp;quot;gzip&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $file
## $file$class
## [1] &amp;quot;File&amp;quot;
##
## $file$path
## [1] &amp;quot;_FILE_PATH_&amp;quot;
##
##
## $SampleName
## [1] &amp;quot;_SampleName_&amp;quot;
##
## $ext
## [1] &amp;quot;csv.gz&amp;quot;
##
## $results_path
## $results_path$class
## [1] &amp;quot;Directory&amp;quot;
##
## $results_path$path
## [1] &amp;quot;./results&amp;quot;
&lt;/code>&lt;/pre>
&lt;h2 id="subsetting-the-workflow-details">Subsetting the workflow details&lt;/h2>
&lt;ul>
&lt;li>The &lt;code>SYSargsList&lt;/code> class and its subsetting operator &lt;code>[&lt;/code>:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">sal[1]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. export_iris --&amp;gt; Status: Pending
##
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">sal[1:3]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. export_iris --&amp;gt; Status: Pending
## 2. gzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 2.1. gzip
## cmdlist: 3 | Pending: 3
## 3. gunzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 3.1. gunzip
## cmdlist: 3 | Pending: 3
##
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">sal[c(1, 3)]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. export_iris --&amp;gt; Status: Pending
## 2. gunzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 2.1. gunzip
## cmdlist: 3 | Pending: 3
##
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The &lt;code>SYSargsList&lt;/code> class and its subsetting by steps and input samples:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">sal_sub &amp;lt;- subset(sal, subset_steps = c(2, 3), input_targets = (&amp;quot;SE&amp;quot;), keep_steps = TRUE)
stepsWF(sal_sub)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## Instance of 'LineWise'
## Code Chunk length: 1
##
## $gzip
## Instance of 'SYSargs2':
## Slot names/accessors:
## targets: 1 (SE...SE), targetsheader: 1 (lines)
## modules: 0
## wf: 1, clt: 1, yamlinput: 4 (inputs)
## input: 1, output: 1
## cmdlist: 1
## Sub Steps:
## 1. gzip (rendered: TRUE)
##
##
##
## $gunzip
## Instance of 'SYSargs2':
## Slot names/accessors:
## targets: 1 (SE...SE), targetsheader: 1 (lines)
## modules: 0
## wf: 1, clt: 1, yamlinput: 4 (inputs)
## input: 1, output: 1
## cmdlist: 1
## Sub Steps:
## 1. gunzip (rendered: TRUE)
##
##
##
## $iris_stats
## Instance of 'LineWise'
## Code Chunk length: 5
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">targetsWF(sal_sub)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## DataFrame with 0 rows and 0 columns
##
## $gzip
## DataFrame with 1 row and 2 columns
## FileName SampleName
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 results/setosa.csv SE
##
## $gunzip
## DataFrame with 1 row and 2 columns
## gzip_file SampleName
## &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;
## 1 results/SE.csv.gz SE
##
## $iris_stats
## DataFrame with 0 rows and 0 columns
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">outfiles(sal_sub)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $export_iris
## DataFrame with 0 rows and 0 columns
##
## $gzip
## DataFrame with 1 row and 1 column
## gzip_file
## &amp;lt;character&amp;gt;
## 1 results/SE.csv.gz
##
## $gunzip
## DataFrame with 1 row and 1 column
## gunzip_file
## &amp;lt;character&amp;gt;
## 1 results/SE.csv
##
## $iris_stats
## DataFrame with 0 rows and 0 columns
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The &lt;code>SYSargsList&lt;/code> class and its operator &lt;code>+&lt;/code>&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">sal[1] + sal[2] + sal[3]
&lt;/code>&lt;/pre>
&lt;h2 id="replacement-methods">Replacement Methods&lt;/h2>
&lt;ul>
&lt;li>Update a &lt;code>input&lt;/code> parameter in the workflow&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">sal_c &amp;lt;- sal
## check values
yamlinput(sal_c, step = &amp;quot;gzip&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $file
## $file$class
## [1] &amp;quot;File&amp;quot;
##
## $file$path
## [1] &amp;quot;_FILE_PATH_&amp;quot;
##
##
## $SampleName
## [1] &amp;quot;_SampleName_&amp;quot;
##
## $ext
## [1] &amp;quot;csv.gz&amp;quot;
##
## $results_path
## $results_path$class
## [1] &amp;quot;Directory&amp;quot;
##
## $results_path$path
## [1] &amp;quot;./results&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">## check on command-line
cmdlist(sal_c, step = &amp;quot;gzip&amp;quot;, targets = 1)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $gzip
## $gzip$SE
## $gzip$SE$gzip
## [1] &amp;quot;gzip -c results/setosa.csv &amp;gt; results/SE.csv.gz&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">## Replace
yamlinput(sal_c, step = &amp;quot;gzip&amp;quot;, paramName = &amp;quot;ext&amp;quot;) &amp;lt;- &amp;quot;txt.gz&amp;quot;
## check NEW values
yamlinput(sal_c, step = &amp;quot;gzip&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $file
## $file$class
## [1] &amp;quot;File&amp;quot;
##
## $file$path
## [1] &amp;quot;_FILE_PATH_&amp;quot;
##
##
## $SampleName
## [1] &amp;quot;_SampleName_&amp;quot;
##
## $ext
## [1] &amp;quot;txt.gz&amp;quot;
##
## $results_path
## $results_path$class
## [1] &amp;quot;Directory&amp;quot;
##
## $results_path$path
## [1] &amp;quot;./results&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">## Check on command-line
cmdlist(sal_c, step = &amp;quot;gzip&amp;quot;, targets = 1)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $gzip
## $gzip$SE
## $gzip$SE$gzip
## [1] &amp;quot;gzip -c results/setosa.csv &amp;gt; results/SE.txt.gz&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Append and Replacement methods for R Code Steps&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">appendCodeLine(sal_c, step = &amp;quot;export_iris&amp;quot;, after = 1) &amp;lt;- &amp;quot;log_cal_100 &amp;lt;- log(100)&amp;quot;
codeLine(sal_c, step = &amp;quot;export_iris&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## export_iris
## mapply(function(x, y) write.csv(x, y), split(iris, factor(iris$Species)), file.path(&amp;quot;results&amp;quot;, paste0(names(split(iris, factor(iris$Species))), &amp;quot;.csv&amp;quot;)))
## log_cal_100 &amp;lt;- log(100)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">replaceCodeLine(sal_c, step = &amp;quot;export_iris&amp;quot;, line = 2) &amp;lt;- LineWise(code = {
log_cal_100 &amp;lt;- log(50)
})
codeLine(sal_c, step = 1)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## export_iris
## mapply(function(x, y) write.csv(x, y), split(iris, factor(iris$Species)), file.path(&amp;quot;results&amp;quot;, paste0(names(split(iris, factor(iris$Species))), &amp;quot;.csv&amp;quot;)))
## 3.91202300542815
&lt;/code>&lt;/pre>
&lt;p>For more details about the &lt;code>LineWise&lt;/code> class, please see &lt;a href="#linewise">below&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>Rename a Step&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">renameStep(sal_c, step = 1) &amp;lt;- &amp;quot;newStep&amp;quot;
renameStep(sal_c, c(1, 2)) &amp;lt;- c(&amp;quot;newStep2&amp;quot;, &amp;quot;newIndex&amp;quot;)
sal_c
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. newStep2 --&amp;gt; Status: Pending
## 2. newIndex --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 2.1. gzip
## cmdlist: 3 | Pending: 3
## 3. gunzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 3.1. gunzip
## cmdlist: 3 | Pending: 3
## 4. iris_stats --&amp;gt; Status: Pending
##
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">names(outfiles(sal_c))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;newStep2&amp;quot; &amp;quot;newIndex&amp;quot; &amp;quot;gunzip&amp;quot; &amp;quot;iris_stats&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">names(targetsWF(sal_c))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;newStep2&amp;quot; &amp;quot;newIndex&amp;quot; &amp;quot;gunzip&amp;quot; &amp;quot;iris_stats&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">dependency(sal_c)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $newStep2
## [1] &amp;quot;&amp;quot;
##
## $newIndex
## [1] &amp;quot;newStep2&amp;quot;
##
## $gunzip
## [1] &amp;quot;newIndex&amp;quot;
##
## $iris_stats
## [1] &amp;quot;newIndex&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Replace a Step&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">sal_test &amp;lt;- sal[c(1, 2)]
replaceStep(sal_test, step = 1, step_name = &amp;quot;gunzip&amp;quot;) &amp;lt;- sal[3]
sal_test
&lt;/code>&lt;/pre>
&lt;p>Note: Please use this method with attention, because it can disrupt all
the dependency graphs.&lt;/p>
&lt;ul>
&lt;li>Removing a Step&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">sal_test &amp;lt;- sal[-2]
sal_test
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. export_iris --&amp;gt; Status: Pending
## 2. gunzip --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 2.1. gunzip
## cmdlist: 3 | Pending: 3
## 3. iris_stats --&amp;gt; Status: Pending
##
&lt;/code>&lt;/pre>
&lt;h1 id="references">References&lt;/h1></description></item><item><title>Sp: Workflow steps overview</title><link>/sp/spr/steps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/steps/</guid><description>
&lt;!--
- Compile from command-line
Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::html_document'), clean=F); knitr::knit('systemPipeR.Rmd', tangle=TRUE)"; Rscript ../md2jekyll.R systemPipeR.knit.md 2; Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::pdf_document'))"
-->
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
document.querySelector("h1").className = "title";
});
&lt;/script>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
var links = document.links;
for (var i = 0, linksLength = links.length; i &lt; linksLength; i++)
if (links[i].hostname != window.location.hostname)
links[i].target = '_blank';
});
&lt;/script>
&lt;h2 id="define-environment-settings-and-samples">Define environment settings and samples&lt;/h2>
&lt;p>A typical workflow starts with generating the expected working environment
containing the proper directory structure, input files, and parameter settings.
To simplify this task, one can load one of the existing NGS workflows templates
provided by &lt;em>&lt;code>systemPipeRdata&lt;/code>&lt;/em> into the current working directory. The
following does this for the &lt;em>&lt;code>rnaseq&lt;/code>&lt;/em> template. The name of the resulting
workflow directory can be specified under the &lt;em>&lt;code>mydirname&lt;/code>&lt;/em> argument. The
default &lt;em>&lt;code>NULL&lt;/code>&lt;/em> uses the name of the chosen workflow. An error is issued if a
directory of the same name and path exists already. On Linux and OS X systems
one can also create new workflow instances from the command-line of a terminal as shown
&lt;a href="http://bioconductor.org/packages/devel/data/experiment/vignettes/systemPipeRdata/inst/doc/systemPipeRdata.html#generate-workflow-template">here&lt;/a>.
To apply workflows to custom data, the user needs to modify the &lt;em>&lt;code>targets&lt;/code>&lt;/em> file and if
necessary update the corresponding &lt;em>&lt;code>.cwl&lt;/code>&lt;/em> and &lt;em>&lt;code>.yml&lt;/code>&lt;/em> files. A collection of pre-generated &lt;em>&lt;code>.cwl&lt;/code>&lt;/em> and &lt;em>&lt;code>.yml&lt;/code>&lt;/em> files are provided in the &lt;em>&lt;code>param/cwl&lt;/code>&lt;/em> subdirectory of each workflow template. They
are also viewable in the GitHub repository of &lt;em>&lt;code>systemPipeRdata&lt;/code>&lt;/em> (&lt;a href="https://github.com/tgirke/systemPipeRdata/tree/master/inst/extdata/param">see
here&lt;/a>).&lt;/p>
&lt;pre>&lt;code class="language-r">library(systemPipeR)
library(systemPipeRdata)
genWorkenvir(workflow = &amp;quot;rnaseq&amp;quot;, mydirname = NULL)
setwd(&amp;quot;rnaseq&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h2 id="read-preprocessing">Read Preprocessing&lt;/h2>
&lt;h3 id="preprocessing-with-preprocessreads-function">Preprocessing with &lt;em>&lt;code>preprocessReads&lt;/code>&lt;/em> function&lt;/h3>
&lt;p>The function &lt;em>&lt;code>preprocessReads&lt;/code>&lt;/em> allows to apply predefined or custom
read preprocessing functions to all FASTQ files referenced in a
&lt;em>&lt;code>SYSargs2&lt;/code>&lt;/em> container, such as quality filtering or adaptor trimming
routines. The paths to the resulting output FASTQ files are stored in the
&lt;em>&lt;code>output&lt;/code>&lt;/em> slot of the &lt;em>&lt;code>SYSargs2&lt;/code>&lt;/em> object. Internally,
&lt;em>&lt;code>preprocessReads&lt;/code>&lt;/em> uses the &lt;em>&lt;code>FastqStreamer&lt;/code>&lt;/em> function from
the &lt;em>&lt;code>ShortRead&lt;/code>&lt;/em> package to stream through large FASTQ files in a
memory-efficient manner. The following example performs adaptor trimming with
the &lt;em>&lt;code>trimLRPatterns&lt;/code>&lt;/em> function from the &lt;em>&lt;code>Biostrings&lt;/code>&lt;/em> package.
After the trimming step a new targets file is generated (here
&lt;em>&lt;code>targets_trimPE.txt&lt;/code>&lt;/em>) containing the paths to the trimmed FASTQ files.
The new targets file can be used for the next workflow step with an updated
&lt;em>&lt;code>SYSargs2&lt;/code>&lt;/em> instance, &lt;em>e.g.&lt;/em> running the NGS alignments with the
trimmed FASTQ files.&lt;/p>
&lt;p>Construct &lt;em>&lt;code>SYSargs2&lt;/code>&lt;/em> object from &lt;em>&lt;code>cwl&lt;/code>&lt;/em> and &lt;em>&lt;code>yml&lt;/code>&lt;/em> param and &lt;em>&lt;code>targets&lt;/code>&lt;/em> files.&lt;/p>
&lt;pre>&lt;code class="language-r">targetsPE &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targetsPE.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/preprocessReads/trim-pe&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
trim &amp;lt;- loadWorkflow(targets = targetsPE, wf_file = &amp;quot;trim-pe.cwl&amp;quot;, input_file = &amp;quot;trim-pe.yml&amp;quot;,
dir_path = dir_path)
trim &amp;lt;- renderWF(trim, inputvars = c(FileName1 = &amp;quot;_FASTQ_PATH1_&amp;quot;, FileName2 = &amp;quot;_FASTQ_PATH2_&amp;quot;,
SampleName = &amp;quot;_SampleName_&amp;quot;))
trim
output(trim)[1:2]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">preprocessReads(args = trim, Fct = &amp;quot;trimLRPatterns(Rpattern='GCCCGGGTAA',
subject=fq)&amp;quot;,
batchsize = 1e+05, overwrite = TRUE, compress = TRUE)
&lt;/code>&lt;/pre>
&lt;p>The following example shows how one can design a custom read preprocessing function
using utilities provided by the &lt;em>&lt;code>ShortRead&lt;/code>&lt;/em> package, and then run it
in batch mode with the &lt;em>‘preprocessReads’&lt;/em> function (here on paired-end reads).&lt;/p>
&lt;pre>&lt;code class="language-r">filterFct &amp;lt;- function(fq, cutoff = 20, Nexceptions = 0) {
qcount &amp;lt;- rowSums(as(quality(fq), &amp;quot;matrix&amp;quot;) &amp;lt;= cutoff, na.rm = TRUE)
# Retains reads where Phred scores are &amp;gt;= cutoff with N exceptions
fq[qcount &amp;lt;= Nexceptions]
}
preprocessReads(args = trim, Fct = &amp;quot;filterFct(fq, cutoff=20, Nexceptions=0)&amp;quot;, batchsize = 1e+05)
&lt;/code>&lt;/pre>
&lt;h3 id="preprocessing-with-trimgalore">Preprocessing with TrimGalore!&lt;/h3>
&lt;p>&lt;a href="http://www.bioinformatics.babraham.ac.uk/projects/trim_galore/">TrimGalore!&lt;/a> is
a wrapper tool to consistently apply quality and adapter trimming to fastq files,
with some extra functionality for removing Reduced Representation Bisulfite-Seq
(RRBS) libraries.&lt;/p>
&lt;pre>&lt;code class="language-r">targets &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targets.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/trim_galore/trim_galore-se&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
trimG &amp;lt;- loadWorkflow(targets = targets, wf_file = &amp;quot;trim_galore-se.cwl&amp;quot;, input_file = &amp;quot;trim_galore-se.yml&amp;quot;,
dir_path = dir_path)
trimG &amp;lt;- renderWF(trimG, inputvars = c(FileName = &amp;quot;_FASTQ_PATH1_&amp;quot;, SampleName = &amp;quot;_SampleName_&amp;quot;))
trimG
cmdlist(trimG)[1:2]
output(trimG)[1:2]
## Run Single Machine Option
trimG &amp;lt;- runCommandline(trimG[1], make_bam = FALSE)
&lt;/code>&lt;/pre>
&lt;h3 id="preprocessing-with-trimmomatic">Preprocessing with Trimmomatic&lt;/h3>
&lt;pre>&lt;code class="language-r">targetsPE &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targetsPE.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/trimmomatic/trimmomatic-pe&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
trimM &amp;lt;- loadWorkflow(targets = targetsPE, wf_file = &amp;quot;trimmomatic-pe.cwl&amp;quot;, input_file = &amp;quot;trimmomatic-pe.yml&amp;quot;,
dir_path = dir_path)
trimM &amp;lt;- renderWF(trimM, inputvars = c(FileName1 = &amp;quot;_FASTQ_PATH1_&amp;quot;, FileName2 = &amp;quot;_FASTQ_PATH2_&amp;quot;,
SampleName = &amp;quot;_SampleName_&amp;quot;))
trimM
cmdlist(trimM)[1:2]
output(trimM)[1:2]
## Run Single Machine Option
trimM &amp;lt;- runCommandline(trimM[1], make_bam = FALSE)
&lt;/code>&lt;/pre>
&lt;h2 id="fastq-quality-report">FASTQ quality report&lt;/h2>
&lt;p>The following &lt;em>&lt;code>seeFastq&lt;/code>&lt;/em> and &lt;em>&lt;code>seeFastqPlot&lt;/code>&lt;/em> functions generate and plot a series of
useful quality statistics for a set of FASTQ files including per cycle quality
box plots, base proportions, base-level quality trends, relative k-mer
diversity, length and occurrence distribution of reads, number of reads above
quality cutoffs and mean quality distribution.&lt;br>
The function &lt;em>&lt;code>seeFastq&lt;/code>&lt;/em> computes the quality statistics and stores the results in a
relatively small list object that can be saved to disk with &lt;em>&lt;code>save()&lt;/code>&lt;/em> and
reloaded with &lt;em>&lt;code>load()&lt;/code>&lt;/em> for later plotting. The argument &lt;em>&lt;code>klength&lt;/code>&lt;/em> specifies the
k-mer length and &lt;em>&lt;code>batchsize&lt;/code>&lt;/em> the number of reads to a random sample from each
FASTQ file.&lt;/p>
&lt;pre>&lt;code class="language-r">fqlist &amp;lt;- seeFastq(fastq = infile1(trim), batchsize = 10000, klength = 8)
pdf(&amp;quot;./results/fastqReport.pdf&amp;quot;, height = 18, width = 4 * length(fqlist))
seeFastqPlot(fqlist)
dev.off()
&lt;/code>&lt;/pre>
&lt;center>
&lt;img src="fastqReport.png">
&lt;/center>
&lt;div align="center">
&lt;p>&lt;strong>Figure 1:&lt;/strong> FASTQ quality report&lt;/p>
&lt;/div>
&lt;/br>
&lt;p>Parallelization of FASTQ quality report on a single machine with multiple cores.&lt;/p>
&lt;pre>&lt;code class="language-r">f &amp;lt;- function(x) seeFastq(fastq = infile1(trim)[x], batchsize = 1e+05, klength = 8)
fqlist &amp;lt;- bplapply(seq(along = trim), f, BPPARAM = MulticoreParam(workers = 4))
seeFastqPlot(unlist(fqlist, recursive = FALSE))
&lt;/code>&lt;/pre>
&lt;p>Parallelization of FASTQ quality report via scheduler (&lt;em>e.g.&lt;/em> Slurm) across several compute nodes.&lt;/p>
&lt;pre>&lt;code class="language-r">library(BiocParallel)
library(batchtools)
f &amp;lt;- function(x) {
library(systemPipeR)
targetsPE &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targetsPE.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/preprocessReads/trim-pe&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
trim &amp;lt;- loadWorkflow(targets = targetsPE, wf_file = &amp;quot;trim-pe.cwl&amp;quot;, input_file = &amp;quot;trim-pe.yml&amp;quot;,
dir_path = dir_path)
trim &amp;lt;- renderWF(trim, inputvars = c(FileName1 = &amp;quot;_FASTQ_PATH1_&amp;quot;, FileName2 = &amp;quot;_FASTQ_PATH2_&amp;quot;,
SampleName = &amp;quot;_SampleName_&amp;quot;))
seeFastq(fastq = infile1(trim)[x], batchsize = 1e+05, klength = 8)
}
resources &amp;lt;- list(walltime = 120, ntasks = 1, ncpus = 4, memory = 1024)
param &amp;lt;- BatchtoolsParam(workers = 4, cluster = &amp;quot;slurm&amp;quot;, template = &amp;quot;batchtools.slurm.tmpl&amp;quot;,
resources = resources)
fqlist &amp;lt;- bplapply(seq(along = trim), f, BPPARAM = param)
seeFastqPlot(unlist(fqlist, recursive = FALSE))
&lt;/code>&lt;/pre>
&lt;h2 id="ngs-alignment-software">NGS Alignment software&lt;/h2>
&lt;p>After quality control, the sequence reads can be aligned to a reference genome or
transcriptome database. The following sessions present some NGS sequence alignment
software. Select the most accurate aligner and determining the optimal parameter
for your custom data set project.&lt;/p>
&lt;p>For all the following examples, it is necessary to install the respective software
and export the &lt;code>PATH&lt;/code> accordingly. If it is available &lt;a href="http://modules.sourceforge.net/">Environment Module&lt;/a>
in the system, you can load all the request software with &lt;em>&lt;code>moduleload(args)&lt;/code>&lt;/em> function.&lt;/p>
&lt;h3 id="alignment-with-hisat2-using-sysargs2">Alignment with &lt;code>HISAT2&lt;/code> using &lt;em>&lt;code>SYSargs2&lt;/code>&lt;/em>&lt;/h3>
&lt;p>The following steps will demonstrate how to use the short read aligner &lt;code>Hisat2&lt;/code>
(Kim, Langmead, and Salzberg 2015) in both interactive job submissions and batch submissions to
queuing systems of clusters using the &lt;em>&lt;code>systemPipeR's&lt;/code>&lt;/em> new CWL command-line interface.&lt;/p>
&lt;p>The parameter settings of the aligner are defined in the &lt;code>hisat2-mapping-se.cwl&lt;/code>
and &lt;code>hisat2-mapping-se.yml&lt;/code> files. The following shows how to construct the
corresponding &lt;em>SYSargs2&lt;/em> object, here &lt;em>args&lt;/em>.&lt;/p>
&lt;pre>&lt;code class="language-r">targets &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targets.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/hisat2/hisat2-se&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
args &amp;lt;- loadWorkflow(targets = targets, wf_file = &amp;quot;hisat2-mapping-se.cwl&amp;quot;, input_file = &amp;quot;hisat2-mapping-se.yml&amp;quot;,
dir_path = dir_path)
args &amp;lt;- renderWF(args, inputvars = c(FileName = &amp;quot;_FASTQ_PATH1_&amp;quot;, SampleName = &amp;quot;_SampleName_&amp;quot;))
args
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargs2':
## Slot names/accessors:
## targets: 18 (M1A...V12B), targetsheader: 4 (lines)
## modules: 1
## wf: 0, clt: 1, yamlinput: 7 (inputs)
## input: 18, output: 18
## cmdlist: 18
## Sub Steps:
## 1. hisat2-mapping-se (rendered: TRUE)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmdlist(args)[1:2]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $M1A
## $M1A$`hisat2-mapping-se`
## [1] &amp;quot;hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 --min-intronlen 30 --max-intronlen 3000 -U ./data/SRR446027_1.fastq.gz --threads 4&amp;quot;
##
##
## $M1B
## $M1B$`hisat2-mapping-se`
## [1] &amp;quot;hisat2 -S ./results/M1B.sam -x ./data/tair10.fasta -k 1 --min-intronlen 30 --max-intronlen 3000 -U ./data/SRR446028_1.fastq.gz --threads 4&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">output(args)[1:2]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $M1A
## $M1A$`hisat2-mapping-se`
## [1] &amp;quot;./results/M1A.sam&amp;quot;
##
##
## $M1B
## $M1B$`hisat2-mapping-se`
## [1] &amp;quot;./results/M1B.sam&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Subsetting &lt;em>&lt;code>SYSargs2&lt;/code>&lt;/em> class slots for each workflow step.&lt;/p>
&lt;pre>&lt;code class="language-r">subsetWF(args, slot = &amp;quot;input&amp;quot;, subset = &amp;quot;FileName&amp;quot;)[1:2] ## Subsetting the input files for this particular workflow
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## M1A M1B
## &amp;quot;./data/SRR446027_1.fastq.gz&amp;quot; &amp;quot;./data/SRR446028_1.fastq.gz&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">subsetWF(args, slot = &amp;quot;output&amp;quot;, subset = 1, index = 1)[1:2] ## Subsetting the output files for one particular step in the workflow
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## M1A M1B
## &amp;quot;./results/M1A.sam&amp;quot; &amp;quot;./results/M1B.sam&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">subsetWF(args, slot = &amp;quot;step&amp;quot;, subset = 1)[1] ## Subsetting the command-lines for one particular step in the workflow
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## M1A
## &amp;quot;hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 --min-intronlen 30 --max-intronlen 3000 -U ./data/SRR446027_1.fastq.gz --threads 4&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">subsetWF(args, slot = &amp;quot;output&amp;quot;, subset = 1, index = 1, delete = TRUE)[1] ## DELETING specific output files
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## The subset cannot be deleted: no such file
## M1A
## &amp;quot;./results/M1A.sam&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Build &lt;code>Hisat2&lt;/code> index.&lt;/p>
&lt;pre>&lt;code class="language-r">dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/hisat2/hisat2-idx&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
idx &amp;lt;- loadWorkflow(targets = NULL, wf_file = &amp;quot;hisat2-index.cwl&amp;quot;, input_file = &amp;quot;hisat2-index.yml&amp;quot;,
dir_path = dir_path)
idx &amp;lt;- renderWF(idx)
idx
cmdlist(idx)
## Run
runCommandline(idx, make_bam = FALSE)
&lt;/code>&lt;/pre>
&lt;h4 id="interactive-job-submissions-in-a-single-machine">Interactive job submissions in a single machine&lt;/h4>
&lt;p>To simplify the short read alignment execution for the user, the command-line
can be run with the &lt;em>&lt;code>runCommandline&lt;/code>&lt;/em> function.
The execution will be on a single machine without submitting to a queuing system
of a computer cluster. This way, the input FASTQ files will be processed sequentially.
By default &lt;em>&lt;code>runCommandline&lt;/code>&lt;/em> auto detects SAM file outputs and converts them
to sorted and indexed BAM files, using internally the &lt;code>Rsamtools&lt;/code> package
(Morgan et al. 2019). Besides, &lt;em>&lt;code>runCommandline&lt;/code>&lt;/em> allows the user to create a dedicated
results folder for each workflow and a sub-folder for each sample
defined in the &lt;em>targets&lt;/em> file. This includes all the output and log files for each
step. When these options are used, the output location will be updated by default
and can be assigned to the same object.&lt;/p>
&lt;pre>&lt;code class="language-r">runCommandline(args, make_bam = FALSE) ## generates alignments and writes *.sam files to ./results folder
args &amp;lt;- runCommandline(args, make_bam = TRUE) ## same as above but writes files and converts *.sam files to sorted and indexed BAM files. Assigning the new extention of the output files to the object args.
&lt;/code>&lt;/pre>
&lt;p>If available, multiple CPU cores can be used for processing each file. The number
of CPU cores (here 4) to use for each process is defined in the &lt;em>&lt;code>*.yml&lt;/code>&lt;/em> file.
With &lt;em>&lt;code>yamlinput(args)['thread']&lt;/code>&lt;/em> one can return this value from the &lt;em>&lt;code>SYSargs2&lt;/code>&lt;/em> object.&lt;/p>
&lt;h4 id="parallelization-on-clusters">Parallelization on clusters&lt;/h4>
&lt;p>Alternatively, the computation can be greatly accelerated by processing many files
in parallel using several compute nodes of a cluster, where a scheduling/queuing
system is used for load balancing. For this the &lt;em>&lt;code>clusterRun&lt;/code>&lt;/em> function submits
the computing requests to the scheduler using the run specifications
defined by &lt;em>&lt;code>runCommandline&lt;/code>&lt;/em>.&lt;/p>
&lt;p>To avoid over-subscription of CPU cores on the compute nodes, the value from
&lt;em>&lt;code>yamlinput(args)['thread']&lt;/code>&lt;/em> is passed on to the submission command, here &lt;em>&lt;code>ncpus&lt;/code>&lt;/em>
in the &lt;em>&lt;code>resources&lt;/code>&lt;/em> list object. The number of independent parallel cluster
processes is defined under the &lt;em>&lt;code>Njobs&lt;/code>&lt;/em> argument. The following example will run
18 processes in parallel using for each 4 CPU cores. If the resources available
on a cluster allow running all 18 processes at the same time then the shown sample
submission will utilize in total 72 CPU cores. Note, &lt;em>&lt;code>clusterRun&lt;/code>&lt;/em> can be used
with most queueing systems as it is based on utilities from the &lt;em>&lt;code>batchtools&lt;/code>&lt;/em>
package which supports the use of template files (&lt;em>&lt;code>*.tmpl&lt;/code>&lt;/em>) for defining the
run parameters of different schedulers. To run the following code, one needs to
have both a conf file (see &lt;em>&lt;code>.batchtools.conf.R&lt;/code>&lt;/em> samples &lt;a href="https://mllg.github.io/batchtools/">here&lt;/a>)
and a template file (see &lt;em>&lt;code>*.tmpl&lt;/code>&lt;/em> samples &lt;a href="https://github.com/mllg/batchtools/tree/master/inst/templates">here&lt;/a>)
for the queueing available on a system. The following example uses the sample
conf and template files for the Slurm scheduler provided by this package.&lt;/p>
&lt;pre>&lt;code class="language-r">library(batchtools)
resources &amp;lt;- list(walltime = 120, ntasks = 1, ncpus = 4, memory = 1024)
reg &amp;lt;- clusterRun(args, FUN = runCommandline, more.args = list(args = args, make_bam = TRUE,
dir = FALSE), conffile = &amp;quot;.batchtools.conf.R&amp;quot;, template = &amp;quot;batchtools.slurm.tmpl&amp;quot;,
Njobs = 18, runid = &amp;quot;01&amp;quot;, resourceList = resources)
getStatus(reg = reg)
waitForJobs(reg = reg)
&lt;/code>&lt;/pre>
&lt;p>Check and update the output location if necessary.&lt;/p>
&lt;pre>&lt;code class="language-r">args &amp;lt;- output_update(args, dir = FALSE, replace = TRUE, extension = c(&amp;quot;.sam&amp;quot;, &amp;quot;.bam&amp;quot;)) ## Updates the output(args) to the right location in the subfolders
output(args)
&lt;/code>&lt;/pre>
&lt;h4 id="create-new-targets-file">Create new targets file&lt;/h4>
&lt;p>To establish the connectivity to the next workflow step, one can write a new
&lt;em>targets&lt;/em> file with the &lt;em>&lt;code>writeTargetsout&lt;/code>&lt;/em> function. The new &lt;em>targets&lt;/em> file
serves as input to the next &lt;em>&lt;code>loadWorkflow&lt;/code>&lt;/em> and &lt;em>&lt;code>renderWF&lt;/code>&lt;/em> call.&lt;/p>
&lt;pre>&lt;code class="language-r">names(clt(args))
writeTargetsout(x = args, file = &amp;quot;default&amp;quot;, step = 1, new_col = &amp;quot;FileName&amp;quot;, new_col_output_index = 1,
overwrite = TRUE)
&lt;/code>&lt;/pre>
&lt;h4 id="alignment-with-hisat2-and-samtools">Alignment with &lt;code>HISAT2&lt;/code> and &lt;code>SAMtools&lt;/code>&lt;/h4>
&lt;p>Alternatively, it possible to build an workflow with &lt;code>HISAT2&lt;/code> and &lt;code>SAMtools&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-r">targets &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targets.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/workflow-hisat2/workflow-hisat2-se&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
WF &amp;lt;- loadWorkflow(targets = targets, wf_file = &amp;quot;workflow_hisat2-se.cwl&amp;quot;, input_file = &amp;quot;workflow_hisat2-se.yml&amp;quot;,
dir_path = dir_path)
WF &amp;lt;- renderWF(WF, inputvars = c(FileName = &amp;quot;_FASTQ_PATH1_&amp;quot;, SampleName = &amp;quot;_SampleName_&amp;quot;))
WF
cmdlist(WF)[1:2]
output(WF)[1:2]
&lt;/code>&lt;/pre>
&lt;h3 id="alignment-with-tophat2">Alignment with &lt;em>&lt;code>Tophat2&lt;/code>&lt;/em>&lt;/h3>
&lt;p>The NGS reads of this project can also be aligned against the reference genome
sequence using &lt;code>Bowtie2/TopHat2&lt;/code> (Kim et al. 2013; Langmead and Salzberg 2012).&lt;/p>
&lt;p>Build &lt;em>&lt;code>Bowtie2&lt;/code>&lt;/em> index.&lt;/p>
&lt;pre>&lt;code class="language-r">dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/bowtie2/bowtie2-idx&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
idx &amp;lt;- loadWorkflow(targets = NULL, wf_file = &amp;quot;bowtie2-index.cwl&amp;quot;, input_file = &amp;quot;bowtie2-index.yml&amp;quot;,
dir_path = dir_path)
idx &amp;lt;- renderWF(idx)
idx
cmdlist(idx)
## Run in single machine
runCommandline(idx, make_bam = FALSE)
&lt;/code>&lt;/pre>
&lt;p>The parameter settings of the aligner are defined in the &lt;code>tophat2-mapping-pe.cwl&lt;/code>
and &lt;code>tophat2-mapping-pe.yml&lt;/code> files. The following shows how to construct the
corresponding &lt;em>SYSargs2&lt;/em> object, here &lt;em>tophat2PE&lt;/em>.&lt;/p>
&lt;pre>&lt;code class="language-r">targetsPE &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targetsPE.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/tophat2/tophat2-pe&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
tophat2PE &amp;lt;- loadWorkflow(targets = targetsPE, wf_file = &amp;quot;tophat2-mapping-pe.cwl&amp;quot;,
input_file = &amp;quot;tophat2-mapping-pe.yml&amp;quot;, dir_path = dir_path)
tophat2PE &amp;lt;- renderWF(tophat2PE, inputvars = c(FileName1 = &amp;quot;_FASTQ_PATH1_&amp;quot;, FileName2 = &amp;quot;_FASTQ_PATH2_&amp;quot;,
SampleName = &amp;quot;_SampleName_&amp;quot;))
tophat2PE
cmdlist(tophat2PE)[1:2]
output(tophat2PE)[1:2]
## Run in single machine
tophat2PE &amp;lt;- runCommandline(tophat2PE[1], make_bam = TRUE)
&lt;/code>&lt;/pre>
&lt;p>Parallelization on clusters.&lt;/p>
&lt;pre>&lt;code class="language-r">resources &amp;lt;- list(walltime = 120, ntasks = 1, ncpus = 4, memory = 1024)
reg &amp;lt;- clusterRun(tophat2PE, FUN = runCommandline, more.args = list(args = tophat2PE,
make_bam = TRUE, dir = FALSE), conffile = &amp;quot;.batchtools.conf.R&amp;quot;, template = &amp;quot;batchtools.slurm.tmpl&amp;quot;,
Njobs = 18, runid = &amp;quot;01&amp;quot;, resourceList = resources)
waitForJobs(reg = reg)
&lt;/code>&lt;/pre>
&lt;p>Create new targets file&lt;/p>
&lt;pre>&lt;code class="language-r">names(clt(tophat2PE))
writeTargetsout(x = tophat2PE, file = &amp;quot;default&amp;quot;, step = 1, new_col = &amp;quot;tophat2PE&amp;quot;,
new_col_output_index = 1, overwrite = TRUE)
&lt;/code>&lt;/pre>
&lt;h3 id="alignment-with-bowtie2-eg-for-mirna-profiling">Alignment with &lt;em>&lt;code>Bowtie2&lt;/code>&lt;/em> (&lt;em>e.g.&lt;/em> for miRNA profiling)&lt;/h3>
&lt;p>The following example runs &lt;em>&lt;code>Bowtie2&lt;/code>&lt;/em> as a single process without submitting it to a cluster.&lt;/p>
&lt;p>Building the index:&lt;/p>
&lt;pre>&lt;code class="language-r">dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/bowtie2/bowtie2-idx&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
idx &amp;lt;- loadWorkflow(targets = NULL, wf_file = &amp;quot;bowtie2-index.cwl&amp;quot;, input_file = &amp;quot;bowtie2-index.yml&amp;quot;,
dir_path = dir_path)
idx &amp;lt;- renderWF(idx)
idx
cmdlist(idx)
## Run in single machine
runCommandline(idx, make_bam = FALSE)
&lt;/code>&lt;/pre>
&lt;p>Building all the command-line:&lt;/p>
&lt;pre>&lt;code class="language-r">targetsPE &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targetsPE.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/bowtie2/bowtie2-pe&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
bowtiePE &amp;lt;- loadWorkflow(targets = targetsPE, wf_file = &amp;quot;bowtie2-mapping-pe.cwl&amp;quot;,
input_file = &amp;quot;bowtie2-mapping-pe.yml&amp;quot;, dir_path = dir_path)
bowtiePE &amp;lt;- renderWF(bowtiePE, inputvars = c(FileName1 = &amp;quot;_FASTQ_PATH1_&amp;quot;, FileName2 = &amp;quot;_FASTQ_PATH2_&amp;quot;,
SampleName = &amp;quot;_SampleName_&amp;quot;))
bowtiePE
cmdlist(bowtiePE)[1:2]
output(bowtiePE)[1:2]
&lt;/code>&lt;/pre>
&lt;p>Running all the jobs to computing nodes.&lt;/p>
&lt;pre>&lt;code class="language-r">resources &amp;lt;- list(walltime = 120, ntasks = 1, ncpus = 4, memory = 1024)
reg &amp;lt;- clusterRun(bowtiePE, FUN = runCommandline, more.args = list(args = bowtiePE,
dir = FALSE), conffile = &amp;quot;.batchtools.conf.R&amp;quot;, template = &amp;quot;batchtools.slurm.tmpl&amp;quot;,
Njobs = 18, runid = &amp;quot;01&amp;quot;, resourceList = resources)
getStatus(reg = reg)
&lt;/code>&lt;/pre>
&lt;p>Alternatively, it possible to run all the jobs in a single machine.&lt;/p>
&lt;pre>&lt;code class="language-r">bowtiePE &amp;lt;- runCommandline(bowtiePE)
&lt;/code>&lt;/pre>
&lt;p>Create new targets file.&lt;/p>
&lt;pre>&lt;code class="language-r">names(clt(bowtiePE))
writeTargetsout(x = bowtiePE, file = &amp;quot;default&amp;quot;, step = 1, new_col = &amp;quot;bowtiePE&amp;quot;, new_col_output_index = 1,
overwrite = TRUE)
&lt;/code>&lt;/pre>
&lt;h3 id="alignment-with-bwa-mem-eg-for-var-seq">Alignment with &lt;em>&lt;code>BWA-MEM&lt;/code>&lt;/em> (&lt;em>e.g.&lt;/em> for VAR-Seq)&lt;/h3>
&lt;p>The following example runs BWA-MEM as a single process without submitting it to a cluster. ##TODO: add reference&lt;/p>
&lt;p>Build the index:&lt;/p>
&lt;pre>&lt;code class="language-r">dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/bwa/bwa-idx&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
idx &amp;lt;- loadWorkflow(targets = NULL, wf_file = &amp;quot;bwa-index.cwl&amp;quot;, input_file = &amp;quot;bwa-index.yml&amp;quot;,
dir_path = dir_path)
idx &amp;lt;- renderWF(idx)
idx
cmdlist(idx) # Indexes reference genome
## Run
runCommandline(idx, make_bam = FALSE)
&lt;/code>&lt;/pre>
&lt;p>Running the alignment:&lt;/p>
&lt;pre>&lt;code class="language-r">targetsPE &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targetsPE.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/bwa/bwa-pe&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
bwaPE &amp;lt;- loadWorkflow(targets = targetsPE, wf_file = &amp;quot;bwa-pe.cwl&amp;quot;, input_file = &amp;quot;bwa-pe.yml&amp;quot;,
dir_path = dir_path)
bwaPE &amp;lt;- renderWF(bwaPE, inputvars = c(FileName1 = &amp;quot;_FASTQ_PATH1_&amp;quot;, FileName2 = &amp;quot;_FASTQ_PATH2_&amp;quot;,
SampleName = &amp;quot;_SampleName_&amp;quot;))
bwaPE
cmdlist(bwaPE)[1:2]
output(bwaPE)[1:2]
## Single Machine
bwaPE &amp;lt;- runCommandline(args = bwaPE, make_bam = FALSE)
## Cluster
library(batchtools)
resources &amp;lt;- list(walltime = 120, ntasks = 1, ncpus = 4, memory = 1024)
reg &amp;lt;- clusterRun(bwaPE, FUN = runCommandline, more.args = list(args = bwaPE, dir = FALSE),
conffile = &amp;quot;.batchtools.conf.R&amp;quot;, template = &amp;quot;batchtools.slurm.tmpl&amp;quot;, Njobs = 18,
runid = &amp;quot;01&amp;quot;, resourceList = resources)
getStatus(reg = reg)
&lt;/code>&lt;/pre>
&lt;p>Create new targets file.&lt;/p>
&lt;pre>&lt;code class="language-r">names(clt(bwaPE))
writeTargetsout(x = bwaPE, file = &amp;quot;default&amp;quot;, step = 1, new_col = &amp;quot;bwaPE&amp;quot;, new_col_output_index = 1,
overwrite = TRUE)
&lt;/code>&lt;/pre>
&lt;h3 id="alignment-with-rsubread-eg-for-rna-seq">Alignment with &lt;em>&lt;code>Rsubread&lt;/code>&lt;/em> (&lt;em>e.g.&lt;/em> for RNA-Seq)&lt;/h3>
&lt;p>The following example shows how one can use within the environment the R-based aligner , allowing running from R or command-line.&lt;/p>
&lt;pre>&lt;code class="language-r">## Build the index:
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/rsubread/rsubread-idx&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
idx &amp;lt;- loadWorkflow(targets = NULL, wf_file = &amp;quot;rsubread-index.cwl&amp;quot;, input_file = &amp;quot;rsubread-index.yml&amp;quot;,
dir_path = dir_path)
idx &amp;lt;- renderWF(idx)
idx
cmdlist(idx)
runCommandline(args = idx, make_bam = FALSE)
## Running the alignment:
targets &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targets.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/rsubread/rsubread-se&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
rsubread &amp;lt;- loadWorkflow(targets = targets, wf_file = &amp;quot;rsubread-mapping-se.cwl&amp;quot;,
input_file = &amp;quot;rsubread-mapping-se.yml&amp;quot;, dir_path = dir_path)
rsubread &amp;lt;- renderWF(rsubread, inputvars = c(FileName = &amp;quot;_FASTQ_PATH1_&amp;quot;, SampleName = &amp;quot;_SampleName_&amp;quot;))
rsubread
cmdlist(rsubread)[1]
## Single Machine
rsubread &amp;lt;- runCommandline(args = rsubread[1])
&lt;/code>&lt;/pre>
&lt;p>Create new targets file.&lt;/p>
&lt;pre>&lt;code class="language-r">names(clt(rsubread))
writeTargetsout(x = rsubread, file = &amp;quot;default&amp;quot;, step = 1, new_col = &amp;quot;rsubread&amp;quot;, new_col_output_index = 1,
overwrite = TRUE)
&lt;/code>&lt;/pre>
&lt;h3 id="alignment-with-gsnap-eg-for-var-seq-and-rna-seq">Alignment with &lt;em>&lt;code>gsnap&lt;/code>&lt;/em> (&lt;em>e.g.&lt;/em> for VAR-Seq and RNA-Seq)&lt;/h3>
&lt;p>Another R-based short read aligner is &lt;em>&lt;code>gsnap&lt;/code>&lt;/em> from the &lt;em>&lt;code>gmapR&lt;/code>&lt;/em> package (Wu and Nacu 2010).
The code sample below introduces how to run this aligner on multiple nodes of a compute cluster.&lt;/p>
&lt;pre>&lt;code class="language-r">## Build the index:
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/gsnap/gsnap-idx&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
idx &amp;lt;- loadWorkflow(targets = NULL, wf_file = &amp;quot;gsnap-index.cwl&amp;quot;, input_file = &amp;quot;gsnap-index.yml&amp;quot;,
dir_path = dir_path)
idx &amp;lt;- renderWF(idx)
idx
cmdlist(idx)
runCommandline(args = idx, make_bam = FALSE)
## Running the alignment:
targetsPE &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targetsPE.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl/gsnap/gsnap-pe&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
gsnap &amp;lt;- loadWorkflow(targets = targetsPE, wf_file = &amp;quot;gsnap-mapping-pe.cwl&amp;quot;, input_file = &amp;quot;gsnap-mapping-pe.yml&amp;quot;,
dir_path = dir_path)
gsnap &amp;lt;- renderWF(gsnap, inputvars = c(FileName1 = &amp;quot;_FASTQ_PATH1_&amp;quot;, FileName2 = &amp;quot;_FASTQ_PATH2_&amp;quot;,
SampleName = &amp;quot;_SampleName_&amp;quot;))
gsnap
cmdlist(gsnap)[1]
output(gsnap)[1]
## Cluster
library(batchtools)
resources &amp;lt;- list(walltime = 120, ntasks = 1, ncpus = 4, memory = 1024)
reg &amp;lt;- clusterRun(gsnap, FUN = runCommandline, more.args = list(args = gsnap, make_bam = FALSE),
conffile = &amp;quot;.batchtools.conf.R&amp;quot;, template = &amp;quot;batchtools.slurm.tmpl&amp;quot;, Njobs = 18,
runid = &amp;quot;01&amp;quot;, resourceList = resources)
getStatus(reg = reg)
gsnap &amp;lt;- output_update(gsnap, dir = FALSE, replace = TRUE, extension = c(&amp;quot;.sam&amp;quot;,
&amp;quot;.bam&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>Create new targets file.&lt;/p>
&lt;pre>&lt;code class="language-r">names(clt(gsnap))
writeTargetsout(x = gsnap, file = &amp;quot;default&amp;quot;, step = 1, new_col = &amp;quot;gsnap&amp;quot;, new_col_output_index = 1,
overwrite = TRUE)
&lt;/code>&lt;/pre>
&lt;h2 id="create-symbolic-links-for-viewing-bam-files-in-igv">Create symbolic links for viewing BAM files in IGV&lt;/h2>
&lt;p>The genome browser IGV supports reading of indexed/sorted BAM files via web URLs. This way it can be avoided to create unnecessary copies of these large files. To enable this approach, an HTML directory with Http access needs to be available in the user account (&lt;em>e.g.&lt;/em> &lt;em>&lt;code>home/publichtml&lt;/code>&lt;/em>) of a system. If this is not the case then the BAM files need to be moved or copied to the system where IGV runs. In the following, &lt;em>&lt;code>htmldir&lt;/code>&lt;/em> defines the path to the HTML directory with http access where the symbolic links to the BAM files will be stored. The corresponding URLs will be written to a text file specified under the &lt;code>_urlfile&lt;/code>_ argument.&lt;/p>
&lt;pre>&lt;code class="language-r">symLink2bam(sysargs = args, htmldir = c(&amp;quot;~/.html/&amp;quot;, &amp;quot;somedir/&amp;quot;), urlbase = &amp;quot;http://myserver.edu/~username/&amp;quot;,
urlfile = &amp;quot;IGVurl.txt&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h2 id="read-counting-for-mrna-profiling-experiments">Read counting for mRNA profiling experiments&lt;/h2>
&lt;p>Create &lt;em>&lt;code>txdb&lt;/code>&lt;/em> (needs to be done only once).&lt;/p>
&lt;pre>&lt;code class="language-r">library(GenomicFeatures)
txdb &amp;lt;- makeTxDbFromGFF(file = &amp;quot;data/tair10.gff&amp;quot;, format = &amp;quot;gff&amp;quot;, dataSource = &amp;quot;TAIR&amp;quot;,
organism = &amp;quot;Arabidopsis thaliana&amp;quot;)
saveDb(txdb, file = &amp;quot;./data/tair10.sqlite&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>The following performs read counting with &lt;em>&lt;code>summarizeOverlaps&lt;/code>&lt;/em> in parallel mode with multiple cores.&lt;/p>
&lt;pre>&lt;code class="language-r">library(BiocParallel)
txdb &amp;lt;- loadDb(&amp;quot;./data/tair10.sqlite&amp;quot;)
eByg &amp;lt;- exonsBy(txdb, by = &amp;quot;gene&amp;quot;)
outpaths &amp;lt;- subsetWF(args, slot = &amp;quot;output&amp;quot;, subset = 1, index = 1)
bfl &amp;lt;- BamFileList(outpaths, yieldSize = 50000, index = character())
multicoreParam &amp;lt;- MulticoreParam(workers = 4)
register(multicoreParam)
registered()
counteByg &amp;lt;- bplapply(bfl, function(x) summarizeOverlaps(eByg, x, mode = &amp;quot;Union&amp;quot;,
ignore.strand = TRUE, inter.feature = TRUE, singleEnd = TRUE))
# Note: for strand-specific RNA-Seq set 'ignore.strand=FALSE' and for PE data
# set 'singleEnd=FALSE'
countDFeByg &amp;lt;- sapply(seq(along = counteByg), function(x) assays(counteByg[[x]])$counts)
rownames(countDFeByg) &amp;lt;- names(rowRanges(counteByg[[1]]))
colnames(countDFeByg) &amp;lt;- names(bfl)
rpkmDFeByg &amp;lt;- apply(countDFeByg, 2, function(x) returnRPKM(counts = x, ranges = eByg))
write.table(countDFeByg, &amp;quot;results/countDFeByg.xls&amp;quot;, col.names = NA, quote = FALSE,
sep = &amp;quot;\t&amp;quot;)
write.table(rpkmDFeByg, &amp;quot;results/rpkmDFeByg.xls&amp;quot;, col.names = NA, quote = FALSE,
sep = &amp;quot;\t&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Please note, in addition to read counts this step generates RPKM normalized expression values. For most statistical differential expression or abundance analysis methods, such as &lt;em>&lt;code>edgeR&lt;/code>&lt;/em> or &lt;em>&lt;code>DESeq2&lt;/code>&lt;/em>, the raw count values should be used as input. The usage of RPKM values should be restricted to specialty applications required by some users, &lt;em>e.g.&lt;/em> manually comparing the expression levels of different genes or features.&lt;/p>
&lt;p>Read counting with &lt;em>&lt;code>summarizeOverlaps&lt;/code>&lt;/em> using multiple nodes of a cluster.&lt;/p>
&lt;pre>&lt;code class="language-r">library(BiocParallel)
f &amp;lt;- function(x) {
library(systemPipeR)
library(BiocParallel)
library(GenomicFeatures)
txdb &amp;lt;- loadDb(&amp;quot;./data/tair10.sqlite&amp;quot;)
eByg &amp;lt;- exonsBy(txdb, by = &amp;quot;gene&amp;quot;)
args &amp;lt;- systemArgs(sysma = &amp;quot;param/tophat.param&amp;quot;, mytargets = &amp;quot;targets.txt&amp;quot;)
outpaths &amp;lt;- subsetWF(args, slot = &amp;quot;output&amp;quot;, subset = 1, index = 1)
bfl &amp;lt;- BamFileList(outpaths, yieldSize = 50000, index = character())
summarizeOverlaps(eByg, bfl[x], mode = &amp;quot;Union&amp;quot;, ignore.strand = TRUE, inter.feature = TRUE,
singleEnd = TRUE)
}
resources &amp;lt;- list(walltime = 120, ntasks = 1, ncpus = 4, memory = 1024)
param &amp;lt;- BatchtoolsParam(workers = 4, cluster = &amp;quot;slurm&amp;quot;, template = &amp;quot;batchtools.slurm.tmpl&amp;quot;,
resources = resources)
counteByg &amp;lt;- bplapply(seq(along = args), f, BPPARAM = param)
countDFeByg &amp;lt;- sapply(seq(along = counteByg), function(x) assays(counteByg[[x]])$counts)
rownames(countDFeByg) &amp;lt;- names(rowRanges(counteByg[[1]]))
colnames(countDFeByg) &amp;lt;- names(outpaths)
&lt;/code>&lt;/pre>
&lt;p>Useful commands for monitoring the progress of submitted jobs&lt;/p>
&lt;pre>&lt;code class="language-r">getStatus(reg = reg)
outpaths &amp;lt;- subsetWF(args, slot = &amp;quot;output&amp;quot;, subset = 1, index = 1)
file.exists(outpaths)
sapply(1:length(outpaths), function(x) loadResult(reg, id = x)) # Works after job completion
&lt;/code>&lt;/pre>
&lt;h4 id="read-and-alignment-count-stats">Read and alignment count stats&lt;/h4>
&lt;p>Generate a table of read and alignment counts for all samples.&lt;/p>
&lt;pre>&lt;code class="language-r">read_statsDF &amp;lt;- alignStats(args)
write.table(read_statsDF, &amp;quot;results/alignStats.xls&amp;quot;, row.names = FALSE, quote = FALSE,
sep = &amp;quot;\t&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>The following shows the first four lines of the sample alignment stats file
provided by the &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> package. For simplicity the number of PE reads
is multiplied here by 2 to approximate proper alignment frequencies where each
read in a pair is counted.&lt;/p>
&lt;pre>&lt;code class="language-r">read.table(system.file(&amp;quot;extdata&amp;quot;, &amp;quot;alignStats.xls&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;), header = TRUE)[1:4,
]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## FileName Nreads2x Nalign Perc_Aligned Nalign_Primary Perc_Aligned_Primary
## 1 M1A 192918 177961 92.24697 177961 92.24697
## 2 M1B 197484 159378 80.70426 159378 80.70426
## 3 A1A 189870 176055 92.72397 176055 92.72397
## 4 A1B 188854 147768 78.24457 147768 78.24457
&lt;/code>&lt;/pre>
&lt;p>Parallelization of read/alignment stats on single machine with multiple cores.&lt;/p>
&lt;pre>&lt;code class="language-r">f &amp;lt;- function(x) alignStats(args[x])
read_statsList &amp;lt;- bplapply(seq(along = args), f, BPPARAM = MulticoreParam(workers = 8))
read_statsDF &amp;lt;- do.call(&amp;quot;rbind&amp;quot;, read_statsList)
&lt;/code>&lt;/pre>
&lt;p>Parallelization of read/alignment stats via scheduler (&lt;em>e.g.&lt;/em> Slurm) across several compute nodes.&lt;/p>
&lt;pre>&lt;code class="language-r">library(BiocParallel)
library(batchtools)
f &amp;lt;- function(x) {
library(systemPipeR)
targets &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targets.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
dir_path &amp;lt;- &amp;quot;param/cwl/hisat2/hisat2-se&amp;quot; ## TODO: replace path to system.file
args &amp;lt;- loadWorkflow(targets = targets, wf_file = &amp;quot;hisat2-mapping-se.cwl&amp;quot;, input_file = &amp;quot;hisat2-mapping-se.yml&amp;quot;,
dir_path = dir_path)
args &amp;lt;- renderWF(args, inputvars = c(FileName = &amp;quot;_FASTQ_PATH1_&amp;quot;, SampleName = &amp;quot;_SampleName_&amp;quot;))
args &amp;lt;- output_update(args, dir = FALSE, replace = TRUE, extension = c(&amp;quot;.sam&amp;quot;,
&amp;quot;.bam&amp;quot;))
alignStats(args[x])
}
resources &amp;lt;- list(walltime = 120, ntasks = 1, ncpus = 4, memory = 1024)
param &amp;lt;- BatchtoolsParam(workers = 4, cluster = &amp;quot;slurm&amp;quot;, template = &amp;quot;batchtools.slurm.tmpl&amp;quot;,
resources = resources)
read_statsList &amp;lt;- bplapply(seq(along = args), f, BPPARAM = param)
read_statsDF &amp;lt;- do.call(&amp;quot;rbind&amp;quot;, read_statsList)
&lt;/code>&lt;/pre>
&lt;h2 id="read-counting-for-mirna-profiling-experiments">Read counting for miRNA profiling experiments&lt;/h2>
&lt;p>Download miRNA genes from miRBase.&lt;/p>
&lt;pre>&lt;code class="language-r">system(&amp;quot;wget ftp://mirbase.org/pub/mirbase/19/genomes/My_species.gff3 -P ./data/&amp;quot;)
gff &amp;lt;- import.gff(&amp;quot;./data/My_species.gff3&amp;quot;)
gff &amp;lt;- split(gff, elementMetadata(gff)$ID)
bams &amp;lt;- names(bampaths)
names(bams) &amp;lt;- targets$SampleName
bfl &amp;lt;- BamFileList(bams, yieldSize = 50000, index = character())
countDFmiR &amp;lt;- summarizeOverlaps(gff, bfl, mode = &amp;quot;Union&amp;quot;, ignore.strand = FALSE,
inter.feature = FALSE) # Note: inter.feature=FALSE important since pre and mature miRNA ranges overlap
rpkmDFmiR &amp;lt;- apply(countDFmiR, 2, function(x) returnRPKM(counts = x, gffsub = gff))
write.table(assays(countDFmiR)$counts, &amp;quot;results/countDFmiR.xls&amp;quot;, col.names = NA,
quote = FALSE, sep = &amp;quot;\t&amp;quot;)
write.table(rpkmDFmiR, &amp;quot;results/rpkmDFmiR.xls&amp;quot;, col.names = NA, quote = FALSE, sep = &amp;quot;\t&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h2 id="correlation-analysis-of-samples">Correlation analysis of samples&lt;/h2>
&lt;p>The following computes the sample-wise Spearman correlation coefficients from the &lt;em>&lt;code>rlog&lt;/code>&lt;/em> (regularized-logarithm) transformed expression values generated with the &lt;em>&lt;code>DESeq2&lt;/code>&lt;/em> package. After transformation to a distance matrix, hierarchical clustering is performed with the &lt;em>&lt;code>hclust&lt;/code>&lt;/em> function and the result is plotted as a dendrogram (&lt;a href="./results/sample_tree.pdf">sample_tree.pdf&lt;/a>).&lt;/p>
&lt;pre>&lt;code class="language-r">library(DESeq2, warn.conflicts = FALSE, quietly = TRUE)
library(ape, warn.conflicts = FALSE)
countDFpath &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;countDFeByg.xls&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
countDF &amp;lt;- as.matrix(read.table(countDFpath))
colData &amp;lt;- data.frame(row.names = targets.as.df(targets(args))$SampleName, condition = targets.as.df(targets(args))$Factor)
dds &amp;lt;- DESeqDataSetFromMatrix(countData = countDF, colData = colData, design = ~condition)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning in DESeqDataSet(se, design = design, ignoreRank): some variables in
## design formula are characters, converting to factors
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">d &amp;lt;- cor(assay(rlog(dds)), method = &amp;quot;spearman&amp;quot;)
hc &amp;lt;- hclust(dist(1 - d))
plot.phylo(as.phylo(hc), type = &amp;quot;p&amp;quot;, edge.col = 4, edge.width = 3, show.node.label = TRUE,
no.margin = TRUE)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="../../en/sp/spr/steps_files/figure-html/sample_tree_rlog-1.png" width="672" />&lt;/p>
&lt;div align="center">
&lt;p>&lt;strong>Figure 2:&lt;/strong> Correlation dendrogram of samples for &lt;em>&lt;code>rlog&lt;/code>&lt;/em> values.&lt;/p>
&lt;/div>
&lt;/br>
&lt;p>Alternatively, the clustering can be performed with &lt;em>&lt;code>RPKM&lt;/code>&lt;/em> normalized expression values. In combination with Spearman correlation the results of the two clustering methods are often relatively similar.&lt;/p>
&lt;pre>&lt;code class="language-r">rpkmDFeBygpath &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;rpkmDFeByg.xls&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
rpkmDFeByg &amp;lt;- read.table(rpkmDFeBygpath, check.names = FALSE)
rpkmDFeByg &amp;lt;- rpkmDFeByg[rowMeans(rpkmDFeByg) &amp;gt; 50, ]
d &amp;lt;- cor(rpkmDFeByg, method = &amp;quot;spearman&amp;quot;)
hc &amp;lt;- hclust(as.dist(1 - d))
plot.phylo(as.phylo(hc), type = &amp;quot;p&amp;quot;, edge.col = &amp;quot;blue&amp;quot;, edge.width = 2, show.node.label = TRUE,
no.margin = TRUE)
&lt;/code>&lt;/pre>
&lt;h2 id="deg-analysis-with-edger">DEG analysis with &lt;em>&lt;code>edgeR&lt;/code>&lt;/em>&lt;/h2>
&lt;p>The following *&lt;code>run_edgeR&lt;/code>* function is a convenience wrapper for
identifying differentially expressed genes (DEGs) in batch mode with
*&lt;code>edgeR&lt;/code>&lt;em>’s GML method (Robinson, McCarthy, and Smyth 2010) for any number of
pairwise sample comparisons specified under the &lt;em>&lt;code>cmp&lt;/code>* argument. Users
are strongly encouraged to consult the
&lt;/em>&lt;code>edgeR&lt;/code>&lt;/em>&lt;a href="%5Chref%7Bhttp://www.bioconductor.org/packages/devel/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf">&lt;/a> vignette
for more detailed information on this topic and how to properly run &lt;em>&lt;code>edgeR&lt;/code>&lt;/em>
on data sets with more complex experimental designs.&lt;/p>
&lt;pre>&lt;code class="language-r">targetspath &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;targets.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
targets &amp;lt;- read.delim(targetspath, comment = &amp;quot;#&amp;quot;)
cmp &amp;lt;- readComp(file = targetspath, format = &amp;quot;matrix&amp;quot;, delim = &amp;quot;-&amp;quot;)
cmp[[1]]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [,1] [,2]
## [1,] &amp;quot;M1&amp;quot; &amp;quot;A1&amp;quot;
## [2,] &amp;quot;M1&amp;quot; &amp;quot;V1&amp;quot;
## [3,] &amp;quot;A1&amp;quot; &amp;quot;V1&amp;quot;
## [4,] &amp;quot;M6&amp;quot; &amp;quot;A6&amp;quot;
## [5,] &amp;quot;M6&amp;quot; &amp;quot;V6&amp;quot;
## [6,] &amp;quot;A6&amp;quot; &amp;quot;V6&amp;quot;
## [7,] &amp;quot;M12&amp;quot; &amp;quot;A12&amp;quot;
## [8,] &amp;quot;M12&amp;quot; &amp;quot;V12&amp;quot;
## [9,] &amp;quot;A12&amp;quot; &amp;quot;V12&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">countDFeBygpath &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;countDFeByg.xls&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
countDFeByg &amp;lt;- read.delim(countDFeBygpath, row.names = 1)
edgeDF &amp;lt;- run_edgeR(countDF = countDFeByg, targets = targets, cmp = cmp[[1]], independent = FALSE,
mdsplot = &amp;quot;&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Loading required namespace: edgeR
## Disp = 0.21829 , BCV = 0.4672
&lt;/code>&lt;/pre>
&lt;p>Filter and plot DEG results for up and down-regulated genes. Because of the small size of the toy data set used by this vignette, the &lt;em>FDR&lt;/em> value has been set to a relatively high threshold (here 10%). More commonly used &lt;em>FDR&lt;/em> cutoffs are 1% or 5%. The definition of ‘&lt;em>up&lt;/em>’ and ‘&lt;em>down&lt;/em>’ is given in the corresponding help file. To open it, type &lt;em>&lt;code>?filterDEGs&lt;/code>&lt;/em> in the R console.&lt;/p>
&lt;pre>&lt;code class="language-r">DEG_list &amp;lt;- filterDEGs(degDF = edgeDF, filter = c(Fold = 2, FDR = 10))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="../../en/sp/spr/steps_files/figure-html/edger_deg_counts-1.png" width="672" />&lt;/p>
&lt;div align="center">
&lt;p>&lt;strong>Figure 3:&lt;/strong> Up and down regulated DEGs identified by &lt;em>&lt;code>edgeR&lt;/code>&lt;/em>.&lt;/p>
&lt;/div>
&lt;/br>
&lt;pre>&lt;code class="language-r">names(DEG_list)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;UporDown&amp;quot; &amp;quot;Up&amp;quot; &amp;quot;Down&amp;quot; &amp;quot;Summary&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">DEG_list$Summary[1:4, ]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Comparisons Counts_Up_or_Down Counts_Up Counts_Down
## M1-A1 M1-A1 0 0 0
## M1-V1 M1-V1 1 1 0
## A1-V1 A1-V1 1 1 0
## M6-A6 M6-A6 0 0 0
&lt;/code>&lt;/pre>
&lt;h2 id="deg-analysis-with-deseq2">DEG analysis with &lt;em>&lt;code>DESeq2&lt;/code>&lt;/em>&lt;/h2>
&lt;p>The following *&lt;code>run_DESeq2&lt;/code>* function is a convenience wrapper for
identifying DEGs in batch mode with &lt;em>&lt;code>DESeq2&lt;/code>* (Love, Huber, and Anders 2014) for any number of
pairwise sample comparisons specified under the &lt;em>&lt;code>cmp&lt;/code>* argument. Users
are strongly encouraged to consult the
&lt;/em>&lt;code>DESeq2&lt;/code>&lt;/em>&lt;a href="http://www.bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.pdf">&lt;/a> vignette
for more detailed information on this topic and how to properly run &lt;em>&lt;code>DESeq2&lt;/code>&lt;/em>
on data sets with more complex experimental designs.&lt;/p>
&lt;pre>&lt;code class="language-r">degseqDF &amp;lt;- run_DESeq2(countDF = countDFeByg, targets = targets, cmp = cmp[[1]],
independent = FALSE)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning in DESeqDataSet(se, design = design, ignoreRank): some variables in
## design formula are characters, converting to factors
&lt;/code>&lt;/pre>
&lt;p>Filter and plot DEG results for up and down-regulated genes.&lt;/p>
&lt;pre>&lt;code class="language-r">DEG_list2 &amp;lt;- filterDEGs(degDF = degseqDF, filter = c(Fold = 2, FDR = 10))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="../../en/sp/spr/steps_files/figure-html/deseq2_deg_counts-1.png" width="672" />&lt;/p>
&lt;div align="center">
&lt;p>&lt;strong>Figure 4:&lt;/strong> Up and down regulated DEGs identified by &lt;em>&lt;code>DESeq2&lt;/code>&lt;/em>.&lt;/p>
&lt;/div>
&lt;/br>
&lt;h2 id="venn-diagrams">Venn Diagrams&lt;/h2>
&lt;p>The function &lt;em>&lt;code>overLapper&lt;/code>&lt;/em> can compute Venn intersects for large numbers of sample sets (up to 20 or more) and &lt;em>&lt;code>vennPlot&lt;/code>&lt;/em> can plot 2-5 way Venn diagrams. A useful feature is the possibility to combine the counts from several Venn comparisons with the same number of sample sets in a single Venn diagram (here for 4 up and down DEG sets).&lt;/p>
&lt;pre>&lt;code class="language-r">vennsetup &amp;lt;- overLapper(DEG_list$Up[6:9], type = &amp;quot;vennsets&amp;quot;)
vennsetdown &amp;lt;- overLapper(DEG_list$Down[6:9], type = &amp;quot;vennsets&amp;quot;)
vennPlot(list(vennsetup, vennsetdown), mymain = &amp;quot;&amp;quot;, mysub = &amp;quot;&amp;quot;, colmode = 2, ccol = c(&amp;quot;blue&amp;quot;,
&amp;quot;red&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="../../en/sp/spr/steps_files/figure-html/vennplot-1.png" width="672" />&lt;/p>
&lt;div align="center">
&lt;p>&lt;strong>Figure 5:&lt;/strong> Venn Diagram for 4 Up and Down DEG Sets.&lt;/p>
&lt;/div>
&lt;/br>
&lt;h2 id="go-term-enrichment-analysis-of-degs">GO term enrichment analysis of DEGs&lt;/h2>
&lt;h3 id="obtain-gene-to-go-mappings">Obtain gene-to-GO mappings&lt;/h3>
&lt;p>The following shows how to obtain gene-to-GO mappings from &lt;em>&lt;code>biomaRt&lt;/code>&lt;/em> (here for &lt;em>A. thaliana&lt;/em>) and how to organize them for the downstream GO term enrichment analysis. Alternatively, the gene-to-GO mappings can be obtained for many organisms from Bioconductor’s &lt;em>&lt;code>*.db&lt;/code>&lt;/em> genome annotation packages or GO annotation files provided by various genome databases. For each annotation, this relatively slow preprocessing step needs to be performed only once. Subsequently, the preprocessed data can be loaded with the &lt;em>&lt;code>load&lt;/code>&lt;/em> function as shown in the next subsection.&lt;/p>
&lt;pre>&lt;code class="language-r">library(&amp;quot;biomaRt&amp;quot;)
listMarts() # To choose BioMart database
listMarts(host = &amp;quot;plants.ensembl.org&amp;quot;)
m &amp;lt;- useMart(&amp;quot;plants_mart&amp;quot;, host = &amp;quot;plants.ensembl.org&amp;quot;)
listDatasets(m)
m &amp;lt;- useMart(&amp;quot;plants_mart&amp;quot;, dataset = &amp;quot;athaliana_eg_gene&amp;quot;, host = &amp;quot;plants.ensembl.org&amp;quot;)
listAttributes(m) # Choose data types you want to download
go &amp;lt;- getBM(attributes = c(&amp;quot;go_id&amp;quot;, &amp;quot;tair_locus&amp;quot;, &amp;quot;namespace_1003&amp;quot;), mart = m)
go &amp;lt;- go[go[, 3] != &amp;quot;&amp;quot;, ]
go[, 3] &amp;lt;- as.character(go[, 3])
go[go[, 3] == &amp;quot;molecular_function&amp;quot;, 3] &amp;lt;- &amp;quot;F&amp;quot;
go[go[, 3] == &amp;quot;biological_process&amp;quot;, 3] &amp;lt;- &amp;quot;P&amp;quot;
go[go[, 3] == &amp;quot;cellular_component&amp;quot;, 3] &amp;lt;- &amp;quot;C&amp;quot;
go[1:4, ]
dir.create(&amp;quot;./data/GO&amp;quot;)
write.table(go, &amp;quot;data/GO/GOannotationsBiomart_mod.txt&amp;quot;, quote = FALSE, row.names = FALSE,
col.names = FALSE, sep = &amp;quot;\t&amp;quot;)
catdb &amp;lt;- makeCATdb(myfile = &amp;quot;data/GO/GOannotationsBiomart_mod.txt&amp;quot;, lib = NULL, org = &amp;quot;&amp;quot;,
colno = c(1, 2, 3), idconv = NULL)
save(catdb, file = &amp;quot;data/GO/catdb.RData&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h3 id="batch-go-term-enrichment-analysis">Batch GO term enrichment analysis&lt;/h3>
&lt;p>Apply the enrichment analysis to the DEG sets obtained in the above differential expression analysis. Note, in the following example the &lt;em>FDR&lt;/em> filter is set here to an unreasonably high value, simply because of the small size of the toy data set used in this vignette. Batch enrichment analysis of many gene sets is performed with the &lt;em>&lt;code>GOCluster_Report&lt;/code>&lt;/em> function. When &lt;em>&lt;code>method=&amp;quot;all&amp;quot;&lt;/code>&lt;/em>, it returns all GO terms passing the p-value cutoff specified under the &lt;em>&lt;code>cutoff&lt;/code>&lt;/em> arguments. When &lt;em>&lt;code>method=&amp;quot;slim&amp;quot;&lt;/code>&lt;/em>, it returns only the GO terms specified under the &lt;em>&lt;code>myslimv&lt;/code>&lt;/em> argument. The given example shows how one can obtain such a GO slim vector from BioMart for a specific organism.&lt;/p>
&lt;pre>&lt;code class="language-r">load(&amp;quot;data/GO/catdb.RData&amp;quot;)
DEG_list &amp;lt;- filterDEGs(degDF = edgeDF, filter = c(Fold = 2, FDR = 50), plot = FALSE)
up_down &amp;lt;- DEG_list$UporDown
names(up_down) &amp;lt;- paste(names(up_down), &amp;quot;_up_down&amp;quot;, sep = &amp;quot;&amp;quot;)
up &amp;lt;- DEG_list$Up
names(up) &amp;lt;- paste(names(up), &amp;quot;_up&amp;quot;, sep = &amp;quot;&amp;quot;)
down &amp;lt;- DEG_list$Down
names(down) &amp;lt;- paste(names(down), &amp;quot;_down&amp;quot;, sep = &amp;quot;&amp;quot;)
DEGlist &amp;lt;- c(up_down, up, down)
DEGlist &amp;lt;- DEGlist[sapply(DEGlist, length) &amp;gt; 0]
BatchResult &amp;lt;- GOCluster_Report(catdb = catdb, setlist = DEGlist, method = &amp;quot;all&amp;quot;,
id_type = &amp;quot;gene&amp;quot;, CLSZ = 2, cutoff = 0.9, gocats = c(&amp;quot;MF&amp;quot;, &amp;quot;BP&amp;quot;, &amp;quot;CC&amp;quot;), recordSpecGO = NULL)
library(&amp;quot;biomaRt&amp;quot;)
m &amp;lt;- useMart(&amp;quot;plants_mart&amp;quot;, dataset = &amp;quot;athaliana_eg_gene&amp;quot;, host = &amp;quot;plants.ensembl.org&amp;quot;)
goslimvec &amp;lt;- as.character(getBM(attributes = c(&amp;quot;goslim_goa_accession&amp;quot;), mart = m)[,
1])
BatchResultslim &amp;lt;- GOCluster_Report(catdb = catdb, setlist = DEGlist, method = &amp;quot;slim&amp;quot;,
id_type = &amp;quot;gene&amp;quot;, myslimv = goslimvec, CLSZ = 10, cutoff = 0.01, gocats = c(&amp;quot;MF&amp;quot;,
&amp;quot;BP&amp;quot;, &amp;quot;CC&amp;quot;), recordSpecGO = NULL)
&lt;/code>&lt;/pre>
&lt;h3 id="plot-batch-go-term-results">Plot batch GO term results&lt;/h3>
&lt;p>The &lt;em>&lt;code>data.frame&lt;/code>&lt;/em> generated by &lt;em>&lt;code>GOCluster_Report&lt;/code>&lt;/em> can be plotted with the &lt;em>&lt;code>goBarplot&lt;/code>&lt;/em> function. Because of the variable size of the sample sets, it may not always be desirable to show the results from different DEG sets in the same bar plot. Plotting single sample sets is achieved by subsetting the input data frame as shown in the first line of the following example.&lt;/p>
&lt;pre>&lt;code class="language-r">gos &amp;lt;- BatchResultslim[grep(&amp;quot;M6-V6_up_down&amp;quot;, BatchResultslim$CLID), ]
gos &amp;lt;- BatchResultslim
pdf(&amp;quot;GOslimbarplotMF.pdf&amp;quot;, height = 8, width = 10)
goBarplot(gos, gocat = &amp;quot;MF&amp;quot;)
dev.off()
goBarplot(gos, gocat = &amp;quot;BP&amp;quot;)
goBarplot(gos, gocat = &amp;quot;CC&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="GOslimbarplotMF.png" alt="">&lt;/p>
&lt;div align="center">
&lt;p>&lt;strong>Figure 6:&lt;/strong> GO Slim Barplot for MF Ontology.&lt;/p>
&lt;/div>
&lt;/br>
&lt;h2 id="clustering-and-heat-maps">Clustering and heat maps&lt;/h2>
&lt;p>The following example performs hierarchical clustering on the &lt;em>&lt;code>rlog&lt;/code>&lt;/em> transformed expression matrix subsetted by the DEGs identified in the
above differential expression analysis. It uses a Pearson correlation-based distance measure and complete linkage for cluster join.&lt;/p>
&lt;pre>&lt;code class="language-r">library(pheatmap)
geneids &amp;lt;- unique(as.character(unlist(DEG_list[[1]])))
y &amp;lt;- assay(rlog(dds))[geneids, ]
pdf(&amp;quot;heatmap1.pdf&amp;quot;)
pheatmap(y, scale = &amp;quot;row&amp;quot;, clustering_distance_rows = &amp;quot;correlation&amp;quot;, clustering_distance_cols = &amp;quot;correlation&amp;quot;)
dev.off()
&lt;/code>&lt;/pre>
&lt;center>
&lt;img src="heatmap1.png">
&lt;/center>
&lt;div align="center">
&lt;p>&lt;strong>Figure 7:&lt;/strong> Heat map with hierarchical clustering dendrograms of DEGs.&lt;/p>
&lt;/div>
&lt;/br>
&lt;h1 id="references">References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-Kim2015-ve" class="csl-entry">
&lt;p>Kim, Daehwan, Ben Langmead, and Steven L Salzberg. 2015. “HISAT: A Fast Spliced Aligner with Low Memory Requirements.” &lt;em>Nat. Methods&lt;/em> 12 (4): 357–60.&lt;/p>
&lt;/div>
&lt;div id="ref-Kim2013-vg" class="csl-entry">
&lt;p>Kim, Daehwan, Geo Pertea, Cole Trapnell, Harold Pimentel, Ryan Kelley, and Steven L Salzberg. 2013. “TopHat2: Accurate Alignment of Transcriptomes in the Presence of Insertions, Deletions and Gene Fusions.” &lt;em>Genome Biol.&lt;/em> 14 (4): R36. &lt;a href="https://doi.org/10.1186/gb-2013-14-4-r36">https://doi.org/10.1186/gb-2013-14-4-r36&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-Langmead2012-bs" class="csl-entry">
&lt;p>Langmead, Ben, and Steven L Salzberg. 2012. “Fast Gapped-Read Alignment with Bowtie 2.” &lt;em>Nat. Methods&lt;/em> 9 (4): 357–59. &lt;a href="https://doi.org/10.1038/nmeth.1923">https://doi.org/10.1038/nmeth.1923&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-Love2014-sh" class="csl-entry">
&lt;p>Love, Michael, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for &lt;span class="nocase">RNA-seq&lt;/span> Data with DESeq2.” &lt;em>Genome Biol.&lt;/em> 15 (12): 550. &lt;a href="https://doi.org/10.1186/s13059-014-0550-8">https://doi.org/10.1186/s13059-014-0550-8&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-Rsamtools" class="csl-entry">
&lt;p>Morgan, Martin, Hervé Pagès, Valerie Obenchain, and Nathaniel Hayden. 2019. &lt;em>Rsamtools: Binary Alignment (BAM), FASTA, Variant Call (BCF), and Tabix File Import&lt;/em>. &lt;a href="http://bioconductor.org/packages/Rsamtools">http://bioconductor.org/packages/Rsamtools&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-Robinson2010-uk" class="csl-entry">
&lt;p>Robinson, M D, D J McCarthy, and G K Smyth. 2010. “edgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” &lt;em>Bioinformatics&lt;/em> 26 (1): 139–40. &lt;a href="https://doi.org/10.1093/bioinformatics/btp616">https://doi.org/10.1093/bioinformatics/btp616&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-Wu2010-iq" class="csl-entry">
&lt;p>Wu, T D, and S Nacu. 2010. “Fast and &lt;span class="nocase">SNP-tolerant&lt;/span> Detection of Complex Variants and Splicing in Short Reads.” &lt;em>Bioinformatics&lt;/em> 26 (7): 873–81. &lt;a href="https://doi.org/10.1093/bioinformatics/btq057">https://doi.org/10.1093/bioinformatics/btq057&lt;/a>.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Sp: Workflow Templates</title><link>/sp/spr/templates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/templates/</guid><description>
&lt;!--
- Compile from command-line
Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::html_document'), clean=F); knitr::knit('systemPipeR.Rmd', tangle=TRUE)"; Rscript ../md2jekyll.R systemPipeR.knit.md 2; Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::pdf_document'))"
-->
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
document.querySelector("h1").className = "title";
});
&lt;/script>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
var links = document.links;
for (var i = 0, linksLength = links.length; i &lt; linksLength; i++)
if (links[i].hostname != window.location.hostname)
links[i].target = '_blank';
});
&lt;/script>
&lt;h1 id="workflow-templates">Workflow templates&lt;/h1>
&lt;p>The intended way of running &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em> workflows is via &lt;em>&lt;code>*.Rmd&lt;/code>&lt;/em> files, which
can be executed either line-wise in interactive mode or with a single command from
R or the command-line. This way comprehensive and reproducible analysis reports
can be generated in PDF or HTML format in a fully automated manner by making use
of the highly functional reporting utilities available for R.
The following shows how to execute a workflow (&lt;em>e.g.&lt;/em>, systemPipeRNAseq.Rmd)
from the command-line.&lt;/p>
&lt;pre>&lt;code class="language-bash">Rscript -e &amp;quot;rmarkdown::render('systemPipeRNAseq.Rmd')&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Templates for setting up custom project reports are provided as *&lt;code>*.Rmd&lt;/code>* files by the helper package *&lt;code>systemPipeRdata&lt;/code>* and in the vignettes subdirectory of &lt;em>&lt;code>systemPipeR&lt;/code>&lt;em>. The corresponding HTML of these report templates are available here: &lt;/em>&lt;code>systemPipeRNAseq&lt;/code>&lt;/em>&lt;a href="http://www.bioconductor.org/packages/devel/data/experiment/vignettes/systemPipeRdata/inst/doc/systemPipeRNAseq.html">&lt;/a>, &lt;a href="http://www.bioconductor.org/packages/devel/data/experiment/vignettes/systemPipeRdata/inst/doc/systemPipeRIBOseq.html">&lt;em>&lt;code>systemPipeRIBOseq&lt;/code>&lt;/em>&lt;/a>, &lt;a href="http://www.bioconductor.org/packages/devel/data/experiment/vignettes/systemPipeRdata/inst/doc/systemPipeChIPseq.html">&lt;em>&lt;code>systemPipeChIPseq&lt;/code>&lt;/em>&lt;/a> and &lt;a href="http://www.bioconductor.org/packages/devel/data/experiment/vignettes/systemPipeRdata/inst/doc/systemPipeVARseq.html">&lt;em>&lt;code>systemPipeVARseq&lt;/code>&lt;/em>&lt;/a>. To work with &lt;em>&lt;code>*.Rnw&lt;/code>* or &lt;em>&lt;code>*.Rmd&lt;/code>* files efficiently, basic knowledge of &lt;/em>&lt;code>Sweave&lt;/code>&lt;/em>&lt;a href="https://www.stat.uni-muenchen.de/~leisch/Sweave/">&lt;/a> or &lt;a href="http://yihui.name/knitr/">&lt;em>&lt;code>knitr&lt;/code>&lt;/em>&lt;/a> and &lt;a href="http://www.latex-project.org/">&lt;em>&lt;code>Latex&lt;/code>&lt;/em>&lt;/a> or &lt;a href="http://rmarkdown.rstudio.com/">&lt;em>&lt;code>R Markdown v2&lt;/code>&lt;/em>&lt;/a> is required.&lt;/p>
&lt;h2 id="rna-seq-sample">RNA-Seq sample&lt;/h2>
&lt;p>Load the RNA-Seq sample workflow into your current working directory.&lt;/p>
&lt;pre>&lt;code class="language-r">library(systemPipeRdata)
genWorkenvir(workflow = &amp;quot;rnaseq&amp;quot;)
setwd(&amp;quot;rnaseq&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h3 id="run-workflow">Run workflow&lt;/h3>
&lt;p>Next, run the chosen sample workflow &lt;em>&lt;code>systemPipeRNAseq&lt;/code>&lt;/em> (&lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/rnaseq/systemPipeRNAseq.pdf?raw=true">PDF&lt;/a>, &lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/rnaseq/systemPipeRNAseq.Rmd">Rmd&lt;/a>) by executing from the command-line &lt;em>&lt;code>make -B&lt;/code>&lt;/em> within the &lt;em>&lt;code>rnaseq&lt;/code>&lt;/em> directory. Alternatively, one can run the code from the provided &lt;em>&lt;code>*.Rmd&lt;/code>&lt;/em> template file from within R interactively.&lt;/p>
&lt;p>The workflow includes following steps:&lt;/p>
&lt;ol>
&lt;li>Read preprocessing
&lt;ul>
&lt;li>Quality filtering (trimming)&lt;/li>
&lt;li>FASTQ quality report&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Alignments: &lt;em>&lt;code>Tophat2&lt;/code>&lt;/em> (or any other RNA-Seq aligner)&lt;/li>
&lt;li>Alignment stats&lt;/li>
&lt;li>Read counting&lt;/li>
&lt;li>Sample-wise correlation analysis&lt;/li>
&lt;li>Analysis of differentially expressed genes (DEGs)&lt;/li>
&lt;li>GO term enrichment analysis&lt;/li>
&lt;li>Gene-wise clustering&lt;/li>
&lt;/ol>
&lt;h2 id="chip-seq-sample">ChIP-Seq sample&lt;/h2>
&lt;p>Load the ChIP-Seq sample workflow into your current working directory.&lt;/p>
&lt;pre>&lt;code class="language-r">library(systemPipeRdata)
genWorkenvir(workflow = &amp;quot;chipseq&amp;quot;)
setwd(&amp;quot;chipseq&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h3 id="run-workflow-1">Run workflow&lt;/h3>
&lt;p>Next, run the chosen sample workflow &lt;em>&lt;code>systemPipeChIPseq_single&lt;/code>&lt;/em> (&lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/chipseq/systemPipeChIPseq.pdf?raw=true">PDF&lt;/a>, &lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/chipseq/systemPipeChIPseq.Rmd">Rmd&lt;/a>) by executing from the command-line &lt;em>&lt;code>make -B&lt;/code>&lt;/em> within the &lt;em>&lt;code>chipseq&lt;/code>&lt;/em> directory. Alternatively, one can run the code from the provided &lt;em>&lt;code>*.Rmd&lt;/code>&lt;/em> template file from within R interactively.&lt;/p>
&lt;p>The workflow includes the following steps:&lt;/p>
&lt;ol>
&lt;li>Read preprocessing
&lt;ul>
&lt;li>Quality filtering (trimming)&lt;/li>
&lt;li>FASTQ quality report&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Alignments: &lt;em>&lt;code>Bowtie2&lt;/code>&lt;/em> or &lt;em>&lt;code>rsubread&lt;/code>&lt;/em>&lt;/li>
&lt;li>Alignment stats&lt;/li>
&lt;li>Peak calling: &lt;em>&lt;code>MACS2&lt;/code>&lt;/em>, &lt;em>&lt;code>BayesPeak&lt;/code>&lt;/em>&lt;/li>
&lt;li>Peak annotation with genomic context&lt;/li>
&lt;li>Differential binding analysis&lt;/li>
&lt;li>GO term enrichment analysis&lt;/li>
&lt;li>Motif analysis&lt;/li>
&lt;/ol>
&lt;h2 id="var-seq-sample">VAR-Seq sample&lt;/h2>
&lt;h3 id="var-seq-workflow-for-the-single-machine">VAR-Seq workflow for the single machine&lt;/h3>
&lt;p>Load the VAR-Seq sample workflow into your current working directory.&lt;/p>
&lt;pre>&lt;code class="language-r">library(systemPipeRdata)
genWorkenvir(workflow = &amp;quot;varseq&amp;quot;)
setwd(&amp;quot;varseq&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h3 id="run-workflow-2">Run workflow&lt;/h3>
&lt;p>Next, run the chosen sample workflow &lt;em>&lt;code>systemPipeVARseq_single&lt;/code>&lt;/em> (&lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/varseq/systemPipeVARseq_single.pdf?raw=true">PDF&lt;/a>, &lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/varseq/systemPipeVARseq_single.Rmd">Rmd&lt;/a>) by executing from the command-line &lt;em>&lt;code>make -B&lt;/code>&lt;/em> within the &lt;em>&lt;code>varseq&lt;/code>&lt;/em> directory. Alternatively, one can run the code from the provided &lt;em>&lt;code>*.Rmd&lt;/code>&lt;/em> template file from within R interactively.&lt;/p>
&lt;p>The workflow includes following steps:&lt;/p>
&lt;ol>
&lt;li>Read preprocessing
&lt;ul>
&lt;li>Quality filtering (trimming)&lt;/li>
&lt;li>FASTQ quality report&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Alignments: &lt;em>&lt;code>gsnap&lt;/code>&lt;/em>, &lt;em>&lt;code>bwa&lt;/code>&lt;/em>&lt;/li>
&lt;li>Variant calling: &lt;em>&lt;code>VariantTools&lt;/code>&lt;/em>, &lt;em>&lt;code>GATK&lt;/code>&lt;/em>, &lt;em>&lt;code>BCFtools&lt;/code>&lt;/em>&lt;/li>
&lt;li>Variant filtering: &lt;em>&lt;code>VariantTools&lt;/code>&lt;/em> and &lt;em>&lt;code>VariantAnnotation&lt;/code>&lt;/em>&lt;/li>
&lt;li>Variant annotation: &lt;em>&lt;code>VariantAnnotation&lt;/code>&lt;/em>&lt;/li>
&lt;li>Combine results from many samples&lt;/li>
&lt;li>Summary statistics of samples&lt;/li>
&lt;/ol>
&lt;h3 id="var-seq-workflow-for-computer-cluster">VAR-Seq workflow for computer cluster&lt;/h3>
&lt;p>The workflow template provided for this step is called &lt;em>&lt;code>systemPipeVARseq.Rmd&lt;/code>&lt;/em> (&lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/varseq/systemPipeVARseq.pdf?raw=true">PDF&lt;/a>, &lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/varseq/systemPipeVARseq.Rmd">Rmd&lt;/a>).
It runs the above VAR-Seq workflow in parallel on multiple compute nodes of an HPC system using Slurm as the scheduler.&lt;/p>
&lt;h2 id="ribo-seq-sample">Ribo-Seq sample&lt;/h2>
&lt;p>Load the Ribo-Seq sample workflow into your current working directory.&lt;/p>
&lt;pre>&lt;code class="language-r">library(systemPipeRdata)
genWorkenvir(workflow = &amp;quot;riboseq&amp;quot;)
setwd(&amp;quot;riboseq&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h3 id="run-workflow-3">Run workflow&lt;/h3>
&lt;p>Next, run the chosen sample workflow &lt;em>&lt;code>systemPipeRIBOseq&lt;/code>&lt;/em> (&lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/riboseq/systemPipeRIBOseq.pdf?raw=true">PDF&lt;/a>, &lt;a href="https://github.com/tgirke/systemPipeRdata/blob/master/inst/extdata/workflows/ribseq/systemPipeRIBOseq.Rmd">Rmd&lt;/a>) by executing from the command-line &lt;em>&lt;code>make -B&lt;/code>&lt;/em> within the &lt;em>&lt;code>ribseq&lt;/code>&lt;/em> directory. Alternatively, one can run the code from the provided &lt;em>&lt;code>*.Rmd&lt;/code>&lt;/em> template file from within R interactively.&lt;/p>
&lt;p>The workflow includes following steps:&lt;/p>
&lt;ol>
&lt;li>Read preprocessing
&lt;ul>
&lt;li>Adaptor trimming and quality filtering&lt;/li>
&lt;li>FASTQ quality report&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Alignments: &lt;em>&lt;code>Tophat2&lt;/code>&lt;/em> (or any other RNA-Seq aligner)&lt;/li>
&lt;li>Alignment stats&lt;/li>
&lt;li>Compute read distribution across genomic features&lt;/li>
&lt;li>Adding custom features to the workflow (e.g. uORFs)&lt;/li>
&lt;li>Genomic read coverage along with transcripts&lt;/li>
&lt;li>Read counting&lt;/li>
&lt;li>Sample-wise correlation analysis&lt;/li>
&lt;li>Analysis of differentially expressed genes (DEGs)&lt;/li>
&lt;li>GO term enrichment analysis&lt;/li>
&lt;li>Gene-wise clustering&lt;/li>
&lt;li>Differential ribosome binding (translational efficiency)&lt;/li>
&lt;/ol>
&lt;h1 id="version-information">Version information&lt;/h1>
&lt;p>&lt;strong>Note:&lt;/strong> the most recent version of this tutorial can be found &lt;a href="http://www.bioconductor.org/packages/devel/bioc/vignettes/systemPipeR/inst/doc/systemPipeR.html">here&lt;/a>.&lt;/p>
&lt;pre>&lt;code class="language-r">sessionInfo()
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## R version 4.0.3 (2020-10-10)
## Platform: x86_64-pc-linux-gnu (64-bit)
## Running under: Ubuntu 20.04.2 LTS
##
## Matrix products: default
## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
## LAPACK: /home/dcassol/src/R-4.0.3/lib/libRlapack.so
##
## locale:
## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C
## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8
## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8
## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C
## [9] LC_ADDRESS=C LC_TELEPHONE=C
## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
##
## attached base packages:
## [1] stats4 parallel stats graphics grDevices utils datasets
## [8] methods base
##
## other attached packages:
## [1] magrittr_2.0.1 batchtools_0.9.15
## [3] ape_5.4-1 ggplot2_3.3.3
## [5] systemPipeR_1.25.6 ShortRead_1.48.0
## [7] GenomicAlignments_1.26.0 SummarizedExperiment_1.20.0
## [9] Biobase_2.50.0 MatrixGenerics_1.2.1
## [11] matrixStats_0.58.0 BiocParallel_1.24.1
## [13] Rsamtools_2.6.0 Biostrings_2.58.0
## [15] XVector_0.30.0 GenomicRanges_1.42.0
## [17] GenomeInfoDb_1.26.2 IRanges_2.24.1
## [19] S4Vectors_0.28.1 BiocGenerics_0.36.0
## [21] BiocStyle_2.18.1
##
## loaded via a namespace (and not attached):
## [1] colorspace_2.0-0 rjson_0.2.20 hwriter_1.3.2
## [4] ellipsis_0.3.1 bit64_4.0.5 AnnotationDbi_1.52.0
## [7] xml2_1.3.2 codetools_0.2-18 splines_4.0.3
## [10] cachem_1.0.3 knitr_1.31 jsonlite_1.7.2
## [13] annotate_1.68.0 GO.db_3.12.1 dbplyr_2.1.0
## [16] png_0.1-7 pheatmap_1.0.12 graph_1.68.0
## [19] BiocManager_1.30.10 compiler_4.0.3 httr_1.4.2
## [22] GOstats_2.56.0 backports_1.2.1 assertthat_0.2.1
## [25] Matrix_1.3-2 fastmap_1.1.0 limma_3.46.0
## [28] formatR_1.7 htmltools_0.5.1.1 prettyunits_1.1.1
## [31] tools_4.0.3 gtable_0.3.0 glue_1.4.2
## [34] GenomeInfoDbData_1.2.4 Category_2.56.0 dplyr_1.0.4
## [37] rsvg_2.1 rappdirs_0.3.3 V8_3.4.0
## [40] Rcpp_1.0.6 vctrs_0.3.6 nlme_3.1-152
## [43] blogdown_1.1.7 rtracklayer_1.50.0 xfun_0.21
## [46] stringr_1.4.0 lifecycle_1.0.0.9000 XML_3.99-0.5
## [49] edgeR_3.32.1 zlibbioc_1.36.0 scales_1.1.1
## [52] BSgenome_1.58.0 VariantAnnotation_1.36.0 hms_1.0.0
## [55] RBGL_1.66.0 RColorBrewer_1.1-2 yaml_2.2.1
## [58] curl_4.3 memoise_2.0.0 biomaRt_2.46.3
## [61] latticeExtra_0.6-29 stringi_1.5.3 RSQLite_2.2.3
## [64] genefilter_1.72.1 checkmate_2.0.0 GenomicFeatures_1.42.1
## [67] DOT_0.1 rlang_0.4.10 pkgconfig_2.0.3
## [70] bitops_1.0-6 evaluate_0.14 lattice_0.20-41
## [73] purrr_0.3.4 bit_4.0.4 tidyselect_1.1.0
## [76] GSEABase_1.52.1 AnnotationForge_1.32.0 bookdown_0.21
## [79] R6_2.5.0 generics_0.1.0 base64url_1.4
## [82] DelayedArray_0.16.1 DBI_1.1.1 withr_2.4.1
## [85] pillar_1.4.7 survival_3.2-7 RCurl_1.98-1.2
## [88] tibble_3.0.6 crayon_1.4.1 BiocFileCache_1.14.0
## [91] rmarkdown_2.6 jpeg_0.1-8.1 progress_1.2.2
## [94] locfit_1.5-9.4 grid_4.0.3 data.table_1.13.6
## [97] blob_1.2.1 Rgraphviz_2.34.0 digest_0.6.27
## [100] xtable_1.8-4 brew_1.0-6 openssl_1.4.3
## [103] munsell_0.5.0 askpass_1.1
&lt;/code>&lt;/pre>
&lt;h1 id="references">References&lt;/h1></description></item><item><title>Sp: CWL syntax</title><link>/sp/spr/cwl_syntax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/cwl_syntax/</guid><description>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
document.querySelector("h1").className = "title";
});
&lt;/script>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
var links = document.links;
for (var i = 0, linksLength = links.length; i &lt; linksLength; i++)
if (links[i].hostname != window.location.hostname)
links[i].target = '_blank';
});
&lt;/script>
&lt;h2 id="cwl-syntax">CWL syntax&lt;/h2>
&lt;p>This section will introduce how CWL describes command-line tools and the
specification and terminology of each file.
For complete documentation, please check the CommandLineTools documentation &lt;a href="https://www.commonwl.org/v1.2/CommandLineTool.html">here&lt;/a>
and &lt;a href="https://www.commonwl.org/v1.2/Workflow.html">here&lt;/a> for Workflows and the user guide &lt;a href="https://www.commonwl.org/user_guide/">here&lt;/a>.&lt;/p>
&lt;p>CWL command-line specifications are written in &lt;a href="http://yaml.org/">YAML&lt;/a> format.&lt;/p>
&lt;p>In CWL, files with the extension &lt;code>.cwl&lt;/code> define the parameters of a chosen
command-line step or workflow, while files with the extension &lt;code>.yml&lt;/code> define
the input variables of command-line steps.&lt;/p>
&lt;h3 id="cwl-commandlinetool">CWL &lt;code>CommandLineTool&lt;/code>&lt;/h3>
&lt;p>&lt;code>CommandLineTool&lt;/code> by CWL definition is a standalone process, with no interaction
if other programs, execute a program, and produce output.&lt;/p>
&lt;p>Let’s explore the &lt;code>*.cwl&lt;/code> file:&lt;/p>
&lt;pre>&lt;code class="language-r">dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
cwl &amp;lt;- yaml::read_yaml(file.path(dir_path, &amp;quot;example/example.cwl&amp;quot;))
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The &lt;code>cwlVersion&lt;/code> component shows the CWL specification version used by the document.&lt;/li>
&lt;li>The &lt;code>class&lt;/code> component shows this document describes a &lt;code>CommandLineTool.&lt;/code>
Note that CWL has another &lt;code>class&lt;/code>, called &lt;code>Workflow&lt;/code> which represents a union of one
or more command-line tools together.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">cwl[1:2]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $cwlVersion
## [1] &amp;quot;v1.0&amp;quot;
##
## $class
## [1] &amp;quot;CommandLineTool&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>baseCommand&lt;/code> component provides the name of the software that we desire to execute.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">cwl[3]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $baseCommand
## [1] &amp;quot;echo&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The &lt;code>inputs&lt;/code> section provides the input information to run the tool. Important
components of this section are:
&lt;ul>
&lt;li>&lt;code>id&lt;/code>: each input has an id describing the input name;&lt;/li>
&lt;li>&lt;code>type&lt;/code>: describe the type of input value (string, int, long, float, double,
File, Directory or Any);&lt;/li>
&lt;li>&lt;code>inputBinding&lt;/code>: It is optional. This component indicates if the input
parameter should appear on the command-line. If this component is missing
when describing an input parameter, it will not appear in the command-line
but can be used to build the command-line.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">cwl[4]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $inputs
## $inputs$message
## $inputs$message$type
## [1] &amp;quot;string&amp;quot;
##
## $inputs$message$inputBinding
## $inputs$message$inputBinding$position
## [1] 1
##
##
##
## $inputs$SampleName
## $inputs$SampleName$type
## [1] &amp;quot;string&amp;quot;
##
##
## $inputs$results_path
## $inputs$results_path$type
## [1] &amp;quot;Directory&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The &lt;code>outputs&lt;/code> section should provide a list of the expected outputs after running the command-line tools. Important
components of this section are:
&lt;ul>
&lt;li>&lt;code>id&lt;/code>: each input has an id describing the output name;&lt;/li>
&lt;li>&lt;code>type&lt;/code>: describe the type of output value (string, int, long, float, double,
File, Directory, Any or &lt;code>stdout&lt;/code>);&lt;/li>
&lt;li>&lt;code>outputBinding&lt;/code>: This component defines how to set the outputs values. The &lt;code>glob&lt;/code> component will define the name of the output value.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">cwl[5]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $outputs
## $outputs$string
## $outputs$string$type
## [1] &amp;quot;stdout&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>stdout&lt;/code>: component to specify a &lt;code>filename&lt;/code> to capture standard output.
Note here we are using a syntax that takes advantage of the inputs section,
using results_path parameter and also the &lt;code>SampleName&lt;/code> to construct the output &lt;code>filename.&lt;/code>&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">cwl[6]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $stdout
## [1] &amp;quot;$(inputs.results_path.basename)/$(inputs.SampleName).txt&amp;quot;
&lt;/code>&lt;/pre>
&lt;h3 id="cwl-workflow">CWL &lt;code>Workflow&lt;/code>&lt;/h3>
&lt;p>&lt;code>Workflow&lt;/code> class in CWL is defined by multiple process steps, where can have
interdependencies between the steps, and the output for one step can be used as
input in the further steps.&lt;/p>
&lt;pre>&lt;code class="language-r">cwl.wf &amp;lt;- yaml::read_yaml(file.path(dir_path, &amp;quot;example/workflow_example.cwl&amp;quot;))
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The &lt;code>cwlVersion&lt;/code> component shows the CWL specification version used by the document.&lt;/li>
&lt;li>The &lt;code>class&lt;/code> component shows this document describes a &lt;code>Workflow&lt;/code>.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">cwl.wf[1:2]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $class
## [1] &amp;quot;Workflow&amp;quot;
##
## $cwlVersion
## [1] &amp;quot;v1.0&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The &lt;code>inputs&lt;/code> section describes the inputs of the workflow.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">cwl.wf[3]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $inputs
## $inputs$message
## [1] &amp;quot;string&amp;quot;
##
## $inputs$SampleName
## [1] &amp;quot;string&amp;quot;
##
## $inputs$results_path
## [1] &amp;quot;Directory&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The &lt;code>outputs&lt;/code> section describes the outputs of the workflow.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">cwl.wf[4]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $outputs
## $outputs$string
## $outputs$string$outputSource
## [1] &amp;quot;echo/string&amp;quot;
##
## $outputs$string$type
## [1] &amp;quot;stdout&amp;quot;
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>The &lt;code>steps&lt;/code> section describes the steps of the workflow. In this simple example,
we demonstrate one step.&lt;/li>
&lt;/ul>
&lt;pre>&lt;code class="language-r">cwl.wf[5]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $steps
## $steps$echo
## $steps$echo$`in`
## $steps$echo$`in`$message
## [1] &amp;quot;message&amp;quot;
##
## $steps$echo$`in`$SampleName
## [1] &amp;quot;SampleName&amp;quot;
##
## $steps$echo$`in`$results_path
## [1] &amp;quot;results_path&amp;quot;
##
##
## $steps$echo$out
## [1] &amp;quot;[string]&amp;quot;
##
## $steps$echo$run
## [1] &amp;quot;example/example.cwl&amp;quot;
&lt;/code>&lt;/pre>
&lt;h3 id="cwl-input-parameter">CWL Input Parameter&lt;/h3>
&lt;p>Next, let’s explore the &lt;em>.yml&lt;/em> file, which provide the input parameter values for all
the components we describe above.&lt;/p>
&lt;p>For this simple example, we have three parameters defined:&lt;/p>
&lt;pre>&lt;code class="language-r">yaml::read_yaml(file.path(dir_path, &amp;quot;example/example_single.yml&amp;quot;))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $message
## [1] &amp;quot;Hello World!&amp;quot;
##
## $SampleName
## [1] &amp;quot;M1&amp;quot;
##
## $results_path
## $results_path$class
## [1] &amp;quot;Directory&amp;quot;
##
## $results_path$path
## [1] &amp;quot;./results&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Note that if we define an input component in the &lt;em>.cwl&lt;/em> file, this value needs
to be also defined here in the &lt;em>.yml&lt;/em> file.&lt;/p>
&lt;h3 id="reference">Reference&lt;/h3></description></item><item><title>Sp: SPR and CWL</title><link>/sp/spr/cwl_and_spr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/cwl_and_spr/</guid><description>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
document.querySelector("h1").className = "title";
});
&lt;/script>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
var links = document.links;
for (var i = 0, linksLength = links.length; i &lt; linksLength; i++)
if (links[i].hostname != window.location.hostname)
links[i].target = '_blank';
});
&lt;/script>
&lt;h2 id="how-to-connect-cwl-description-files-within-systempiper">How to connect CWL description files within &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>&lt;/h2>
&lt;p>This section will demonstrate how to connect CWL parameters files to create
workflows. In addition, we will show how the workflow can be easily scalable
with &lt;em>&lt;code>systemPipeR&lt;/code>&lt;/em>.&lt;/p>
&lt;p>&lt;code>SYSargsList&lt;/code> container stores all the information and instructions needed for processing
a set of input files with a single or many command-line steps within a workflow
(i.e. several components of the software or several independent software tools).
The &lt;code>SYSargsList&lt;/code> object is created and fully populated with the &lt;code>SYSargsList&lt;/code> construct
function.
Full documentation of &lt;code>SYSargsList&lt;/code> management instances can be found &lt;a href="#sysargslist">here&lt;/a>
and &lt;a href="#appendstep">here&lt;/a>.&lt;/p>
&lt;p>The following imports a &lt;code>.cwl&lt;/code> file (here &lt;code>example.cwl&lt;/code>) for running the &lt;code>echo Hello World!&lt;/code>
example.&lt;/p>
&lt;pre>&lt;code class="language-r">HW &amp;lt;- SYSargsList(wf_file = &amp;quot;example/workflow_example.cwl&amp;quot;, input_file = &amp;quot;example/example_single.yml&amp;quot;,
dir_path = system.file(&amp;quot;extdata/cwl&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;))
HW
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. Step_x --&amp;gt; Status: Pending
## Total Files: 1 | Existing: 0 | Missing: 1
## 1.1. echo
## cmdlist: 1 | Pending: 1
##
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmdlist(HW)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $Step_x
## $Step_x$defaultid
## $Step_x$defaultid$echo
## [1] &amp;quot;echo Hello World! &amp;gt; results/M1.txt&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>However, we are limited to run just one command-line or one sample in this example.
To scale the command-line over many samples, a simple solution offered by &lt;code>systemPipeR&lt;/code>
is to provide a &lt;code>variable&lt;/code> for each of the parameters that we want to run with multiple samples.&lt;/p>
&lt;p>Let’s explore the example:&lt;/p>
&lt;pre>&lt;code class="language-r">dir_path &amp;lt;- system.file(&amp;quot;extdata/cwl&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
yml &amp;lt;- yaml::read_yaml(file.path(dir_path, &amp;quot;example/example.yml&amp;quot;))
yml
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $message
## [1] &amp;quot;_STRING_&amp;quot;
##
## $SampleName
## [1] &amp;quot;_SAMPLE_&amp;quot;
##
## $results_path
## $results_path$class
## [1] &amp;quot;Directory&amp;quot;
##
## $results_path$path
## [1] &amp;quot;./results&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>For the &lt;code>message&lt;/code> and &lt;code>SampleName&lt;/code> parameter, we are passing a variable connecting
with a third file called &lt;code>targets.&lt;/code>&lt;/p>
&lt;p>Now, let’s explore the &lt;code>targets&lt;/code> file structure:&lt;/p>
&lt;pre>&lt;code class="language-r">targetspath &amp;lt;- system.file(&amp;quot;extdata/cwl/example/targets_example.txt&amp;quot;, package = &amp;quot;systemPipeR&amp;quot;)
read.delim(targetspath, comment.char = &amp;quot;#&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Message SampleName
## 1 Hello World! M1
## 2 Hello USA! M2
## 3 Hello Bioconductor! M3
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>targets&lt;/code> file defines all input files or values and sample ids of an analysis workflow.
For this example, we have defined a string message for the &lt;code>echo&lt;/code> command-line tool,
in the first column that will be evaluated, and the second column is the
&lt;code>SampleName&lt;/code> id for each one of the messages.
Any number of additional columns can be added as needed.&lt;/p>
&lt;p>Users should note here, the usage of &lt;code>targets&lt;/code> files is optional when using
&lt;code>systemPipeR's&lt;/code> new CWL interface. Since for organizing experimental variables targets
files are extremely useful and user-friendly. Thus, we encourage users to keep using them.&lt;/p>
&lt;h3 id="how-to-connect-the-parameter-files-and-targets-file-information">How to connect the parameter files and &lt;code>targets&lt;/code> file information?&lt;/h3>
&lt;p>The constructor function creates an &lt;code>SYSargsList&lt;/code> S4 class object connecting three input files:&lt;/p>
&lt;ul>
&lt;li>CWL command-line specification file (&lt;code>wf_file&lt;/code> argument);&lt;/li>
&lt;li>Input variables (&lt;code>input_file&lt;/code> argument);&lt;/li>
&lt;li>Targets file (&lt;code>targets&lt;/code> argument).&lt;/li>
&lt;/ul>
&lt;p>As demonstrated above, the latter is optional for workflow steps lacking input files.
The connection between input variables (here defined by &lt;code>input_file&lt;/code> argument)
and the &lt;code>targets&lt;/code> file are defined under the &lt;code>inputvars&lt;/code> argument.
A named vector is required, where each element name needs to match with column
names in the &lt;code>targets&lt;/code> file, and the value must match the names of the &lt;em>.yml&lt;/em>
variables. This is used to replace the CWL variable and construct all the command-line
for that particular step.&lt;/p>
&lt;p>The variable pattern &lt;code>_XXXX_&lt;/code> is used to distinguish CWL variables that target
columns will replace. This pattern is recommended for consistency and easy identification
but not enforced.&lt;/p>
&lt;p>The following imports a &lt;code>.cwl&lt;/code> file (same example demonstrated above) for running
the &lt;code>echo Hello World&lt;/code> example. However, now we are connecting the variable defined
on the &lt;code>.yml&lt;/code> file with the &lt;code>targets&lt;/code> file inputs.&lt;/p>
&lt;pre>&lt;code class="language-r">HW_mul &amp;lt;- SYSargsList(step_name = &amp;quot;echo&amp;quot;, targets = targetspath, wf_file = &amp;quot;example/workflow_example.cwl&amp;quot;,
input_file = &amp;quot;example/example.yml&amp;quot;, dir_path = dir_path, inputvars = c(Message = &amp;quot;_STRING_&amp;quot;,
SampleName = &amp;quot;_SAMPLE_&amp;quot;))
HW_mul
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Instance of 'SYSargsList':
## WF Steps:
## 1. echo --&amp;gt; Status: Pending
## Total Files: 3 | Existing: 0 | Missing: 3
## 1.1. echo
## cmdlist: 3 | Pending: 3
##
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmdlist(HW_mul)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $echo
## $echo$M1
## $echo$M1$echo
## [1] &amp;quot;echo Hello World! &amp;gt; results/M1.txt&amp;quot;
##
##
## $echo$M2
## $echo$M2$echo
## [1] &amp;quot;echo Hello USA! &amp;gt; results/M2.txt&amp;quot;
##
##
## $echo$M3
## $echo$M3$echo
## [1] &amp;quot;echo Hello Bioconductor! &amp;gt; results/M3.txt&amp;quot;
&lt;/code>&lt;/pre>
&lt;div class="figure" style="text-align: center">
&lt;p>&lt;img src="../../home/dcassol/src/R-4.1.0/library/systemPipeR/extdata/images/SPR_CWL_hello.png" alt="WConnectivity between CWL param files and targets files." width="100%" />&lt;/p>
&lt;p class="caption">
Figure 1: WConnectivity between CWL param files and targets files.
&lt;/p>
&lt;/div>
&lt;h2 id="creating-the-cwl-param-files-from-the-command-line">Creating the CWL param files from the command-line&lt;/h2>
&lt;p>Users need to define the command-line in a pseudo-bash script format:&lt;/p>
&lt;pre>&lt;code class="language-r">command &amp;lt;- &amp;quot;
hisat2 \
-S &amp;lt;F, out: ./results/M1A.sam&amp;gt; \
-x &amp;lt;F: ./data/tair10.fasta&amp;gt; \
-k &amp;lt;int: 1&amp;gt; \
-min-intronlen &amp;lt;int: 30&amp;gt; \
-max-intronlen &amp;lt;int: 3000&amp;gt; \
-threads &amp;lt;int: 4&amp;gt; \
-U &amp;lt;F: ./data/SRR446027_1.fastq.gz&amp;gt;
&amp;quot;
&lt;/code>&lt;/pre>
&lt;h3 id="define-prefix-and-defaults">Define prefix and defaults&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>First line is the base command. Each line is an argument with its default value.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For argument lines (starting from the second line), any word before the first
space with leading &lt;code>-&lt;/code> or &lt;code>--&lt;/code> in each will be treated as a prefix, like &lt;code>-S&lt;/code> or
&lt;code>--min&lt;/code>. Any line without this first word will be treated as no prefix.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>All defaults are placed inside &lt;code>&amp;lt;...&amp;gt;&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>First argument is the input argument type. &lt;code>F&lt;/code> for “File,” “int,” “string” are unchanged.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Optional: use the keyword &lt;code>out&lt;/code> followed the type with a &lt;code>,&lt;/code> comma separation to
indicate if this argument is also an CWL output.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Then, use &lt;code>:&lt;/code> to separate keywords and default values, any non-space value after the &lt;code>:&lt;/code>
will be treated as the default value.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If any argument has no default value, just a flag, like &lt;code>--verbose&lt;/code>, there is no need to add any &lt;code>&amp;lt;...&amp;gt;&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="createparam-function">&lt;code>createParam&lt;/code> Function&lt;/h3>
&lt;p>&lt;code>createParam&lt;/code> function requires the &lt;code>string&lt;/code> as defined above as an input.&lt;/p>
&lt;p>First of all, the function will print the three components of the &lt;code>cwl&lt;/code> file:&lt;/p>
&lt;ul>
&lt;li>&lt;code>BaseCommand&lt;/code>: Specifies the program to execute.&lt;/li>
&lt;li>&lt;code>Inputs&lt;/code>: Defines the input parameters of the process.&lt;/li>
&lt;li>&lt;code>Outputs&lt;/code>: Defines the parameters representing the output of the process.&lt;/li>
&lt;/ul>
&lt;p>The four component is the original command-line.&lt;/p>
&lt;p>If in interactive mode, the function will verify that everything is correct and
will ask you to proceed. Here, the user can answer “no” and provide more
information at the string level. Another question is to save the param created here.&lt;/p>
&lt;p>If running the workflow in non-interactive mode, the &lt;code>createParam&lt;/code> function will
consider “yes” and returning the container.&lt;/p>
&lt;pre>&lt;code class="language-r">cmd &amp;lt;- createParam(command, writeParamFiles = FALSE)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## *****BaseCommand*****
## hisat2
## *****Inputs*****
## S:
## type: File
## preF: -S
## yml: ./results/M1A.sam
## x:
## type: File
## preF: -x
## yml: ./data/tair10.fasta
## k:
## type: int
## preF: -k
## yml: 1
## min-intronlen:
## type: int
## preF: -min-intronlen
## yml: 30
## max-intronlen:
## type: int
## preF: -max-intronlen
## yml: 3000
## threads:
## type: int
## preF: -threads
## yml: 4
## U:
## type: File
## preF: -U
## yml: ./data/SRR446027_1.fastq.gz
## *****Outputs*****
## output1:
## type: File
## value: ./results/M1A.sam
## *****Parsed raw command line*****
## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz
&lt;/code>&lt;/pre>
&lt;p>If the user chooses not to save the &lt;code>param&lt;/code> files on the above operation,
it can use the &lt;code>writeParamFiles&lt;/code> function.&lt;/p>
&lt;pre>&lt;code class="language-r">writeParamFiles(cmd, overwrite = TRUE)
&lt;/code>&lt;/pre>
&lt;h3 id="how-to-access-and-edit-param-files">How to access and edit param files&lt;/h3>
&lt;h4 id="print-a-component">Print a component&lt;/h4>
&lt;pre>&lt;code class="language-r">printParam(cmd, position = &amp;quot;baseCommand&amp;quot;) ## Print a baseCommand section
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## *****BaseCommand*****
## hisat2
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">printParam(cmd, position = &amp;quot;outputs&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## *****Outputs*****
## output1:
## type: File
## value: ./results/M1A.sam
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">printParam(cmd, position = &amp;quot;inputs&amp;quot;, index = 1:2) ## Print by index
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## *****Inputs*****
## S:
## type: File
## preF: -S
## yml: ./results/M1A.sam
## x:
## type: File
## preF: -x
## yml: ./data/tair10.fasta
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">printParam(cmd, position = &amp;quot;inputs&amp;quot;, index = -1:-2) ## Negative indexing printing to exclude certain indices in a position
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## *****Inputs*****
## k:
## type: int
## preF: -k
## yml: 1
## min-intronlen:
## type: int
## preF: -min-intronlen
## yml: 30
## max-intronlen:
## type: int
## preF: -max-intronlen
## yml: 3000
## threads:
## type: int
## preF: -threads
## yml: 4
## U:
## type: File
## preF: -U
## yml: ./data/SRR446027_1.fastq.gz
&lt;/code>&lt;/pre>
&lt;h4 id="subsetting-the-command-line">Subsetting the command-line&lt;/h4>
&lt;pre>&lt;code class="language-r">cmd2 &amp;lt;- subsetParam(cmd, position = &amp;quot;inputs&amp;quot;, index = 1:2, trim = TRUE)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## *****Inputs*****
## S:
## type: File
## preF: -S
## yml: ./results/M1A.sam
## x:
## type: File
## preF: -x
## yml: ./data/tair10.fasta
## *****Parsed raw command line*****
## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmdlist(cmd2)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $defaultid
## $defaultid$hisat2
## [1] &amp;quot;hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmd2 &amp;lt;- subsetParam(cmd, position = &amp;quot;inputs&amp;quot;, index = c(&amp;quot;S&amp;quot;, &amp;quot;x&amp;quot;), trim = TRUE)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## *****Inputs*****
## S:
## type: File
## preF: -S
## yml: ./results/M1A.sam
## x:
## type: File
## preF: -x
## yml: ./data/tair10.fasta
## *****Parsed raw command line*****
## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmdlist(cmd2)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $defaultid
## $defaultid$hisat2
## [1] &amp;quot;hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta&amp;quot;
&lt;/code>&lt;/pre>
&lt;h4 id="replacing-a-existing-argument-in-the-command-line">Replacing a existing argument in the command-line&lt;/h4>
&lt;pre>&lt;code class="language-r">cmd3 &amp;lt;- replaceParam(cmd, &amp;quot;base&amp;quot;, index = 1, replace = list(baseCommand = &amp;quot;bwa&amp;quot;))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Replacing baseCommand
## *****BaseCommand*****
## bwa
## *****Parsed raw command line*****
## bwa -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmdlist(cmd3)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $defaultid
## $defaultid$hisat2
## [1] &amp;quot;bwa -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">new_inputs &amp;lt;- new_inputs &amp;lt;- list(new_input1 = list(type = &amp;quot;File&amp;quot;, preF = &amp;quot;-b&amp;quot;, yml = &amp;quot;myfile&amp;quot;),
new_input2 = &amp;quot;-L &amp;lt;int: 4&amp;gt;&amp;quot;)
cmd4 &amp;lt;- replaceParam(cmd, &amp;quot;inputs&amp;quot;, index = 1:2, replace = new_inputs)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Replacing inputs
## *****Inputs*****
## new_input1:
## type: File
## preF: -b
## yml: myfile
## new_input2:
## type: int
## preF: -L
## yml: 4
## k:
## type: int
## preF: -k
## yml: 1
## min-intronlen:
## type: int
## preF: -min-intronlen
## yml: 30
## max-intronlen:
## type: int
## preF: -max-intronlen
## yml: 3000
## threads:
## type: int
## preF: -threads
## yml: 4
## U:
## type: File
## preF: -U
## yml: ./data/SRR446027_1.fastq.gz
## *****Parsed raw command line*****
## hisat2 -b myfile -L 4 -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmdlist(cmd4)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $defaultid
## $defaultid$hisat2
## [1] &amp;quot;hisat2 -b myfile -L 4 -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz&amp;quot;
&lt;/code>&lt;/pre>
&lt;h4 id="adding-new-arguments">Adding new arguments&lt;/h4>
&lt;pre>&lt;code class="language-r">newIn &amp;lt;- new_inputs &amp;lt;- list(new_input1 = list(type = &amp;quot;File&amp;quot;, preF = &amp;quot;-b1&amp;quot;, yml = &amp;quot;myfile1&amp;quot;),
new_input2 = list(type = &amp;quot;File&amp;quot;, preF = &amp;quot;-b2&amp;quot;, yml = &amp;quot;myfile2&amp;quot;), new_input3 = &amp;quot;-b3 &amp;lt;F: myfile3&amp;gt;&amp;quot;)
cmd5 &amp;lt;- appendParam(cmd, &amp;quot;inputs&amp;quot;, index = 1:2, append = new_inputs)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Replacing inputs
## *****Inputs*****
## S:
## type: File
## preF: -S
## yml: ./results/M1A.sam
## x:
## type: File
## preF: -x
## yml: ./data/tair10.fasta
## k:
## type: int
## preF: -k
## yml: 1
## min-intronlen:
## type: int
## preF: -min-intronlen
## yml: 30
## max-intronlen:
## type: int
## preF: -max-intronlen
## yml: 3000
## threads:
## type: int
## preF: -threads
## yml: 4
## U:
## type: File
## preF: -U
## yml: ./data/SRR446027_1.fastq.gz
## new_input1:
## type: File
## preF: -b1
## yml: myfile1
## new_input2:
## type: File
## preF: -b2
## yml: myfile2
## new_input3:
## type: File
## preF: -b3
## yml: myfile3
## *****Parsed raw command line*****
## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz -b1 myfile1 -b2 myfile2 -b3 myfile3
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmdlist(cmd5)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $defaultid
## $defaultid$hisat2
## [1] &amp;quot;hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz -b1 myfile1 -b2 myfile2 -b3 myfile3&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmd6 &amp;lt;- appendParam(cmd, &amp;quot;inputs&amp;quot;, index = 1:2, after = 0, append = new_inputs)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Replacing inputs
## *****Inputs*****
## new_input1:
## type: File
## preF: -b1
## yml: myfile1
## new_input2:
## type: File
## preF: -b2
## yml: myfile2
## new_input3:
## type: File
## preF: -b3
## yml: myfile3
## S:
## type: File
## preF: -S
## yml: ./results/M1A.sam
## x:
## type: File
## preF: -x
## yml: ./data/tair10.fasta
## k:
## type: int
## preF: -k
## yml: 1
## min-intronlen:
## type: int
## preF: -min-intronlen
## yml: 30
## max-intronlen:
## type: int
## preF: -max-intronlen
## yml: 3000
## threads:
## type: int
## preF: -threads
## yml: 4
## U:
## type: File
## preF: -U
## yml: ./data/SRR446027_1.fastq.gz
## *****Parsed raw command line*****
## hisat2 -b1 myfile1 -b2 myfile2 -b3 myfile3 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">cmdlist(cmd6)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $defaultid
## $defaultid$hisat2
## [1] &amp;quot;hisat2 -b1 myfile1 -b2 myfile2 -b3 myfile3 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz&amp;quot;
&lt;/code>&lt;/pre>
&lt;h4 id="editing-output-param">Editing &lt;code>output&lt;/code> param&lt;/h4>
&lt;pre>&lt;code class="language-r">new_outs &amp;lt;- list(sam_out = &amp;quot;&amp;lt;F: $(inputs.results_path)/test.sam&amp;gt;&amp;quot;)
cmd7 &amp;lt;- replaceParam(cmd, &amp;quot;outputs&amp;quot;, index = 1, replace = new_outs)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Replacing outputs
## *****Outputs*****
## sam_out:
## type: File
## value: $(inputs.results_path)/test.sam
## *****Parsed raw command line*****
## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">output(cmd7)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## $defaultid
## $defaultid$hisat2
## [1] &amp;quot;./results/test.sam&amp;quot;
&lt;/code>&lt;/pre></description></item><item><title>Sp: SPR detailed installation instructions</title><link>/sp/spr/sprinstall/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/sprinstall/</guid><description>
&lt;script src="../../rmarkdown-libs/kePrint/kePrint.js">&lt;/script>
&lt;link href="../../rmarkdown-libs/lightable/lightable.css" rel="stylesheet" />
&lt;style type="text/css">
pre code {
white-space: pre !important;
overflow-x: scroll !important;
word-break: keep-all !important;
word-wrap: initial !important;
}
&lt;/style>
&lt;!--
- Compile from command-line
Rscript -e "rmarkdown::render('SPRinstall.Rmd', c('BiocStyle::html_document'), clean=F); knitr::knit('SPRinstall.Rmd', tangle=TRUE)"; Rscript -e "rmarkdown::render('SPRinstall.Rmd', c('BiocStyle::pdf_document'))"
-->
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
document.querySelector("h1").className = "title";
});
&lt;/script>
&lt;script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
var links = document.links;
for (var i = 0, linksLength = links.length; i &lt; linksLength; i++)
if (links[i].hostname != window.location.hostname)
links[i].target = '_blank';
});
&lt;/script>
&lt;h2 id="systempiper-installation">&lt;code>systemPipeR&lt;/code> Installation&lt;/h2>
&lt;p>To install the &lt;code>systemPipeR&lt;/code> package (H Backman and Girke 2016), please use
the &lt;em>&lt;code>BiocManager::install&lt;/code>&lt;/em> command:&lt;/p>
&lt;pre>&lt;code class="language-r">if (!requireNamespace(&amp;quot;BiocManager&amp;quot;, quietly = TRUE)) install.packages(&amp;quot;BiocManager&amp;quot;)
BiocManager::install(&amp;quot;systemPipeR&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>To obtain the most recent updates immediately, one can install it directly from
&lt;a href="https://github.com/tgirke/systemPipeR">GitHub&lt;/a> as follow:&lt;/p>
&lt;pre>&lt;code class="language-r">if (!requireNamespace(&amp;quot;BiocManager&amp;quot;, quietly = TRUE)) install.packages(&amp;quot;BiocManager&amp;quot;)
BiocManager::install(&amp;quot;tgirke/systemPipeR&amp;quot;, dependencies = TRUE)
&lt;/code>&lt;/pre>
&lt;h2 id="third-party-software-tools-in-spr">Third-party software tools in &lt;em>&lt;code>SPR&lt;/code>&lt;/em>&lt;/h2>
&lt;p>Current, &lt;em>systemPipeR&lt;/em> provides the &lt;em>&lt;code>param&lt;/code>&lt;/em> file templates for third-party
software tools. Please check the listed software tools.&lt;/p>
&lt;div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; ">
&lt;table class="table table-striped table-hover table-condensed" style="margin-left: auto; margin-right: auto;">
&lt;thead>
&lt;tr>
&lt;th style="text-align:center;position: sticky; top:0; background-color: #FFFFFF;">
Tool Name
&lt;/th>
&lt;th style="text-align:center;position: sticky; top:0; background-color: #FFFFFF;">
Description
&lt;/th>
&lt;th style="text-align:center;position: sticky; top:0; background-color: #FFFFFF;">
Step
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="http://bio-bwa.sourceforge.net/bwa.shtml">bwa&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. 
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #8FBC8F !important;">Alignment&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml">Bowtie2&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #8FBC8F !important;">Alignment&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="http://hannonlab.cshl.edu/fastx_toolkit/commandline.html">FASTX-Toolkit&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
FASTX-Toolkit is a collection of command line tools for Short-Reads FASTA/FASTQ files preprocessing.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #EC7770 !important;">Read Preprocessing&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="http://hibberdlab.com/transrate/">TransRate&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Transrate is software for de-novo transcriptome assembly quality analysis.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #D98576 !important;">Quality&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="http://research-pub.gene.com/gmap/">Gsnap&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
GSNAP is a genomic short-read nucleotide alignment program.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #8FBC8F !important;">Alignment&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="http://www.htslib.org/doc/samtools-1.2.html">Samtools&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Samtools is a suite of programs for interacting with high-throughput sequencing data.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #D08C79 !important;">Post-processing&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="http://www.usadellab.org/cms/?page=trimmomatic">Trimmomatic&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Trimmomatic is a flexible read trimming tool for Illumina NGS data.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #EC7770 !important;">Read Preprocessing&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://bioconductor.org/packages/release/bioc/vignettes/Rsubread/inst/doc/SubreadUsersGuide.pdf">Rsubread&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Rsubread is a Bioconductor software package that provides high-performance alignment and read counting functions for RNA-seq reads.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #8FBC8F !important;">Alignment&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://broadinstitute.github.io/picard/">Picard&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Picard is a set of command line tools for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #B4A082 !important;">Manipulating HTS data&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://busco.ezlab.org/">Busco&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
BUSCO assesses genome assembly and annotation completeness with Benchmarking Universal Single-Copy Orthologs.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #D98576 !important;">Quality&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://ccb.jhu.edu/software/hisat2/manual.shtml">Hisat2&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
HISAT2 is a fast and sensitive alignment program for mapping NGS reads (both DNA and RNA) to reference genomes.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #8FBC8F !important;">Alignment&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://ccb.jhu.edu/software/tophat/manual.shtml">Tophat2&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
TopHat is a fast splice junction mapper for RNA-Seq reads.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #8FBC8F !important;">Alignment&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://gatk.broadinstitute.org/hc/en-us">GATK&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Variant Discovery in High-Throughput Sequencing Data.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #FF6A6A !important;">Variant Discovery&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://github.com/alexdobin/STAR">STAR&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
STAR is an ultrafast universal RNA-seq aligner.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #8FBC8F !important;">Alignment&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://github.com/FelixKrueger/TrimGalore">Trim\_galore&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Trim Galore is a wrapper around Cutadapt and FastQC to consistently apply adapter and quality trimming to FastQ files.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #EC7770 !important;">Read Preprocessing&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://github.com/TransDecoder/TransDecoder/wiki">TransDecoder&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
TransDecoder identifies candidate coding regions within transcript sequences.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #ABA785 !important;">Find Coding Regions&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://github.com/trinityrnaseq/trinityrnaseq/wiki">Trinity&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Trinity assembles transcript sequences from Illumina RNA-Seq data.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #A1AE88 !important;">denovo Transcriptome Assembly&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://github.com/Trinotate/Trinotate.github.io/wiki">Trinotate&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Trinotate is a comprehensive annotation suite designed for automatic functional annotation of transcriptomes.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #F5706D !important;">Transcriptome Functional Annotation&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://macs3-project.github.io/MACS/">MACS2&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
MACS2 identifies transcription factor binding sites in ChIP-seq data.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #C7937C !important;">Peak calling&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://pachterlab.github.io/kallisto/manual">Kallisto&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
kallisto is a program for quantifying abundances of transcripts from RNA-Seq data.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #E37E73 !important;">Read counting&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://samtools.github.io/bcftools/howtos/index.html">BCFtools&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
BCFtools is a program for variant calling and manipulating files in the Variant Call Format (VCF) and its binary counterpart BCF.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #FF6A6A !important;">Variant Discovery&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://www.bioinformatics.babraham.ac.uk/projects/bismark/">Bismark&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
Bismark is a program to map bisulfite treated sequencing reads to a genome of interest and perform methylation calls in a single step.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #98B58B !important;">Bisulfite mapping&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://www.bioinformatics.babraham.ac.uk/projects/fastqc/">Fastqc&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
FastQC is a quality control tool for high throughput sequence data.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #D98576 !important;">Quality&lt;/span>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center;">
&lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK279690/">Blast&lt;/a>
&lt;/td>
&lt;td style="text-align:center;">
BLAST finds regions of similarity between biological sequences.
&lt;/td>
&lt;td style="text-align:center;">
&lt;span style=" font-weight: bold; color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #BD997F !important;">Blast&lt;/span>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;h2 id="references">References&lt;/h2>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-H_Backman2016-bt" class="csl-entry">
&lt;p>H Backman, Tyler W, and Thomas Girke. 2016. “&lt;span class="nocase">systemPipeR: NGS workflow and report generation environment&lt;/span>.” &lt;em>BMC Bioinformatics&lt;/em> 17 (1): 388. &lt;a href="https://doi.org/10.1186/s12859-016-1241-0">https://doi.org/10.1186/s12859-016-1241-0&lt;/a>.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Sp: Help Manual</title><link>/sp/spr/spr_funcs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/spr_funcs/</guid><description>
&lt;hr>
&lt;style>
.td-content li a {
font-size: 1.5rem;
}
&lt;/style>
&lt;h2 id="systempiper-functions-reference-manuals">&lt;em>systempipeR&lt;/em> Functions Reference Manuals&lt;/h2>
&lt;p>The following reference manual was created by &lt;a href="http://bioconductor.org/packages/devel/bioc/html/systemPipeR.html">systemPipeR Development version&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="../../spr/funcs/spr/reference">systemPipeR Reference Manual&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr></description></item><item><title>Sp: SPR Docker container</title><link>/sp/spr/sp_docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/sp_docker/</guid><description>
&lt;blockquote>
&lt;p>Guidelines from &lt;a href="https://github.com/Bioconductor/bioconductor_docker">bioconductor_docker&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h1 id="running-the-systempiper-with-docker">Running the &lt;code>systemPipeR&lt;/code> with Docker&lt;/h1>
&lt;h2 id="get-a-copy-of-the-public-docker-image">Get a copy of the public docker image&lt;/h2>
&lt;pre>&lt;code class="language-bash">docker pull systempipe/systempipe_docker:latest
&lt;/code>&lt;/pre>
&lt;h2 id="to-run-rstudio-server">To run RStudio Server:&lt;/h2>
&lt;pre>&lt;code class="language-bash">docker run -e PASSWORD=systemPipe -p 8787:8787 \
systempipe/systempipe_docker:latest
&lt;/code>&lt;/pre>
&lt;p>You can then open a web browser pointing to your docker host on
port 8787. If you&amp;rsquo;re on Linux and using default settings, the docker
host is &lt;code>127.0.0.1&lt;/code> (or &lt;code>localhost&lt;/code>, so the full URL to RStudio would
be &lt;code>http://localhost:8787)&lt;/code>. If you are on Mac or Windows and running
&lt;code>Docker Toolbox&lt;/code>, you can determine the docker host with the
&lt;code>docker-machine ip default&lt;/code> command.&lt;/p>
&lt;p>In the above command, &lt;code>-e PASSWORD=&lt;/code> is setting the RStudio password
and is required by the RStudio Docker image. It can be whatever you
like except it cannot be &lt;code>rstudio&lt;/code>. Log in to RStudio with the
username &lt;code>rstudio&lt;/code> and whatever password was specified, in this
example &lt;code>systemPipe&lt;/code>.&lt;/p>
&lt;h2 id="to-run-r-from-the-command-line">To run R from the command line:&lt;/h2>
&lt;pre>&lt;code class="language-bash">docker run -it --user rstudio systempipe/systempipe_docker:latest R
&lt;/code>&lt;/pre>
&lt;h2 id="to-open-a-bash-shell-on-the-container">To open a Bash shell on the container:&lt;/h2>
&lt;pre>&lt;code class="language-bash">docker run -it --user rstudio systempipe/systempipe_docker:latest bash
&lt;/code>&lt;/pre>
&lt;!-- # Full Documentation -->
&lt;!-- This tutorial shows how to create, access, run, build a Docker container. -->
&lt;!-- ## Table of Content -->
&lt;!-- 1. [Install](#Install) -->
&lt;!-- 2. [Docker Hub Account](#dockerHub) -->
&lt;!-- 3. [Log in to the Docker Hub](#login) -->
&lt;!-- 4. [Run Docker](#run) -->
&lt;!-- 5. [Create your first repository](#create) -->
&lt;!-- 6. [Make changes to the container and Create the new image](#changes) -->
&lt;!-- 7. [Commands](#commands) -->
&lt;!-- 8. [Docker and GitHub Actions](#github) -->
&lt;!-- 9. [Common Problems](#faq) -->
&lt;!-- 10. [Singularity Container](#singularity) -->
&lt;!-- 11. [Resources](#resources) -->
&lt;hr>
&lt;div id='Install'/>
&lt;h1 id="install">Install&lt;/h1>
&lt;p>&lt;strong>Prerequisites&lt;/strong>:
&lt;a href="https://docs.docker.com/installation/">Linux&lt;/a>
&lt;a href="http://docs.docker.com/installation/mac/">Mac&lt;/a>
&lt;a href="http://docs.docker.com/installation/windows/">Windows&lt;/a>&lt;/p>
&lt;p>Instructions &lt;a href="https://docs.docker.com/engine/install/ubuntu/">here&lt;/a> on how to install Docker Engine on Ubuntu.&lt;/p>
&lt;pre>&lt;code>sudo apt-get update
sudo apt-get install \
apt-transport-https \
ca-certificates \
curl \
gnupg
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo \
&amp;quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
$(lsb_release -cs) stable&amp;quot; | sudo tee /etc/apt/sources.list.d/docker.list &amp;gt; /dev/null
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io
## Verify that Docker Engine is installed correctly by running the hello-world image.
sudo docker run hello-world
&lt;/code>&lt;/pre>
&lt;h2 id="uninstall">Uninstall&lt;/h2>
&lt;pre>&lt;code>sudo apt-get remove docker docker-engine docker.io containerd runc
&lt;/code>&lt;/pre>
&lt;hr>
&lt;div id='dockerHUb'/>
&lt;h1 id="docker-hub-account">Docker Hub Account&lt;/h1>
&lt;p>To be able to share a custom image, please go to &lt;a href="https://hub.docker.com">https://hub.docker.com&lt;/a> and
create a free account.&lt;/p>
&lt;hr>
&lt;div id='login'/>
&lt;h2 id="log-in-to-the-docker-hub-locally">Log in to the Docker Hub locally&lt;/h2>
&lt;p>Login with your Docker ID to push and pull images from Docker Hub. If you don&amp;rsquo;t
have a Docker ID, head over to &lt;a href="https://hub.docker.com">https://hub.docker.com&lt;/a> to create one.&lt;/p>
&lt;pre>&lt;code>docker login
# Username: XXXX
# Password: xxx
# Login Succeeded
&lt;/code>&lt;/pre>
&lt;hr>
&lt;div id='run'/>
&lt;h1 id="run-docker">Run Docker&lt;/h1>
&lt;pre>&lt;code>docker run-dP systempipe/systempipe_docker:latest
&lt;/code>&lt;/pre>
&lt;p>Make sure the container is running:&lt;/p>
&lt;pre>&lt;code>docker ps
# CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS # NAMES
#5d007f66a7b3 systempipe/systempipe_docker:latest &amp;quot;/init&amp;quot; 5 minutes ago Up 5 minutes 0.0.0.0:49153-&amp;gt;8787/tcp determined_easle
&lt;/code>&lt;/pre>
&lt;h2 id="login-to-the-container">Login to the container&lt;/h2>
&lt;p>Please check the &lt;code>NAMES&lt;/code> in this example, &lt;code>determined_easle,&lt;/code> to login into the container.&lt;/p>
&lt;pre>&lt;code>docker exec -it determined_easle /bin/bash
&lt;/code>&lt;/pre>
&lt;h2 id="other-alternatives-to-run-the-container">Other alternatives to run the container&lt;/h2>
&lt;h3 id="to-run-rstudio-server-1">To run RStudio Server:&lt;/h3>
&lt;pre>&lt;code>docker run -e PASSWORD=systemPipe -p 8787:8787 \
systempipe/systempipe_docker:latest
&lt;/code>&lt;/pre>
&lt;h3 id="to-run-r-from-the-command-line-1">To run R from the command line:&lt;/h3>
&lt;pre>&lt;code>docker run -it --user rstudio systempipe/systempipe_docker:latest R
&lt;/code>&lt;/pre>
&lt;h3 id="to-open-a-bash-shell-on-the-container-1">To open a Bash shell on the container:&lt;/h3>
&lt;pre>&lt;code>docker run -it --user rstudio systempipe/systempipe_docker:latest bash
&lt;/code>&lt;/pre>
&lt;h3 id="check-r-version-into-the-container">Check R Version into the container&lt;/h3>
&lt;pre>&lt;code>R --version
&lt;/code>&lt;/pre>
&lt;h2 id="stop-docker">Stop Docker&lt;/h2>
&lt;pre>&lt;code>docker stop determined_easle
&lt;/code>&lt;/pre>
&lt;hr>
&lt;div id='create'/>
&lt;h1 id="create-your-first-repository-linkhttpsdocsdockercomdocker-hub">Create your first repository &lt;a href="https://docs.docker.com/docker-hub/">Link&lt;/a>&lt;/h1>
&lt;h2 id="create-a-repository">Create a repository:&lt;/h2>
&lt;ul>
&lt;li>Sign in to Docker Hub.&lt;/li>
&lt;li>Click Create a Repository on the Docker Hub welcome page:&lt;/li>
&lt;li>Name it &lt;your-username>/my-repo.&lt;/li>
&lt;li>Click Create.&lt;/li>
&lt;/ul>
&lt;h2 id="build-and-push-a-container-image-to-docker-hub-from-your-computer">Build and push a container image to Docker Hub from your computer&lt;/h2>
&lt;h3 id="start-by-creating-a-dockerfile-to-specify-your-application">Start by creating a &lt;em>Dockerfile&lt;/em> to specify your application&lt;/h3>
&lt;pre>&lt;code>mkdir docker_test
cd docker_test
touch Dockerfile
&lt;/code>&lt;/pre>
&lt;pre>&lt;code># Docker inheritance
FROM systempipe/systempipe_docker:latest
## Install BiocStyle
RUN R -e 'BiocManager::install(&amp;quot;BiocStyle&amp;quot;)'
# Install required Bioconductor package from devel version
RUN R -e 'BiocManager::install(&amp;quot;tgirke/systemPipeR&amp;quot;)'
RUN R -e 'BiocManager::install(&amp;quot;tgirke/systemPipeRdata&amp;quot;)'
WORKDIR /home/rstudio/SPRojects
COPY --chown=rstudio:rstudio . /home/rstudio/SPRojects
# Metadata
LABEL name=&amp;quot;systempipe/systempipe_docker&amp;quot; \
version=$BIOCONDUCTOR_DOCKER_systempipe \
url=&amp;quot;https://github.com/systemPipeR/systempipe/systempipe_docker&amp;quot; \
vendor=&amp;quot;systemPipeR Project&amp;quot; \
maintainer=&amp;quot;email@gmail.com&amp;quot; \
description=&amp;quot;Bioconductor docker image containing the systemPipeR Project&amp;quot; \
license=&amp;quot;Artistic-2.0&amp;quot;
&lt;/code>&lt;/pre>
&lt;h3 id="run-docker-build-to-build-your-docker-image">Run &lt;code>docker build&lt;/code> to build your Docker image&lt;/h3>
&lt;pre>&lt;code>docker build -t systempipe/systempipe_docker .
&lt;/code>&lt;/pre>
&lt;h3 id="run-docker-run-to-test-your-docker-image-locally">Run &lt;code>docker run&lt;/code> to test your Docker image locally&lt;/h3>
&lt;pre>&lt;code>docker run -e PASSWORD=systemPipe -p 8787:8787 systempipe/systempipe_docker:latest
&lt;/code>&lt;/pre>
&lt;h3 id="run-docker-push-to-push-your-docker-image-to-docker-hub">Run &lt;code>docker push&lt;/code> to push your Docker image to Docker Hub&lt;/h3>
&lt;pre>&lt;code>docker push systempipe/systempipe_docker
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>Your repository in Docker Hub should now display a new latest tag under &lt;code>Tags&lt;/code>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;div id='changes'/>
&lt;h1 id="make-changes-to-the-container-and-create-the-new-image">Make changes to the container and Create the new image&lt;/h1>
&lt;p>Create a folder, for example:&lt;/p>
&lt;pre>&lt;code>docker run -dP systempipe/systempipe_docker
docker ps ## To check the NAME &amp;lt;lucid_grothendieck&amp;gt;
docker exec -it lucid_grothendieck /bin/bash
root@33c758eb1626:/# R
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">setwd(&amp;quot;home/rstudio/&amp;quot;)
systemPipeRdata::genWorkenvir(&amp;quot;rnaseq&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>exit
docker commit -m &amp;quot;Added rnaseq template&amp;quot; -a &amp;quot;Dani Cassol&amp;quot; lucid_grothendieck dcassol/systempipeworkshop2021:rnaseq
docker push systempipe/systempipe_docker:rnaseq
&lt;/code>&lt;/pre>
&lt;p>Run the new image:&lt;/p>
&lt;pre>&lt;code>docker run -e PASSWORD=systemPipe -p 8787:8787 systempipe/systempipe_docker:rnaseq
&lt;/code>&lt;/pre>
&lt;hr>
&lt;div id='commands'/>
&lt;h1 id="commands">Commands&lt;/h1>
&lt;h2 id="list-which-docker-machines-are-available-locally">List which docker machines are available locally&lt;/h2>
&lt;p>&lt;code>docker images&lt;/code>&lt;/p>
&lt;h2 id="list-running-containers">List running containers&lt;/h2>
&lt;p>&lt;code>docker ps&lt;/code>&lt;/p>
&lt;h2 id="list-all-containers">List all containers&lt;/h2>
&lt;p>&lt;code>docker ps -a&lt;/code>&lt;/p>
&lt;h2 id="resume-a-stopped-container">Resume a stopped container&lt;/h2>
&lt;p>&lt;code>docker start &amp;lt;CONTAINER ID&amp;gt;&lt;/code>&lt;/p>
&lt;h2 id="shell-into-a-running-container">Shell into a running container&lt;/h2>
&lt;p>&lt;code>docker exec -it &amp;lt;CONTAINER ID&amp;gt; /bin/bash&lt;/code>&lt;/p>
&lt;h2 id="stop-or-remove-a-cointainer">Stop OR remove a cointainer&lt;/h2>
&lt;p>&lt;code>docker stop &amp;lt;CONTAINER ID&amp;gt;&lt;/code>
&lt;code>docker rm &amp;lt;CONTAINER ID&amp;gt;&lt;/code>&lt;/p>
&lt;h2 id="remove-a-image">Remove a image&lt;/h2>
&lt;p>&lt;code>docker rmi dcassol/systempipeworkshop2021:rnaseq&lt;/code>&lt;/p>
&lt;hr>
&lt;div id='github'/>
&lt;h1 id="docker-and-github-actions">Docker and GitHub Actions&lt;/h1>
&lt;ol>
&lt;li>To create a new token, go to Docker Hub Settings&lt;/li>
&lt;/ol>
&lt;p>1.1. Account Settings &amp;raquo; Security &amp;raquo; New Access Token
1.2. Add Access Token Description &amp;raquo; Create
1.3. Copy the Access Token &amp;raquo; Copy and Close&lt;/p>
&lt;ol start="2">
&lt;li>Go to the Repository at GitHub&lt;/li>
&lt;/ol>
&lt;p>2.1. Settings &amp;gt; Secrets &amp;gt; New repository secret
2.2. Create a new secret with the name &lt;code>DOCKER_HUB_USERNAME&lt;/code> and your &lt;code>Docker ID&lt;/code> as value
2.3. Click at Add secret
2.4. Create a new secret with the name &lt;code>DOCKER_HUB_ACCESS_TOKEN&lt;/code> and your &lt;code>Personal Access Token (PAT)&lt;/code> as value (generated in the previous step)&lt;/p>
&lt;ol start="3">
&lt;li>Set up the GitHub Actions workflow&lt;/li>
&lt;/ol>
&lt;pre>&lt;code> steps:
- name: Checkout Repo
uses: actions/checkout@v2
- name: Login to Docker Hub
uses: docker/login-action@v1
with:
username: ${{ secrets.DOCKER_HUB_USERNAME }}
password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;div id='faq'/>
&lt;h1 id="common-problems">Common Problems&lt;/h1>
&lt;pre>&lt;code>## Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/auth: dial unix /var/run/docker.sock: connect: permission denied
&lt;/code>&lt;/pre>
&lt;p>Solution:&lt;/p>
&lt;pre>&lt;code class="language-bash">sudo chmod 666 /var/run/docker.sock
&lt;/code>&lt;/pre>
&lt;hr>
&lt;div id='singularity'/>
&lt;h1 id="singularity-container">Singularity Container&lt;/h1>
&lt;p>Please download the Docker image of systemPipe, as follow:&lt;/p>
&lt;pre>&lt;code class="language-bash">singularity pull docker://systempipe/systempipe_docker:latest
&lt;/code>&lt;/pre>
&lt;p>You can also use the &lt;code>build&lt;/code> command to download pre-built images from Docker.
Unlike &lt;code>pull&lt;/code>, &lt;code>build&lt;/code> will convert the image to the latest Singularity image format after
downloading it.&lt;/p>
&lt;pre>&lt;code class="language-bash">singularity build systempipe_docker_latest.sif docker://systempipe/systempipe_docker:latest
&lt;/code>&lt;/pre>
&lt;p>To run the container:&lt;/p>
&lt;pre>&lt;code class="language-bash">singularity shell systempipe_docker_latest.sif
&lt;/code>&lt;/pre>
&lt;hr>
&lt;div id='resources'/>
&lt;h1 id="resources">Resources&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://www.mirantis.com/blog/how-do-i-create-a-new-docker-image-for-my-application/">Docker Run: How to create images from an application&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.docker.com/docker-hub/">Docker Hub Quickstart&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.docker.com/ci-cd/github-actions/">Configure GitHub Actions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://sylabs.io/guides/3.0/user-guide/quick_start.html#interact-with-images">Singularity&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Sp: Workflow Plot Editor</title><link>/sp/spr/editor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/editor/</guid><description>
&lt;p>This is a SPR workflow editor, which will allow you to edit the workflow plot before/after running a workflow in SPR.&lt;/p>
&lt;ol>
&lt;li>Create a SPR workflow with the sysArgsList (sal) object, or directly use the sal object after workflow running.&lt;/li>
&lt;li>make a workflow plot with &lt;code>plotWF(sal)&lt;/code> to take a glimpse of the plot preview.&lt;/li>
&lt;li>Use &lt;code>plotWF(sal, out_format = &amp;quot;dot_print&amp;quot;)&lt;/code> to print out the plot in DOT language, copy the whole content to your clipboard.&lt;/li>
&lt;li>Use &lt;a href="../viz_editor">this link to open &lt;strong>Workflow Plot Editor&lt;/strong>&lt;/a>.&lt;/li>
&lt;li>Paste plot code in the editor to start editing.&lt;/li>
&lt;/ol></description></item><item><title>Sp:</title><link>/sp/spr/viz_editor/bower_components/svg-pan-zoom/issue_template/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/sp/spr/viz_editor/bower_components/svg-pan-zoom/issue_template/</guid><description>
&lt;p>Hi there!&lt;/p>
&lt;p>Thanks for submitting an issue to svg-pan-zoom.&lt;/p>
&lt;p>To help us help you better, please do the following before submitting an issue:&lt;/p>
&lt;ol>
&lt;li>Review the available &lt;a href="https://github.com/ariutta/svg-pan-zoom/blob/master/README.md">documentation&lt;/a> and existing &lt;a href="https://github.com/ariutta/svg-pan-zoom#demos">examples&lt;/a>&lt;/li>
&lt;li>Check if the same bug/feature request &lt;a href="https://github.com/ariutta/svg-pan-zoom/issues?q=is%3Aissue%20">wasn&amp;rsquo;t previously reported&lt;/a>&lt;/li>
&lt;li>Make sure you are not asking a usage or debugging question. If you are, use &lt;a href="http://stackoverflow.com/questions/tagged/svgpanzoom">StackOverflow&lt;/a>.&lt;/li>
&lt;li>Fill in the information that corresponds to your type of issue below&lt;/li>
&lt;li>If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem via &lt;a href="https://jsfiddle.net">https://jsfiddle.net&lt;/a> or similar (template: &lt;a href="http://jsfiddle.net/bumbu/167usffr/)">http://jsfiddle.net/bumbu/167usffr/)&lt;/a>.&lt;/li>
&lt;li>Delete this intro and any unrelated text :smile: (if you do not we&amp;rsquo;ll assume you haven&amp;rsquo;t read these instructions and automatically close the issue)&lt;/li>
&lt;/ol>
&lt;h2 id="bug-report">Bug report&lt;/h2>
&lt;h3 id="expected-behaviour">Expected behaviour&lt;/h3>
&lt;p>&lt;em>your text here&lt;/em>&lt;/p>
&lt;h3 id="actual-behaviour">Actual behaviour&lt;/h3>
&lt;p>&lt;em>your text here&lt;/em>&lt;/p>
&lt;h3 id="steps-to-reproduce-the-behaviour">Steps to reproduce the behaviour&lt;/h3>
&lt;ol>
&lt;li>&lt;/li>
&lt;li>&lt;/li>
&lt;/ol>
&lt;h3 id="configuration">Configuration&lt;/h3>
&lt;ul>
&lt;li>svg-pan-zoom version: ``&lt;/li>
&lt;li>Browser(s): ``&lt;/li>
&lt;li>Operating system(s): ``&lt;/li>
&lt;li>A relevant example URL:&lt;/li>
&lt;/ul>
&lt;h2 id="feature-request">Feature Request&lt;/h2>
&lt;ul>
&lt;li>Feature description&lt;/li>
&lt;li>Reasons for adopting new feature&lt;/li>
&lt;li>Is this a breaking change? (How will this affect existing functionality)&lt;/li>
&lt;/ul></description></item></channel></rss>