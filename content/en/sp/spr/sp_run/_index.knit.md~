---
title: "Design and run Workflows" 
author: "Author: Daniela Cassol (danielac@ucr.edu) and Thomas Girke (thomas.girke@ucr.edu)"
date: "Last update: 01 June, 2022" 
output:
  BiocStyle::html_document:
    toc_float: true
    code_folding: show
  BiocStyle::pdf_document: default
package: systemPipeR
vignette: |
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{systemPipeR: Workflow design and reporting generation environment}
  %\VignetteEngine{knitr::rmarkdown}
fontsize: 14pt
bibliography: bibtex.bib
editor_options: 
  chunk_output_type: console
type: docs
weight: 3
---

<!--
- Compile from command-line
Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::html_document'), clean=F); knitr::knit('systemPipeR.Rmd', tangle=TRUE)"; Rscript ../md2jekyll.R systemPipeR.knit.md 2; Rscript -e "rmarkdown::render('systemPipeR.Rmd', c('BiocStyle::pdf_document'))"
-->

<script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
  document.querySelector("h1").className = "title";
});
</script>
<script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
  var links = document.links;  
  for (var i = 0, linksLength = links.length; i < linksLength; i++)
    if (links[i].hostname != window.location.hostname)
      links[i].target = '_blank';
});
</script>





# About this section

In this section, we will discuss following topics: 

- How to create SPR data analysis projects.
- How to build workflow step by step interactively or use use a template as starting point.
- After step design, how to run a workflow.
- After workflow finished running, how we can check the status, visualize the workflow.
- Different options for managing the workflow, _e.g._ resume, restart, overwrite a 
  SPR project. 
- How to explore the workflow object (methods).
- Finally, how to generate some data analysis reports.


# Project initialization

To create a workflow within _`systemPipeR`_, we can start by defining an empty
container and checking the directory structure:


```r
sal <- SPRproject(projPath = getwd(), overwrite = TRUE)
```

```
## Creating directory:  C:\Users\lz\Desktop\lz100\systemPipeR.github.io\content\en\sp\spr\sp_run/data 
## Creating directory:  C:\Users\lz\Desktop\lz100\systemPipeR.github.io\content\en\sp\spr\sp_run/param 
## Creating directory:  C:\Users\lz\Desktop\lz100\systemPipeR.github.io\content\en\sp\spr\sp_run/results 
## Creating directory 'C:\Users\lz\Desktop\lz100\systemPipeR.github.io\content\en\sp\spr\sp_run/.SPRproject'
## Creating file 'C:\Users\lz\Desktop\lz100\systemPipeR.github.io\content\en\sp\spr\sp_run/.SPRproject/SYSargsList.yml'
```

```
## Your current working directory is different from the directory chosen for the Project Workflow.
## For accurate location of the files and running the Workflow, please set the working directory to 
## 'setwd('C:\Users\lz\Desktop\lz100\systemPipeR.github.io\content\en\sp\spr\sp_run')'
```

Internally, `SPRproject` function will create a hidden folder called `.SPRproject`,
by default, to store all the log files.
A `YAML` file, here called `SYSargsList.yml`, has been created, which initially
contains the basic location of the project structure; however, every time the 
workflow object `sal` is updated in R, the new information will also be store in this 
flat-file database for easy recovery.
If you desire different names for the logs folder and the `YAML` file, these can 
be modified as follows:


```r
sal <- SPRproject(logs.dir = ".SPRproject", sys.file = ".SPRproject/SYSargsList.yml")
```

Also, this function will check and/or create the basic folder structure if missing, 
which means `data`, `param`, and `results` folder, as described [here](https://systempipe.org/sp/spr/gettingstarted/#directory-structure). 
If the user wants to use a different names for these directories, can be specified 
as follows:


```r
sal <- SPRproject(data = "data", param = "param", results = "results")
```

It is possible to separate all the R objects created within the workflow analysis 
from the current environment. `SPRproject` function provides the option to create 
a new environment, and in this way, it is not overwriting any object you may want
to have at your current section. 


```r
sal <- SPRproject(envir = new.env())
```

In this stage, the object `sal` is a empty container, except for the project information. The project information can be accessed by the `projectInfo` method:


```r
sal
```

```
## Instance of 'SYSargsList': 
##  No workflow steps added
```

```r
projectInfo(sal)
```

```
## $project
## [1] "C:\\Users\\lz\\Desktop\\lz100\\systemPipeR.github.io\\content\\en\\sp\\spr\\sp_run"
## 
## $data
## [1] "data"
## 
## $param
## [1] "param"
## 
## $results
## [1] "results"
## 
## $logsDir
## [1] ".SPRproject"
## 
## $sysargslist
## [1] ".SPRproject/SYSargsList.yml"
```

Also, the `length` function will return how many steps this workflow contains, 
and in this case, it is empty, as follow:


```r
length(sal)
```

```
## [1] 0
```

# Workflow Design 

_`systemPipeR`_ workflows can be designed and built from start to finish with a single command, importing from an R Markdown file or stepwise in interactive mode from the R console. 
In the [next section](#appendstep), we will demonstrate how to build the workflow in an interactive mode, and in the [following section](#importWF), we will show how to build from a file. 

New workflows are constructed, or existing ones modified, by connecting each step
via `appendStep` method. Each `SYSargsList` instance contains instructions needed 
for processing a set of input files with a specific command-line and the paths to 
the corresponding outfiles generated.

The constructor function `Linewise` is used to build the R code-based step. 
For more details about this S4 class container, see [here](#linewise). 

## Build workflow interactive {#appendstep}

This tutorial shows a straightforward example for describing and explaining all main features available within systemPipeR to design, build, manage, run, and visualize the workflow. In summary, we are exporting a dataset to multiple files, compressing and decompressing each one of the files, importing to R, and finally performing a statistical analysis. 

In the previous section, we initialize the project by building the `sal` object.
Until this moment, the container has no steps:


```r
sal
```

```
## Instance of 'SYSargsList': 
##  No workflow steps added
```

In the next subsection, we will discuss how to populate the object created with the first step in the
workflow interactively.



# Running the workflow

For running the workflow, `runWF` function will execute all the command lines 
store in the workflow container.


```r
sal <- runWF(sal)
```

This essential function allows the user to choose one or multiple steps to be 
executed using the `steps` argument. However, it is necessary to follow the 
workflow dependency graph. If a selected step depends on a previous step(s) that
was not executed, the execution will fail. 


```r
sal <- runWF(sal, steps = c(1, 3))
```

Also, it allows forcing the execution of the steps, even if the status of the 
step is `'Success'` and all the expected `outfiles` exists. 
Another feature of the `runWF` function is ignoring all the warnings 
and errors and running the workflow by the arguments `warning.stop` and 
`error.stop`, respectively.


```r
sal <- runWF(sal, force = TRUE, warning.stop = FALSE, error.stop = TRUE)
```

When the project was initialized by `SPRproject` function, it was created an 
environment for all objects created during the workflow execution. This 
environment can be accessed as follows:


```r
viewEnvir(sal)
```

The workflow execution allows to save this environment for future recovery:


```r
sal <- runWF(sal, saveEnv = TRUE)
```

## Workflow status

To check the summary of the workflow, we can use:


```r
sal
```

```
## Instance of 'SYSargsList': 
##     WF Steps:
##        1. export_iris --> Status: Pending
##        2. gzip --> Status: Pending 
##            Total Files: 3 | Existing: 0 | Missing: 3 
##          2.1. gzip
##              cmdlist: 3 | Pending: 3
##        3. gunzip --> Status: Pending 
##            Total Files: 3 | Existing: 0 | Missing: 3 
##          3.1. gunzip
##              cmdlist: 3 | Pending: 3
##        4. iris_stats --> Status: Pending
## 
```

To access more details about the workflow instances, we can use the `statusWF` method:


```r
statusWF(sal)
```

```
## $export_iris
## DataFrame with 1 row and 2 columns
##          Step status.summary
##   <character>    <character>
## 1 export_iris        Pending
## 
## $gzip
## DataFrame with 3 rows and 5 columns
##        Targets Total_Files Existing_Files Missing_Files     gzip
##    <character>   <numeric>      <numeric>     <numeric> <matrix>
## SE          SE           1              0             1  Pending
## VE          VE           1              0             1  Pending
## VI          VI           1              0             1  Pending
## 
## $gunzip
## DataFrame with 3 rows and 5 columns
##        Targets Total_Files Existing_Files Missing_Files   gunzip
##    <character>   <numeric>      <numeric>     <numeric> <matrix>
## SE          SE           1              0             1  Pending
## VE          VE           1              0             1  Pending
## VI          VI           1              0             1  Pending
## 
## $iris_stats
## DataFrame with 1 row and 2 columns
##          Step status.summary
##   <character>    <character>
## 1  iris_stats        Pending
```

## Parallelization on clusters

This section of the tutorial provides an introduction to the usage of the 
_`systemPipeR`_ features on a cluster.

Alternatively, the computation can be greatly accelerated by processing many files 
in parallel using several compute nodes of a cluster, where a scheduling/queuing
system is used for load balancing. 

The `resources` list object provides the number of independent parallel cluster 
processes defined under the `Njobs` element in the list. The following example 
will run 18 processes in parallel using each 4 CPU cores. 
If the resources available on a cluster allow running all 18 processes at the 
same time, then the shown sample submission will utilize in a total of 72 CPU cores.

Note, `runWF` can be used with most queueing systems as it is based on utilities 
from the `batchtools` package, which supports the use of template files (_`*.tmpl`_)
for defining the run parameters of different schedulers. To run the following 
code, one needs to have both a `conffile` (see _`.batchtools.conf.R`_ samples [here](https://mllg.github.io/batchtools/)) 
and a `template` file (see _`*.tmpl`_ samples [here](https://github.com/mllg/batchtools/tree/master/inst/templates)) 
for the queueing available on a system. The following example uses the sample 
`conffile` and `template` files for the Slurm scheduler provided by this package. 

The resources can be appended when the step is generated, or it is possible to 
add these resources later, as the following example using the `addResources` 
function:


```r
resources <- list(conffile=".batchtools.conf.R",
                  template="batchtools.slurm.tmpl", 
                  Njobs=18, 
                  walltime=120,##minutes
                  ntasks=1,
                  ncpus=4, 
                  memory=1024,##Mb
                  partition = "short"
                  )
sal <- addResources(sal, c("hisat2_mapping"), resources = resources)
sal <- runWF(sal)
```

Note: The example is submitting the jog to `short` partition. If you desire to 
use a different partition, please adjust accordingly.

# Visualize workflow

_`systemPipeR`_ workflows instances can be visualized with the `plotWF` function.

This function will make a plot of selected workflow instance and the following 
information is displayed on the plot:

    - Workflow structure (dependency graphs between different steps); 
    - Workflow step status, *e.g.* `Success`, `Error`, `Pending`, `Warnings`; 
    - Sample status and statistics; 
    - Workflow timing: running duration time. 

If no argument is provided, the basic plot will automatically detect width, 
height, layout, plot method, branches, _etc_. 





































































